


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html id="htmlId">
<head>
  <title>Coverage Report :: TransportAnalyzeAction</title>
  <style type="text/css">
    @import "../../.css/coverage.css";
  </style>
</head>

<body>
<div class="header"></div>

<div class="content">
<div class="breadCrumbs">
    [ <a href="../../index.html">all classes</a> ]
    [ <a href="../index.html">org.elasticsearch.action.admin.indices.analyze</a> ]
</div>

<h1>Coverage Summary for Class: TransportAnalyzeAction (org.elasticsearch.action.admin.indices.analyze)</h1>

<table class="coverageStats">
<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Class, %
</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">TransportAnalyzeAction</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (1/ 1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    5.6%
  </span>
  <span class="absValue">
    (1/ 18)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    2.1%
  </span>
  <span class="absValue">
    (4/ 191)
  </span>
</td>
</tr>

</table>

<br/>
<br/>


<div class="sourceCode"><i>1</i>&nbsp;/*
<i>2</i>&nbsp; * Licensed to Elasticsearch under one or more contributor
<i>3</i>&nbsp; * license agreements. See the NOTICE file distributed with
<i>4</i>&nbsp; * this work for additional information regarding copyright
<i>5</i>&nbsp; * ownership. Elasticsearch licenses this file to you under
<i>6</i>&nbsp; * the Apache License, Version 2.0 (the &quot;License&quot;); you may
<i>7</i>&nbsp; * not use this file except in compliance with the License.
<i>8</i>&nbsp; * You may obtain a copy of the License at
<i>9</i>&nbsp; *
<i>10</i>&nbsp; *    http://www.apache.org/licenses/LICENSE-2.0
<i>11</i>&nbsp; *
<i>12</i>&nbsp; * Unless required by applicable law or agreed to in writing,
<i>13</i>&nbsp; * software distributed under the License is distributed on an
<i>14</i>&nbsp; * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
<i>15</i>&nbsp; * KIND, either express or implied.  See the License for the
<i>16</i>&nbsp; * specific language governing permissions and limitations
<i>17</i>&nbsp; * under the License.
<i>18</i>&nbsp; */
<i>19</i>&nbsp;package org.elasticsearch.action.admin.indices.analyze;
<i>20</i>&nbsp;
<i>21</i>&nbsp;import org.apache.lucene.analysis.Analyzer;
<i>22</i>&nbsp;import org.apache.lucene.analysis.TokenStream;
<i>23</i>&nbsp;import org.apache.lucene.analysis.Tokenizer;
<i>24</i>&nbsp;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
<i>25</i>&nbsp;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
<i>26</i>&nbsp;import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
<i>27</i>&nbsp;import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;
<i>28</i>&nbsp;import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
<i>29</i>&nbsp;import org.apache.lucene.util.BytesRef;
<i>30</i>&nbsp;import org.elasticsearch.ElasticsearchException;
<i>31</i>&nbsp;import org.elasticsearch.action.support.ActionFilters;
<i>32</i>&nbsp;import org.elasticsearch.action.support.single.shard.TransportSingleShardAction;
<i>33</i>&nbsp;import org.elasticsearch.cluster.ClusterState;
<i>34</i>&nbsp;import org.elasticsearch.cluster.block.ClusterBlockException;
<i>35</i>&nbsp;import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
<i>36</i>&nbsp;import org.elasticsearch.cluster.routing.ShardsIterator;
<i>37</i>&nbsp;import org.elasticsearch.cluster.service.ClusterService;
<i>38</i>&nbsp;import org.elasticsearch.common.inject.Inject;
<i>39</i>&nbsp;import org.elasticsearch.common.io.stream.Writeable;
<i>40</i>&nbsp;import org.elasticsearch.common.settings.Settings;
<i>41</i>&nbsp;import org.elasticsearch.core.internal.io.IOUtils;
<i>42</i>&nbsp;import org.elasticsearch.index.IndexService;
<i>43</i>&nbsp;import org.elasticsearch.index.IndexSettings;
<i>44</i>&nbsp;import org.elasticsearch.index.analysis.AnalysisRegistry;
<i>45</i>&nbsp;import org.elasticsearch.index.analysis.AnalyzerComponents;
<i>46</i>&nbsp;import org.elasticsearch.index.analysis.AnalyzerComponentsProvider;
<i>47</i>&nbsp;import org.elasticsearch.index.analysis.CharFilterFactory;
<i>48</i>&nbsp;import org.elasticsearch.index.analysis.NameOrDefinition;
<i>49</i>&nbsp;import org.elasticsearch.index.analysis.NamedAnalyzer;
<i>50</i>&nbsp;import org.elasticsearch.index.analysis.TokenFilterFactory;
<i>51</i>&nbsp;import org.elasticsearch.index.analysis.TokenizerFactory;
<i>52</i>&nbsp;import org.elasticsearch.index.mapper.KeywordFieldMapper;
<i>53</i>&nbsp;import org.elasticsearch.index.mapper.MappedFieldType;
<i>54</i>&nbsp;import org.elasticsearch.index.shard.ShardId;
<i>55</i>&nbsp;import org.elasticsearch.indices.IndicesService;
<i>56</i>&nbsp;import org.elasticsearch.threadpool.ThreadPool;
<i>57</i>&nbsp;import org.elasticsearch.transport.TransportService;
<i>58</i>&nbsp;
<i>59</i>&nbsp;import java.io.IOException;
<i>60</i>&nbsp;import java.io.Reader;
<i>61</i>&nbsp;import java.io.StringReader;
<i>62</i>&nbsp;import java.util.ArrayList;
<i>63</i>&nbsp;import java.util.HashSet;
<i>64</i>&nbsp;import java.util.List;
<i>65</i>&nbsp;import java.util.Locale;
<i>66</i>&nbsp;import java.util.Map;
<i>67</i>&nbsp;import java.util.Set;
<i>68</i>&nbsp;import java.util.TreeMap;
<i>69</i>&nbsp;
<i>70</i>&nbsp;/**
<i>71</i>&nbsp; * Transport action used to execute analyze requests
<i>72</i>&nbsp; */
<b class="nc"><i>73</i>&nbsp;public class TransportAnalyzeAction extends TransportSingleShardAction&lt;AnalyzeAction.Request, AnalyzeAction.Response&gt; {</b>
<i>74</i>&nbsp;
<i>75</i>&nbsp;    private final Settings settings;
<i>76</i>&nbsp;    private final IndicesService indicesService;
<i>77</i>&nbsp;
<i>78</i>&nbsp;    @Inject
<i>79</i>&nbsp;    public TransportAnalyzeAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
<i>80</i>&nbsp;                                  TransportService transportService, IndicesService indicesService, ActionFilters actionFilters,
<i>81</i>&nbsp;                                  IndexNameExpressionResolver indexNameExpressionResolver) {
<b class="fc"><i>82</i>&nbsp;        super(AnalyzeAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver,</b>
<i>83</i>&nbsp;            AnalyzeAction.Request::new, ThreadPool.Names.ANALYZE);
<b class="fc"><i>84</i>&nbsp;        this.settings = settings;</b>
<b class="fc"><i>85</i>&nbsp;        this.indicesService = indicesService;</b>
<b class="fc"><i>86</i>&nbsp;    }</b>
<i>87</i>&nbsp;
<i>88</i>&nbsp;    @Override
<i>89</i>&nbsp;    protected Writeable.Reader&lt;AnalyzeAction.Response&gt; getResponseReader() {
<b class="nc"><i>90</i>&nbsp;        return AnalyzeAction.Response::new;</b>
<i>91</i>&nbsp;    }
<i>92</i>&nbsp;
<i>93</i>&nbsp;    @Override
<i>94</i>&nbsp;    protected boolean resolveIndex(AnalyzeAction.Request request) {
<b class="nc"><i>95</i>&nbsp;        return request.index() != null;</b>
<i>96</i>&nbsp;    }
<i>97</i>&nbsp;
<i>98</i>&nbsp;    @Override
<i>99</i>&nbsp;    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
<b class="nc"><i>100</i>&nbsp;        if (request.concreteIndex() != null) {</b>
<b class="nc"><i>101</i>&nbsp;            return super.checkRequestBlock(state, request);</b>
<i>102</i>&nbsp;        }
<b class="nc"><i>103</i>&nbsp;        return null;</b>
<i>104</i>&nbsp;    }
<i>105</i>&nbsp;
<i>106</i>&nbsp;    @Override
<i>107</i>&nbsp;    protected ShardsIterator shards(ClusterState state, InternalRequest request) {
<b class="nc"><i>108</i>&nbsp;        if (request.concreteIndex() == null) {</b>
<i>109</i>&nbsp;            // just execute locally....
<b class="nc"><i>110</i>&nbsp;            return null;</b>
<i>111</i>&nbsp;        }
<b class="nc"><i>112</i>&nbsp;        return state.routingTable().index(request.concreteIndex()).randomAllActiveShardsIt();</b>
<i>113</i>&nbsp;    }
<i>114</i>&nbsp;
<i>115</i>&nbsp;    @Override
<i>116</i>&nbsp;    protected AnalyzeAction.Response shardOperation(AnalyzeAction.Request request, ShardId shardId) throws IOException {
<b class="nc"><i>117</i>&nbsp;        final IndexService indexService = getIndexService(shardId);</b>
<b class="nc"><i>118</i>&nbsp;        final int maxTokenCount = indexService == null ?</b>
<b class="nc"><i>119</i>&nbsp;            IndexSettings.MAX_TOKEN_COUNT_SETTING.get(settings) : indexService.getIndexSettings().getMaxTokenCount();</b>
<i>120</i>&nbsp;
<b class="nc"><i>121</i>&nbsp;        return analyze(request, indicesService.getAnalysis(), indexService, maxTokenCount);</b>
<i>122</i>&nbsp;    }
<i>123</i>&nbsp;
<i>124</i>&nbsp;    public static AnalyzeAction.Response analyze(AnalyzeAction.Request request, AnalysisRegistry analysisRegistry,
<i>125</i>&nbsp;                                          IndexService indexService, int maxTokenCount) throws IOException {
<i>126</i>&nbsp;
<b class="nc"><i>127</i>&nbsp;        IndexSettings settings = indexService == null ? null : indexService.getIndexSettings();</b>
<i>128</i>&nbsp;
<i>129</i>&nbsp;        // First, we check to see if the request requires a custom analyzer.  If so, then we
<i>130</i>&nbsp;        // need to build it and then close it after use.
<b class="nc"><i>131</i>&nbsp;        try (Analyzer analyzer = buildCustomAnalyzer(request, analysisRegistry, settings)) {</b>
<b class="nc"><i>132</i>&nbsp;            if (analyzer != null) {</b>
<b class="nc"><i>133</i>&nbsp;                return analyze(request, analyzer, maxTokenCount);</b>
<i>134</i>&nbsp;            }
<b class="nc"><i>135</i>&nbsp;        }</b>
<i>136</i>&nbsp;
<i>137</i>&nbsp;        // Otherwise we use a built-in analyzer, which should not be closed
<b class="nc"><i>138</i>&nbsp;        return analyze(request, getAnalyzer(request, analysisRegistry, indexService), maxTokenCount);</b>
<i>139</i>&nbsp;    }
<i>140</i>&nbsp;
<i>141</i>&nbsp;    private IndexService getIndexService(ShardId shardId) {
<b class="nc"><i>142</i>&nbsp;        if (shardId != null) {</b>
<b class="nc"><i>143</i>&nbsp;            return indicesService.indexServiceSafe(shardId.getIndex());</b>
<i>144</i>&nbsp;        }
<b class="nc"><i>145</i>&nbsp;        return null;</b>
<i>146</i>&nbsp;    }
<i>147</i>&nbsp;
<i>148</i>&nbsp;    private static Analyzer getAnalyzer(AnalyzeAction.Request request, AnalysisRegistry analysisRegistry,
<i>149</i>&nbsp;                                        IndexService indexService) throws IOException {
<b class="nc"><i>150</i>&nbsp;        if (request.analyzer() != null) {</b>
<b class="nc"><i>151</i>&nbsp;            if (indexService == null) {</b>
<b class="nc"><i>152</i>&nbsp;                Analyzer analyzer = analysisRegistry.getAnalyzer(request.analyzer());</b>
<b class="nc"><i>153</i>&nbsp;                if (analyzer == null) {</b>
<b class="nc"><i>154</i>&nbsp;                    throw new IllegalArgumentException(&quot;failed to find global analyzer [&quot; + request.analyzer() + &quot;]&quot;);</b>
<i>155</i>&nbsp;                }
<b class="nc"><i>156</i>&nbsp;                return analyzer;</b>
<i>157</i>&nbsp;            } else {
<b class="nc"><i>158</i>&nbsp;                Analyzer analyzer = indexService.getIndexAnalyzers().get(request.analyzer());</b>
<b class="nc"><i>159</i>&nbsp;                if (analyzer == null) {</b>
<b class="nc"><i>160</i>&nbsp;                    throw new IllegalArgumentException(&quot;failed to find analyzer [&quot; + request.analyzer() + &quot;]&quot;);</b>
<i>161</i>&nbsp;                }
<b class="nc"><i>162</i>&nbsp;                return analyzer;</b>
<i>163</i>&nbsp;            }
<i>164</i>&nbsp;        }
<b class="nc"><i>165</i>&nbsp;        if (request.normalizer() != null) {</b>
<i>166</i>&nbsp;            // Get normalizer from indexAnalyzers
<b class="nc"><i>167</i>&nbsp;            if (indexService == null) {</b>
<b class="nc"><i>168</i>&nbsp;                throw new IllegalArgumentException(&quot;analysis based on a normalizer requires an index&quot;);</b>
<i>169</i>&nbsp;            }
<b class="nc"><i>170</i>&nbsp;            Analyzer analyzer = indexService.getIndexAnalyzers().getNormalizer(request.normalizer());</b>
<b class="nc"><i>171</i>&nbsp;            if (analyzer == null) {</b>
<b class="nc"><i>172</i>&nbsp;                throw new IllegalArgumentException(&quot;failed to find normalizer under [&quot; + request.normalizer() + &quot;]&quot;);</b>
<i>173</i>&nbsp;            }
<i>174</i>&nbsp;        }
<b class="nc"><i>175</i>&nbsp;        if (request.field() != null) {</b>
<b class="nc"><i>176</i>&nbsp;            if (indexService == null) {</b>
<b class="nc"><i>177</i>&nbsp;                throw new IllegalArgumentException(&quot;analysis based on a specific field requires an index&quot;);</b>
<i>178</i>&nbsp;            }
<b class="nc"><i>179</i>&nbsp;            MappedFieldType fieldType = indexService.mapperService().fullName(request.field());</b>
<b class="nc"><i>180</i>&nbsp;            if (fieldType != null) {</b>
<b class="nc"><i>181</i>&nbsp;                if (fieldType.tokenized() || fieldType instanceof KeywordFieldMapper.KeywordFieldType) {</b>
<b class="nc"><i>182</i>&nbsp;                    return fieldType.indexAnalyzer();</b>
<i>183</i>&nbsp;                } else {
<b class="nc"><i>184</i>&nbsp;                    throw new IllegalArgumentException(&quot;Can&#39;t process field [&quot; + request.field() +</b>
<i>185</i>&nbsp;                        &quot;], Analysis requests are only supported on tokenized fields&quot;);
<i>186</i>&nbsp;                }
<i>187</i>&nbsp;            }
<i>188</i>&nbsp;        }
<b class="nc"><i>189</i>&nbsp;        if (indexService == null) {</b>
<b class="nc"><i>190</i>&nbsp;            return analysisRegistry.getAnalyzer(&quot;standard&quot;);</b>
<i>191</i>&nbsp;        } else {
<b class="nc"><i>192</i>&nbsp;            return indexService.getIndexAnalyzers().getDefaultIndexAnalyzer();</b>
<i>193</i>&nbsp;        }
<i>194</i>&nbsp;    }
<i>195</i>&nbsp;
<i>196</i>&nbsp;    private static Analyzer buildCustomAnalyzer(AnalyzeAction.Request request, AnalysisRegistry analysisRegistry,
<i>197</i>&nbsp;                                                IndexSettings indexSettings) throws IOException {
<b class="nc"><i>198</i>&nbsp;        if (request.tokenizer() != null) {</b>
<b class="nc"><i>199</i>&nbsp;            return analysisRegistry.buildCustomAnalyzer(indexSettings, false,</b>
<b class="nc"><i>200</i>&nbsp;                request.tokenizer(), request.charFilters(), request.tokenFilters());</b>
<b class="nc"><i>201</i>&nbsp;        } else if (((request.tokenFilters() != null &amp;&amp; request.tokenFilters().size() &gt; 0)</b>
<b class="nc"><i>202</i>&nbsp;            || (request.charFilters() != null &amp;&amp; request.charFilters().size() &gt; 0))) {</b>
<b class="nc"><i>203</i>&nbsp;            return analysisRegistry.buildCustomAnalyzer(indexSettings, true, new NameOrDefinition(&quot;keyword&quot;),</b>
<b class="nc"><i>204</i>&nbsp;                request.charFilters(), request.tokenFilters());</b>
<i>205</i>&nbsp;        }
<b class="nc"><i>206</i>&nbsp;        return null;</b>
<i>207</i>&nbsp;    }
<i>208</i>&nbsp;
<i>209</i>&nbsp;    private static AnalyzeAction.Response analyze(AnalyzeAction.Request request, Analyzer analyzer, int maxTokenCount) {
<b class="nc"><i>210</i>&nbsp;        if (request.explain()) {</b>
<b class="nc"><i>211</i>&nbsp;            return new AnalyzeAction.Response(null, detailAnalyze(request, analyzer, maxTokenCount));</b>
<i>212</i>&nbsp;        }
<b class="nc"><i>213</i>&nbsp;        return new AnalyzeAction.Response(simpleAnalyze(request, analyzer, maxTokenCount), null);</b>
<i>214</i>&nbsp;    }
<i>215</i>&nbsp;
<i>216</i>&nbsp;    private static List&lt;AnalyzeAction.AnalyzeToken&gt; simpleAnalyze(AnalyzeAction.Request request,
<i>217</i>&nbsp;                                                                  Analyzer analyzer, int maxTokenCount) {
<b class="nc"><i>218</i>&nbsp;        TokenCounter tc = new TokenCounter(maxTokenCount);</b>
<b class="nc"><i>219</i>&nbsp;        List&lt;AnalyzeAction.AnalyzeToken&gt; tokens = new ArrayList&lt;&gt;();</b>
<b class="nc"><i>220</i>&nbsp;        int lastPosition = -1;</b>
<b class="nc"><i>221</i>&nbsp;        int lastOffset = 0;</b>
<i>222</i>&nbsp;        // Note that we always pass &quot;&quot; as the field to the various Analyzer methods, because
<i>223</i>&nbsp;        // the analyzers we use here are all field-specific and so ignore this parameter
<b class="nc"><i>224</i>&nbsp;        for (String text : request.text()) {</b>
<b class="nc"><i>225</i>&nbsp;            try (TokenStream stream = analyzer.tokenStream(&quot;&quot;, text)) {</b>
<b class="nc"><i>226</i>&nbsp;                stream.reset();</b>
<b class="nc"><i>227</i>&nbsp;                CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);</b>
<b class="nc"><i>228</i>&nbsp;                PositionIncrementAttribute posIncr = stream.addAttribute(PositionIncrementAttribute.class);</b>
<b class="nc"><i>229</i>&nbsp;                OffsetAttribute offset = stream.addAttribute(OffsetAttribute.class);</b>
<b class="nc"><i>230</i>&nbsp;                TypeAttribute type = stream.addAttribute(TypeAttribute.class);</b>
<b class="nc"><i>231</i>&nbsp;                PositionLengthAttribute posLen = stream.addAttribute(PositionLengthAttribute.class);</b>
<i>232</i>&nbsp;
<b class="nc"><i>233</i>&nbsp;                while (stream.incrementToken()) {</b>
<b class="nc"><i>234</i>&nbsp;                    int increment = posIncr.getPositionIncrement();</b>
<b class="nc"><i>235</i>&nbsp;                    if (increment &gt; 0) {</b>
<b class="nc"><i>236</i>&nbsp;                        lastPosition = lastPosition + increment;</b>
<i>237</i>&nbsp;                    }
<b class="nc"><i>238</i>&nbsp;                    tokens.add(new AnalyzeAction.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(),</b>
<b class="nc"><i>239</i>&nbsp;                        lastOffset + offset.endOffset(), posLen.getPositionLength(), type.type(), null));</b>
<b class="nc"><i>240</i>&nbsp;                    tc.increment();</b>
<b class="nc"><i>241</i>&nbsp;                }</b>
<b class="nc"><i>242</i>&nbsp;                stream.end();</b>
<b class="nc"><i>243</i>&nbsp;                lastOffset += offset.endOffset();</b>
<b class="nc"><i>244</i>&nbsp;                lastPosition += posIncr.getPositionIncrement();</b>
<i>245</i>&nbsp;
<b class="nc"><i>246</i>&nbsp;                lastPosition += analyzer.getPositionIncrementGap(&quot;&quot;);</b>
<b class="nc"><i>247</i>&nbsp;                lastOffset += analyzer.getOffsetGap(&quot;&quot;);</b>
<b class="nc"><i>248</i>&nbsp;            } catch (IOException e) {</b>
<b class="nc"><i>249</i>&nbsp;                throw new ElasticsearchException(&quot;failed to analyze&quot;, e);</b>
<b class="nc"><i>250</i>&nbsp;            }</b>
<i>251</i>&nbsp;        }
<b class="nc"><i>252</i>&nbsp;        return tokens;</b>
<i>253</i>&nbsp;    }
<i>254</i>&nbsp;
<i>255</i>&nbsp;    private static AnalyzeAction.DetailAnalyzeResponse detailAnalyze(AnalyzeAction.Request request, Analyzer analyzer,
<i>256</i>&nbsp;                                                                     int maxTokenCount) {
<i>257</i>&nbsp;        AnalyzeAction.DetailAnalyzeResponse detailResponse;
<b class="nc"><i>258</i>&nbsp;        final Set&lt;String&gt; includeAttributes = new HashSet&lt;&gt;();</b>
<b class="nc"><i>259</i>&nbsp;        if (request.attributes() != null) {</b>
<b class="nc"><i>260</i>&nbsp;            for (String attribute : request.attributes()) {</b>
<b class="nc"><i>261</i>&nbsp;                includeAttributes.add(attribute.toLowerCase(Locale.ROOT));</b>
<i>262</i>&nbsp;            }
<i>263</i>&nbsp;        }
<i>264</i>&nbsp;
<i>265</i>&nbsp;        // maybe unwrap analyzer from NamedAnalyzer
<b class="nc"><i>266</i>&nbsp;        Analyzer potentialCustomAnalyzer = analyzer;</b>
<b class="nc"><i>267</i>&nbsp;        if (analyzer instanceof NamedAnalyzer) {</b>
<b class="nc"><i>268</i>&nbsp;            potentialCustomAnalyzer = ((NamedAnalyzer) analyzer).analyzer();</b>
<i>269</i>&nbsp;        }
<i>270</i>&nbsp;
<b class="nc"><i>271</i>&nbsp;        if (potentialCustomAnalyzer instanceof AnalyzerComponentsProvider) {</b>
<b class="nc"><i>272</i>&nbsp;            AnalyzerComponentsProvider customAnalyzer = (AnalyzerComponentsProvider) potentialCustomAnalyzer;</b>
<i>273</i>&nbsp;            // note: this is not field-name dependent in our cases so we can leave out the argument
<b class="nc"><i>274</i>&nbsp;            int positionIncrementGap = potentialCustomAnalyzer.getPositionIncrementGap(&quot;&quot;);</b>
<b class="nc"><i>275</i>&nbsp;            int offsetGap = potentialCustomAnalyzer.getOffsetGap(&quot;&quot;);</b>
<b class="nc"><i>276</i>&nbsp;            AnalyzerComponents components = customAnalyzer.getComponents();</b>
<i>277</i>&nbsp;            // divide charfilter, tokenizer tokenfilters
<b class="nc"><i>278</i>&nbsp;            CharFilterFactory[] charFilterFactories = components.getCharFilters();</b>
<b class="nc"><i>279</i>&nbsp;            TokenizerFactory tokenizerFactory = components.getTokenizerFactory();</b>
<b class="nc"><i>280</i>&nbsp;            TokenFilterFactory[] tokenFilterFactories = components.getTokenFilters();</b>
<i>281</i>&nbsp;
<b class="nc"><i>282</i>&nbsp;            String[][] charFiltersTexts = new String[charFilterFactories != null ? charFilterFactories.length : 0][request.text().length];</b>
<b class="nc"><i>283</i>&nbsp;            TokenListCreator[] tokenFiltersTokenListCreator = new TokenListCreator[tokenFilterFactories != null ?</b>
<b class="nc"><i>284</i>&nbsp;                tokenFilterFactories.length : 0];</b>
<i>285</i>&nbsp;
<b class="nc"><i>286</i>&nbsp;            TokenListCreator tokenizerTokenListCreator = new TokenListCreator(maxTokenCount);</b>
<i>287</i>&nbsp;
<b class="nc"><i>288</i>&nbsp;            for (int textIndex = 0; textIndex &lt; request.text().length; textIndex++) {</b>
<b class="nc"><i>289</i>&nbsp;                String charFilteredSource = request.text()[textIndex];</b>
<i>290</i>&nbsp;
<b class="nc"><i>291</i>&nbsp;                Reader reader = new StringReader(charFilteredSource);</b>
<b class="nc"><i>292</i>&nbsp;                if (charFilterFactories != null) {</b>
<i>293</i>&nbsp;
<b class="nc"><i>294</i>&nbsp;                    for (int charFilterIndex = 0; charFilterIndex &lt; charFilterFactories.length; charFilterIndex++) {</b>
<b class="nc"><i>295</i>&nbsp;                        reader = charFilterFactories[charFilterIndex].create(reader);</b>
<b class="nc"><i>296</i>&nbsp;                        Reader readerForWriteOut = new StringReader(charFilteredSource);</b>
<b class="nc"><i>297</i>&nbsp;                        readerForWriteOut = charFilterFactories[charFilterIndex].create(readerForWriteOut);</b>
<b class="nc"><i>298</i>&nbsp;                        charFilteredSource = writeCharStream(readerForWriteOut);</b>
<b class="nc"><i>299</i>&nbsp;                        charFiltersTexts[charFilterIndex][textIndex] = charFilteredSource;</b>
<i>300</i>&nbsp;                    }
<i>301</i>&nbsp;                }
<i>302</i>&nbsp;
<i>303</i>&nbsp;                // analyzing only tokenizer
<b class="nc"><i>304</i>&nbsp;                Tokenizer tokenizer = tokenizerFactory.create();</b>
<b class="nc"><i>305</i>&nbsp;                tokenizer.setReader(reader);</b>
<b class="nc"><i>306</i>&nbsp;                tokenizerTokenListCreator.analyze(tokenizer, includeAttributes, positionIncrementGap, offsetGap);</b>
<i>307</i>&nbsp;
<i>308</i>&nbsp;                // analyzing each tokenfilter
<b class="nc"><i>309</i>&nbsp;                if (tokenFilterFactories != null) {</b>
<b class="nc"><i>310</i>&nbsp;                    for (int tokenFilterIndex = 0; tokenFilterIndex &lt; tokenFilterFactories.length; tokenFilterIndex++) {</b>
<b class="nc"><i>311</i>&nbsp;                        if (tokenFiltersTokenListCreator[tokenFilterIndex] == null) {</b>
<b class="nc"><i>312</i>&nbsp;                            tokenFiltersTokenListCreator[tokenFilterIndex] = new TokenListCreator(maxTokenCount);</b>
<i>313</i>&nbsp;                        }
<b class="nc"><i>314</i>&nbsp;                        TokenStream stream = createStackedTokenStream(request.text()[textIndex],</b>
<i>315</i>&nbsp;                            charFilterFactories, tokenizerFactory, tokenFilterFactories, tokenFilterIndex + 1);
<b class="nc"><i>316</i>&nbsp;                        tokenFiltersTokenListCreator[tokenFilterIndex].analyze(stream, includeAttributes, positionIncrementGap, offsetGap);</b>
<i>317</i>&nbsp;                    }
<i>318</i>&nbsp;                }
<i>319</i>&nbsp;            }
<i>320</i>&nbsp;
<b class="nc"><i>321</i>&nbsp;            AnalyzeAction.CharFilteredText[] charFilteredLists =</b>
<i>322</i>&nbsp;                new AnalyzeAction.CharFilteredText[charFiltersTexts.length];
<i>323</i>&nbsp;
<b class="nc"><i>324</i>&nbsp;            if (charFilterFactories != null) {</b>
<b class="nc"><i>325</i>&nbsp;                for (int charFilterIndex = 0; charFilterIndex &lt; charFiltersTexts.length; charFilterIndex++) {</b>
<b class="nc"><i>326</i>&nbsp;                    charFilteredLists[charFilterIndex] = new AnalyzeAction.CharFilteredText(</b>
<b class="nc"><i>327</i>&nbsp;                        charFilterFactories[charFilterIndex].name(), charFiltersTexts[charFilterIndex]);</b>
<i>328</i>&nbsp;                }
<i>329</i>&nbsp;            }
<b class="nc"><i>330</i>&nbsp;            AnalyzeAction.AnalyzeTokenList[] tokenFilterLists =</b>
<i>331</i>&nbsp;                new AnalyzeAction.AnalyzeTokenList[tokenFiltersTokenListCreator.length];
<i>332</i>&nbsp;
<b class="nc"><i>333</i>&nbsp;            if (tokenFilterFactories != null) {</b>
<b class="nc"><i>334</i>&nbsp;                for (int tokenFilterIndex = 0; tokenFilterIndex &lt; tokenFiltersTokenListCreator.length; tokenFilterIndex++) {</b>
<b class="nc"><i>335</i>&nbsp;                    tokenFilterLists[tokenFilterIndex] = new AnalyzeAction.AnalyzeTokenList(</b>
<b class="nc"><i>336</i>&nbsp;                        tokenFilterFactories[tokenFilterIndex].name(), tokenFiltersTokenListCreator[tokenFilterIndex].getArrayTokens());</b>
<i>337</i>&nbsp;                }
<i>338</i>&nbsp;            }
<b class="nc"><i>339</i>&nbsp;            detailResponse = new AnalyzeAction.DetailAnalyzeResponse(charFilteredLists,</b>
<b class="nc"><i>340</i>&nbsp;                new AnalyzeAction.AnalyzeTokenList(tokenizerFactory.name(), tokenizerTokenListCreator.getArrayTokens()),</b>
<i>341</i>&nbsp;                tokenFilterLists);
<b class="nc"><i>342</i>&nbsp;        } else {</b>
<i>343</i>&nbsp;            String name;
<b class="nc"><i>344</i>&nbsp;            if (analyzer instanceof NamedAnalyzer) {</b>
<b class="nc"><i>345</i>&nbsp;                name = ((NamedAnalyzer) analyzer).name();</b>
<i>346</i>&nbsp;            } else {
<b class="nc"><i>347</i>&nbsp;                name = analyzer.getClass().getName();</b>
<i>348</i>&nbsp;            }
<i>349</i>&nbsp;
<b class="nc"><i>350</i>&nbsp;            TokenListCreator tokenListCreator = new TokenListCreator(maxTokenCount);</b>
<b class="nc"><i>351</i>&nbsp;            for (String text : request.text()) {</b>
<b class="nc"><i>352</i>&nbsp;                tokenListCreator.analyze(analyzer.tokenStream(&quot;&quot;, text), includeAttributes, analyzer.getPositionIncrementGap(&quot;&quot;),</b>
<b class="nc"><i>353</i>&nbsp;                        analyzer.getOffsetGap(&quot;&quot;));</b>
<i>354</i>&nbsp;            }
<b class="nc"><i>355</i>&nbsp;            detailResponse</b>
<b class="nc"><i>356</i>&nbsp;                = new AnalyzeAction.DetailAnalyzeResponse(new AnalyzeAction.AnalyzeTokenList(name, tokenListCreator.getArrayTokens()));</b>
<i>357</i>&nbsp;        }
<b class="nc"><i>358</i>&nbsp;        return detailResponse;</b>
<i>359</i>&nbsp;    }
<i>360</i>&nbsp;
<i>361</i>&nbsp;    private static TokenStream createStackedTokenStream(String source, CharFilterFactory[] charFilterFactories,
<i>362</i>&nbsp;                                                        TokenizerFactory tokenizerFactory, TokenFilterFactory[] tokenFilterFactories,
<i>363</i>&nbsp;                                                        int current) {
<b class="nc"><i>364</i>&nbsp;        Reader reader = new StringReader(source);</b>
<b class="nc"><i>365</i>&nbsp;        for (CharFilterFactory charFilterFactory : charFilterFactories) {</b>
<b class="nc"><i>366</i>&nbsp;            reader = charFilterFactory.create(reader);</b>
<i>367</i>&nbsp;        }
<b class="nc"><i>368</i>&nbsp;        Tokenizer tokenizer = tokenizerFactory.create();</b>
<b class="nc"><i>369</i>&nbsp;        tokenizer.setReader(reader);</b>
<b class="nc"><i>370</i>&nbsp;        TokenStream tokenStream = tokenizer;</b>
<b class="nc"><i>371</i>&nbsp;        for (int i = 0; i &lt; current; i++) {</b>
<b class="nc"><i>372</i>&nbsp;            tokenStream = tokenFilterFactories[i].create(tokenStream);</b>
<i>373</i>&nbsp;        }
<b class="nc"><i>374</i>&nbsp;        return tokenStream;</b>
<i>375</i>&nbsp;    }
<i>376</i>&nbsp;
<i>377</i>&nbsp;    private static String writeCharStream(Reader input) {
<b class="nc"><i>378</i>&nbsp;        final int BUFFER_SIZE = 1024;</b>
<b class="nc"><i>379</i>&nbsp;        char[] buf = new char[BUFFER_SIZE];</b>
<i>380</i>&nbsp;        int len;
<b class="nc"><i>381</i>&nbsp;        StringBuilder sb = new StringBuilder();</b>
<i>382</i>&nbsp;        do {
<i>383</i>&nbsp;            try {
<b class="nc"><i>384</i>&nbsp;                len = input.read(buf, 0, BUFFER_SIZE);</b>
<b class="nc"><i>385</i>&nbsp;            } catch (IOException e) {</b>
<b class="nc"><i>386</i>&nbsp;                throw new ElasticsearchException(&quot;failed to analyze (charFiltering)&quot;, e);</b>
<b class="nc"><i>387</i>&nbsp;            }</b>
<b class="nc"><i>388</i>&nbsp;            if (len &gt; 0) {</b>
<b class="nc"><i>389</i>&nbsp;                sb.append(buf, 0, len);</b>
<i>390</i>&nbsp;            }
<b class="nc"><i>391</i>&nbsp;        } while (len == BUFFER_SIZE);</b>
<b class="nc"><i>392</i>&nbsp;        return sb.toString();</b>
<i>393</i>&nbsp;    }
<i>394</i>&nbsp;
<i>395</i>&nbsp;    private static class TokenCounter{
<i>396</i>&nbsp;        private int tokenCount = 0;
<i>397</i>&nbsp;        private int maxTokenCount;
<i>398</i>&nbsp;
<i>399</i>&nbsp;        private TokenCounter(int maxTokenCount){
<i>400</i>&nbsp;            this.maxTokenCount = maxTokenCount;
<i>401</i>&nbsp;        }
<i>402</i>&nbsp;        private void increment(){
<i>403</i>&nbsp;            tokenCount++;
<i>404</i>&nbsp;            if (tokenCount &gt; maxTokenCount) {
<i>405</i>&nbsp;                throw new IllegalStateException(
<i>406</i>&nbsp;                    &quot;The number of tokens produced by calling _analyze has exceeded the allowed maximum of [&quot; + maxTokenCount + &quot;].&quot;
<i>407</i>&nbsp;                        + &quot; This limit can be set by changing the [index.analyze.max_token_count] index level setting.&quot;);
<i>408</i>&nbsp;            }
<i>409</i>&nbsp;        }
<i>410</i>&nbsp;    }
<i>411</i>&nbsp;
<i>412</i>&nbsp;    private static class TokenListCreator {
<i>413</i>&nbsp;        int lastPosition = -1;
<i>414</i>&nbsp;        int lastOffset = 0;
<i>415</i>&nbsp;        List&lt;AnalyzeAction.AnalyzeToken&gt; tokens;
<i>416</i>&nbsp;        private TokenCounter tc;
<i>417</i>&nbsp;
<i>418</i>&nbsp;        TokenListCreator(int maxTokenCount) {
<i>419</i>&nbsp;            tokens = new ArrayList&lt;&gt;();
<i>420</i>&nbsp;            tc = new TokenCounter(maxTokenCount);
<i>421</i>&nbsp;        }
<i>422</i>&nbsp;
<i>423</i>&nbsp;        private void analyze(TokenStream stream, Set&lt;String&gt; includeAttributes, int positionIncrementGap, int offsetGap) {
<i>424</i>&nbsp;            try {
<i>425</i>&nbsp;                stream.reset();
<i>426</i>&nbsp;                CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
<i>427</i>&nbsp;                PositionIncrementAttribute posIncr = stream.addAttribute(PositionIncrementAttribute.class);
<i>428</i>&nbsp;                OffsetAttribute offset = stream.addAttribute(OffsetAttribute.class);
<i>429</i>&nbsp;                TypeAttribute type = stream.addAttribute(TypeAttribute.class);
<i>430</i>&nbsp;                PositionLengthAttribute posLen = stream.addAttribute(PositionLengthAttribute.class);
<i>431</i>&nbsp;
<i>432</i>&nbsp;                while (stream.incrementToken()) {
<i>433</i>&nbsp;                    int increment = posIncr.getPositionIncrement();
<i>434</i>&nbsp;                    if (increment &gt; 0) {
<i>435</i>&nbsp;                        lastPosition = lastPosition + increment;
<i>436</i>&nbsp;                    }
<i>437</i>&nbsp;                    tokens.add(new AnalyzeAction.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(),
<i>438</i>&nbsp;                        lastOffset + offset.endOffset(), posLen.getPositionLength(), type.type(),
<i>439</i>&nbsp;                        extractExtendedAttributes(stream, includeAttributes)));
<i>440</i>&nbsp;                    tc.increment();
<i>441</i>&nbsp;                }
<i>442</i>&nbsp;                stream.end();
<i>443</i>&nbsp;                lastOffset += offset.endOffset();
<i>444</i>&nbsp;                lastPosition += posIncr.getPositionIncrement();
<i>445</i>&nbsp;
<i>446</i>&nbsp;                lastPosition += positionIncrementGap;
<i>447</i>&nbsp;                lastOffset += offsetGap;
<i>448</i>&nbsp;
<i>449</i>&nbsp;            } catch (IOException e) {
<i>450</i>&nbsp;                throw new ElasticsearchException(&quot;failed to analyze&quot;, e);
<i>451</i>&nbsp;            } finally {
<i>452</i>&nbsp;                IOUtils.closeWhileHandlingException(stream);
<i>453</i>&nbsp;            }
<i>454</i>&nbsp;        }
<i>455</i>&nbsp;
<i>456</i>&nbsp;        private AnalyzeAction.AnalyzeToken[] getArrayTokens() {
<i>457</i>&nbsp;            return tokens.toArray(new AnalyzeAction.AnalyzeToken[0]);
<i>458</i>&nbsp;        }
<i>459</i>&nbsp;
<i>460</i>&nbsp;    }
<i>461</i>&nbsp;
<i>462</i>&nbsp;    /**
<i>463</i>&nbsp;     * other attribute extract object.
<i>464</i>&nbsp;     * Extracted object group by AttributeClassName
<i>465</i>&nbsp;     *
<i>466</i>&nbsp;     * @param stream current TokenStream
<i>467</i>&nbsp;     * @param includeAttributes filtering attributes
<i>468</i>&nbsp;     * @return Map&amp;lt;key value&amp;gt;
<i>469</i>&nbsp;     */
<i>470</i>&nbsp;    private static Map&lt;String, Object&gt; extractExtendedAttributes(TokenStream stream, final Set&lt;String&gt; includeAttributes) {
<b class="nc"><i>471</i>&nbsp;        final Map&lt;String, Object&gt; extendedAttributes = new TreeMap&lt;&gt;();</b>
<i>472</i>&nbsp;
<b class="nc"><i>473</i>&nbsp;        stream.reflectWith((attClass, key, value) -&gt; {</b>
<b class="nc"><i>474</i>&nbsp;            if (CharTermAttribute.class.isAssignableFrom(attClass)) {</b>
<b class="nc"><i>475</i>&nbsp;                return;</b>
<i>476</i>&nbsp;            }
<b class="nc"><i>477</i>&nbsp;            if (PositionIncrementAttribute.class.isAssignableFrom(attClass)) {</b>
<b class="nc"><i>478</i>&nbsp;                return;</b>
<i>479</i>&nbsp;            }
<b class="nc"><i>480</i>&nbsp;            if (OffsetAttribute.class.isAssignableFrom(attClass)) {</b>
<b class="nc"><i>481</i>&nbsp;                return;</b>
<i>482</i>&nbsp;            }
<b class="nc"><i>483</i>&nbsp;            if (TypeAttribute.class.isAssignableFrom(attClass)) {</b>
<b class="nc"><i>484</i>&nbsp;                return;</b>
<i>485</i>&nbsp;            }
<b class="nc"><i>486</i>&nbsp;            if (includeAttributes == null || includeAttributes.isEmpty() || includeAttributes.contains(key.toLowerCase(Locale.ROOT))) {</b>
<b class="nc"><i>487</i>&nbsp;                if (value instanceof BytesRef) {</b>
<b class="nc"><i>488</i>&nbsp;                    final BytesRef p = (BytesRef) value;</b>
<b class="nc"><i>489</i>&nbsp;                    value = p.toString();</b>
<i>490</i>&nbsp;                }
<b class="nc"><i>491</i>&nbsp;                extendedAttributes.put(key, value);</b>
<i>492</i>&nbsp;            }
<b class="nc"><i>493</i>&nbsp;        });</b>
<i>494</i>&nbsp;
<b class="nc"><i>495</i>&nbsp;        return extendedAttributes;</b>
<i>496</i>&nbsp;    }
<i>497</i>&nbsp;
<i>498</i>&nbsp;}
</div>
</div>

<div class="footer">
    
    <div style="float:right;">generated on 2020-02-09 18:46</div>
</div>
</body>
</html>
