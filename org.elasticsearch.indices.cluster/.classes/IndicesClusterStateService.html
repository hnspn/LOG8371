


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html id="htmlId">
<head>
  <title>Coverage Report :: IndicesClusterStateService</title>
  <style type="text/css">
    @import "../../.css/coverage.css";
  </style>
</head>

<body>
<div class="header"></div>

<div class="content">
<div class="breadCrumbs">
    [ <a href="../../index.html">all classes</a> ]
    [ <a href="../index.html">org.elasticsearch.indices.cluster</a> ]
</div>

<h1>Coverage Summary for Class: IndicesClusterStateService (org.elasticsearch.indices.cluster)</h1>

<table class="coverageStats">

<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">IndicesClusterStateService</td>
<td class="coverageStat">
  <span class="percent">
    73.1%
  </span>
  <span class="absValue">
    (19/ 26)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    58.7%
  </span>
  <span class="absValue">
    (179/ 305)
  </span>
</td>
</tr>
  <tr>
    <td class="name">IndicesClusterStateService$1</td>
<td class="coverageStat">
  <span class="percent">
    33.3%
  </span>
  <span class="absValue">
    (1/ 3)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    20%
  </span>
  <span class="absValue">
    (1/ 5)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$2</td>
<td class="coverageStat">
  <span class="percent">
    66.7%
  </span>
  <span class="absValue">
    (2/ 3)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    40%
  </span>
  <span class="absValue">
    (4/ 10)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$AllocatedIndex</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$AllocatedIndices</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (1/ 1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    75%
  </span>
  <span class="absValue">
    (3/ 4)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$AllocatedIndices$IndexRemovalReason</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (2/ 2)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (6/ 6)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$FailedShardHandler</td>
<td class="coverageStat">
  <span class="percent">
    33.3%
  </span>
  <span class="absValue">
    (1/ 3)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    11.1%
  </span>
  <span class="absValue">
    (1/ 9)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$RecoveryListener</td>
<td class="coverageStat">
  <span class="percent">
    75%
  </span>
  <span class="absValue">
    (3/ 4)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    77.8%
  </span>
  <span class="absValue">
    (7/ 9)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">IndicesClusterStateService$Shard</td>
  </tr>
<tr>
  <td class="name"><strong>total</strong></td>
<td class="coverageStat">
  <span class="percent">
    69%
  </span>
  <span class="absValue">
    (29/ 42)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    57.8%
  </span>
  <span class="absValue">
    (201/ 348)
  </span>
</td>
</tr>
</table>

<br/>
<br/>


<div class="sourceCode"><i>1</i>&nbsp;/*
<i>2</i>&nbsp; * Licensed to Elasticsearch under one or more contributor
<i>3</i>&nbsp; * license agreements. See the NOTICE file distributed with
<i>4</i>&nbsp; * this work for additional information regarding copyright
<i>5</i>&nbsp; * ownership. Elasticsearch licenses this file to you under
<i>6</i>&nbsp; * the Apache License, Version 2.0 (the &quot;License&quot;); you may
<i>7</i>&nbsp; * not use this file except in compliance with the License.
<i>8</i>&nbsp; * You may obtain a copy of the License at
<i>9</i>&nbsp; *
<i>10</i>&nbsp; *    http://www.apache.org/licenses/LICENSE-2.0
<i>11</i>&nbsp; *
<i>12</i>&nbsp; * Unless required by applicable law or agreed to in writing,
<i>13</i>&nbsp; * software distributed under the License is distributed on an
<i>14</i>&nbsp; * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
<i>15</i>&nbsp; * KIND, either express or implied.  See the License for the
<i>16</i>&nbsp; * specific language governing permissions and limitations
<i>17</i>&nbsp; * under the License.
<i>18</i>&nbsp; */
<i>19</i>&nbsp;
<i>20</i>&nbsp;package org.elasticsearch.indices.cluster;
<i>21</i>&nbsp;
<i>22</i>&nbsp;import org.apache.logging.log4j.LogManager;
<i>23</i>&nbsp;import org.apache.logging.log4j.Logger;
<i>24</i>&nbsp;import org.apache.logging.log4j.message.ParameterizedMessage;
<i>25</i>&nbsp;import org.elasticsearch.ResourceAlreadyExistsException;
<i>26</i>&nbsp;import org.elasticsearch.action.ActionListener;
<i>27</i>&nbsp;import org.elasticsearch.action.support.replication.ReplicationResponse;
<i>28</i>&nbsp;import org.elasticsearch.cluster.ClusterChangedEvent;
<i>29</i>&nbsp;import org.elasticsearch.cluster.ClusterState;
<i>30</i>&nbsp;import org.elasticsearch.cluster.ClusterStateApplier;
<i>31</i>&nbsp;import org.elasticsearch.cluster.action.index.NodeMappingRefreshAction;
<i>32</i>&nbsp;import org.elasticsearch.cluster.action.shard.ShardStateAction;
<i>33</i>&nbsp;import org.elasticsearch.cluster.metadata.IndexMetaData;
<i>34</i>&nbsp;import org.elasticsearch.cluster.node.DiscoveryNode;
<i>35</i>&nbsp;import org.elasticsearch.cluster.node.DiscoveryNodes;
<i>36</i>&nbsp;import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
<i>37</i>&nbsp;import org.elasticsearch.cluster.routing.RecoverySource.Type;
<i>38</i>&nbsp;import org.elasticsearch.cluster.routing.RoutingNode;
<i>39</i>&nbsp;import org.elasticsearch.cluster.routing.RoutingTable;
<i>40</i>&nbsp;import org.elasticsearch.cluster.routing.ShardRouting;
<i>41</i>&nbsp;import org.elasticsearch.cluster.service.ClusterService;
<i>42</i>&nbsp;import org.elasticsearch.common.Nullable;
<i>43</i>&nbsp;import org.elasticsearch.common.component.AbstractLifecycleComponent;
<i>44</i>&nbsp;import org.elasticsearch.common.inject.Inject;
<i>45</i>&nbsp;import org.elasticsearch.common.settings.Settings;
<i>46</i>&nbsp;import org.elasticsearch.common.unit.TimeValue;
<i>47</i>&nbsp;import org.elasticsearch.common.util.concurrent.AbstractRunnable;
<i>48</i>&nbsp;import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
<i>49</i>&nbsp;import org.elasticsearch.env.ShardLockObtainFailedException;
<i>50</i>&nbsp;import org.elasticsearch.gateway.GatewayService;
<i>51</i>&nbsp;import org.elasticsearch.index.Index;
<i>52</i>&nbsp;import org.elasticsearch.index.IndexComponent;
<i>53</i>&nbsp;import org.elasticsearch.index.IndexService;
<i>54</i>&nbsp;import org.elasticsearch.index.IndexSettings;
<i>55</i>&nbsp;import org.elasticsearch.index.seqno.GlobalCheckpointSyncAction;
<i>56</i>&nbsp;import org.elasticsearch.index.seqno.ReplicationTracker;
<i>57</i>&nbsp;import org.elasticsearch.index.seqno.RetentionLeaseBackgroundSyncAction;
<i>58</i>&nbsp;import org.elasticsearch.index.seqno.RetentionLeaseSyncAction;
<i>59</i>&nbsp;import org.elasticsearch.index.seqno.RetentionLeaseSyncer;
<i>60</i>&nbsp;import org.elasticsearch.index.seqno.RetentionLeases;
<i>61</i>&nbsp;import org.elasticsearch.index.shard.IndexEventListener;
<i>62</i>&nbsp;import org.elasticsearch.index.shard.IndexShard;
<i>63</i>&nbsp;import org.elasticsearch.index.shard.IndexShardRelocatedException;
<i>64</i>&nbsp;import org.elasticsearch.index.shard.IndexShardState;
<i>65</i>&nbsp;import org.elasticsearch.index.shard.PrimaryReplicaSyncer;
<i>66</i>&nbsp;import org.elasticsearch.index.shard.PrimaryReplicaSyncer.ResyncTask;
<i>67</i>&nbsp;import org.elasticsearch.index.shard.ShardId;
<i>68</i>&nbsp;import org.elasticsearch.index.shard.ShardNotFoundException;
<i>69</i>&nbsp;import org.elasticsearch.indices.IndicesService;
<i>70</i>&nbsp;import org.elasticsearch.indices.flush.SyncedFlushService;
<i>71</i>&nbsp;import org.elasticsearch.indices.recovery.PeerRecoverySourceService;
<i>72</i>&nbsp;import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;
<i>73</i>&nbsp;import org.elasticsearch.indices.recovery.RecoveryFailedException;
<i>74</i>&nbsp;import org.elasticsearch.indices.recovery.RecoveryState;
<i>75</i>&nbsp;import org.elasticsearch.repositories.RepositoriesService;
<i>76</i>&nbsp;import org.elasticsearch.search.SearchService;
<i>77</i>&nbsp;import org.elasticsearch.snapshots.SnapshotShardsService;
<i>78</i>&nbsp;import org.elasticsearch.threadpool.ThreadPool;
<i>79</i>&nbsp;
<i>80</i>&nbsp;import java.io.IOException;
<i>81</i>&nbsp;import java.util.ArrayList;
<i>82</i>&nbsp;import java.util.Arrays;
<i>83</i>&nbsp;import java.util.HashMap;
<i>84</i>&nbsp;import java.util.HashSet;
<i>85</i>&nbsp;import java.util.Iterator;
<i>86</i>&nbsp;import java.util.List;
<i>87</i>&nbsp;import java.util.Map;
<i>88</i>&nbsp;import java.util.Objects;
<i>89</i>&nbsp;import java.util.Set;
<i>90</i>&nbsp;import java.util.concurrent.ConcurrentMap;
<i>91</i>&nbsp;import java.util.concurrent.TimeUnit;
<i>92</i>&nbsp;import java.util.function.BiConsumer;
<i>93</i>&nbsp;import java.util.function.Consumer;
<i>94</i>&nbsp;
<i>95</i>&nbsp;import static org.elasticsearch.indices.cluster.IndicesClusterStateService.AllocatedIndices.IndexRemovalReason.CLOSED;
<i>96</i>&nbsp;import static org.elasticsearch.indices.cluster.IndicesClusterStateService.AllocatedIndices.IndexRemovalReason.DELETED;
<i>97</i>&nbsp;import static org.elasticsearch.indices.cluster.IndicesClusterStateService.AllocatedIndices.IndexRemovalReason.FAILURE;
<i>98</i>&nbsp;import static org.elasticsearch.indices.cluster.IndicesClusterStateService.AllocatedIndices.IndexRemovalReason.NO_LONGER_ASSIGNED;
<i>99</i>&nbsp;import static org.elasticsearch.indices.cluster.IndicesClusterStateService.AllocatedIndices.IndexRemovalReason.REOPENED;
<i>100</i>&nbsp;
<b class="fc"><i>101</i>&nbsp;public class IndicesClusterStateService extends AbstractLifecycleComponent implements ClusterStateApplier {</b>
<b class="fc"><i>102</i>&nbsp;    private static final Logger logger = LogManager.getLogger(IndicesClusterStateService.class);</b>
<i>103</i>&nbsp;
<i>104</i>&nbsp;    final AllocatedIndices&lt;? extends Shard, ? extends AllocatedIndex&lt;? extends Shard&gt;&gt; indicesService;
<i>105</i>&nbsp;    private final ClusterService clusterService;
<i>106</i>&nbsp;    private final ThreadPool threadPool;
<i>107</i>&nbsp;    private final PeerRecoveryTargetService recoveryTargetService;
<i>108</i>&nbsp;    private final ShardStateAction shardStateAction;
<i>109</i>&nbsp;    private final NodeMappingRefreshAction nodeMappingRefreshAction;
<i>110</i>&nbsp;
<b class="fc"><i>111</i>&nbsp;    private static final ActionListener&lt;Void&gt; SHARD_STATE_ACTION_LISTENER = ActionListener.wrap(() -&gt; {});</b>
<i>112</i>&nbsp;
<i>113</i>&nbsp;    private final Settings settings;
<i>114</i>&nbsp;    // a list of shards that failed during recovery
<i>115</i>&nbsp;    // we keep track of these shards in order to prevent repeated recovery of these shards on each cluster state update
<b class="fc"><i>116</i>&nbsp;    final ConcurrentMap&lt;ShardId, ShardRouting&gt; failedShardsCache = ConcurrentCollections.newConcurrentMap();</b>
<i>117</i>&nbsp;    private final RepositoriesService repositoriesService;
<i>118</i>&nbsp;
<b class="fc"><i>119</i>&nbsp;    private final FailedShardHandler failedShardHandler = new FailedShardHandler();</b>
<i>120</i>&nbsp;
<i>121</i>&nbsp;    private final boolean sendRefreshMapping;
<i>122</i>&nbsp;    private final List&lt;IndexEventListener&gt; buildInIndexListener;
<i>123</i>&nbsp;    private final PrimaryReplicaSyncer primaryReplicaSyncer;
<i>124</i>&nbsp;    private final Consumer&lt;ShardId&gt; globalCheckpointSyncer;
<i>125</i>&nbsp;    private final RetentionLeaseSyncer retentionLeaseSyncer;
<i>126</i>&nbsp;
<i>127</i>&nbsp;    @Inject
<i>128</i>&nbsp;    public IndicesClusterStateService(
<i>129</i>&nbsp;            final Settings settings,
<i>130</i>&nbsp;            final IndicesService indicesService,
<i>131</i>&nbsp;            final ClusterService clusterService,
<i>132</i>&nbsp;            final ThreadPool threadPool,
<i>133</i>&nbsp;            final PeerRecoveryTargetService recoveryTargetService,
<i>134</i>&nbsp;            final ShardStateAction shardStateAction,
<i>135</i>&nbsp;            final NodeMappingRefreshAction nodeMappingRefreshAction,
<i>136</i>&nbsp;            final RepositoriesService repositoriesService,
<i>137</i>&nbsp;            final SearchService searchService,
<i>138</i>&nbsp;            final SyncedFlushService syncedFlushService,
<i>139</i>&nbsp;            final PeerRecoverySourceService peerRecoverySourceService,
<i>140</i>&nbsp;            final SnapshotShardsService snapshotShardsService,
<i>141</i>&nbsp;            final PrimaryReplicaSyncer primaryReplicaSyncer,
<i>142</i>&nbsp;            final GlobalCheckpointSyncAction globalCheckpointSyncAction,
<i>143</i>&nbsp;            final RetentionLeaseSyncAction retentionLeaseSyncAction,
<i>144</i>&nbsp;            final RetentionLeaseBackgroundSyncAction retentionLeaseBackgroundSyncAction) {
<b class="fc"><i>145</i>&nbsp;        this(</b>
<i>146</i>&nbsp;                settings,
<i>147</i>&nbsp;                (AllocatedIndices&lt;? extends Shard, ? extends AllocatedIndex&lt;? extends Shard&gt;&gt;) indicesService,
<i>148</i>&nbsp;                clusterService,
<i>149</i>&nbsp;                threadPool,
<i>150</i>&nbsp;                recoveryTargetService,
<i>151</i>&nbsp;                shardStateAction,
<i>152</i>&nbsp;                nodeMappingRefreshAction,
<i>153</i>&nbsp;                repositoriesService,
<i>154</i>&nbsp;                searchService,
<i>155</i>&nbsp;                syncedFlushService,
<i>156</i>&nbsp;                peerRecoverySourceService,
<i>157</i>&nbsp;                snapshotShardsService,
<i>158</i>&nbsp;                primaryReplicaSyncer,
<b class="fc"><i>159</i>&nbsp;                globalCheckpointSyncAction::updateGlobalCheckpointForShard,</b>
<b class="fc"><i>160</i>&nbsp;                new RetentionLeaseSyncer() {</b>
<i>161</i>&nbsp;                    @Override
<i>162</i>&nbsp;                    public void sync(
<i>163</i>&nbsp;                            final ShardId shardId,
<i>164</i>&nbsp;                            final RetentionLeases retentionLeases,
<i>165</i>&nbsp;                            final ActionListener&lt;ReplicationResponse&gt; listener) {
<b class="nc"><i>166</i>&nbsp;                        Objects.requireNonNull(retentionLeaseSyncAction).sync(shardId, retentionLeases, listener);</b>
<b class="nc"><i>167</i>&nbsp;                    }</b>
<i>168</i>&nbsp;
<i>169</i>&nbsp;                    @Override
<i>170</i>&nbsp;                    public void backgroundSync(final ShardId shardId, final RetentionLeases retentionLeases) {
<b class="nc"><i>171</i>&nbsp;                        Objects.requireNonNull(retentionLeaseBackgroundSyncAction).backgroundSync(shardId, retentionLeases);</b>
<b class="nc"><i>172</i>&nbsp;                    }</b>
<i>173</i>&nbsp;                });
<b class="fc"><i>174</i>&nbsp;    }</b>
<i>175</i>&nbsp;
<i>176</i>&nbsp;    // for tests
<i>177</i>&nbsp;    IndicesClusterStateService(
<i>178</i>&nbsp;            final Settings settings,
<i>179</i>&nbsp;            final AllocatedIndices&lt;? extends Shard, ? extends AllocatedIndex&lt;? extends Shard&gt;&gt; indicesService,
<i>180</i>&nbsp;            final ClusterService clusterService,
<i>181</i>&nbsp;            final ThreadPool threadPool,
<i>182</i>&nbsp;            final PeerRecoveryTargetService recoveryTargetService,
<i>183</i>&nbsp;            final ShardStateAction shardStateAction,
<i>184</i>&nbsp;            final NodeMappingRefreshAction nodeMappingRefreshAction,
<i>185</i>&nbsp;            final RepositoriesService repositoriesService,
<i>186</i>&nbsp;            final SearchService searchService,
<i>187</i>&nbsp;            final SyncedFlushService syncedFlushService,
<i>188</i>&nbsp;            final PeerRecoverySourceService peerRecoverySourceService,
<i>189</i>&nbsp;            final SnapshotShardsService snapshotShardsService,
<i>190</i>&nbsp;            final PrimaryReplicaSyncer primaryReplicaSyncer,
<i>191</i>&nbsp;            final Consumer&lt;ShardId&gt; globalCheckpointSyncer,
<b class="fc"><i>192</i>&nbsp;            final RetentionLeaseSyncer retentionLeaseSyncer) {</b>
<b class="fc"><i>193</i>&nbsp;        this.settings = settings;</b>
<b class="fc"><i>194</i>&nbsp;        this.buildInIndexListener =</b>
<b class="fc"><i>195</i>&nbsp;                Arrays.asList(</b>
<i>196</i>&nbsp;                        peerRecoverySourceService,
<i>197</i>&nbsp;                        recoveryTargetService,
<i>198</i>&nbsp;                        searchService,
<i>199</i>&nbsp;                        syncedFlushService,
<i>200</i>&nbsp;                        snapshotShardsService);
<b class="fc"><i>201</i>&nbsp;        this.indicesService = indicesService;</b>
<b class="fc"><i>202</i>&nbsp;        this.clusterService = clusterService;</b>
<b class="fc"><i>203</i>&nbsp;        this.threadPool = threadPool;</b>
<b class="fc"><i>204</i>&nbsp;        this.recoveryTargetService = recoveryTargetService;</b>
<b class="fc"><i>205</i>&nbsp;        this.shardStateAction = shardStateAction;</b>
<b class="fc"><i>206</i>&nbsp;        this.nodeMappingRefreshAction = nodeMappingRefreshAction;</b>
<b class="fc"><i>207</i>&nbsp;        this.repositoriesService = repositoriesService;</b>
<b class="fc"><i>208</i>&nbsp;        this.primaryReplicaSyncer = primaryReplicaSyncer;</b>
<b class="fc"><i>209</i>&nbsp;        this.globalCheckpointSyncer = globalCheckpointSyncer;</b>
<b class="fc"><i>210</i>&nbsp;        this.retentionLeaseSyncer = Objects.requireNonNull(retentionLeaseSyncer);</b>
<b class="fc"><i>211</i>&nbsp;        this.sendRefreshMapping = settings.getAsBoolean(&quot;indices.cluster.send_refresh_mapping&quot;, true);</b>
<b class="fc"><i>212</i>&nbsp;    }</b>
<i>213</i>&nbsp;
<i>214</i>&nbsp;    @Override
<i>215</i>&nbsp;    protected void doStart() {
<i>216</i>&nbsp;        // Doesn&#39;t make sense to manage shards on non-master and non-data nodes
<b class="fc"><i>217</i>&nbsp;        if (DiscoveryNode.isDataNode(settings) || DiscoveryNode.isMasterNode(settings)) {</b>
<b class="fc"><i>218</i>&nbsp;            clusterService.addHighPriorityApplier(this);</b>
<i>219</i>&nbsp;        }
<b class="fc"><i>220</i>&nbsp;    }</b>
<i>221</i>&nbsp;
<i>222</i>&nbsp;    @Override
<i>223</i>&nbsp;    protected void doStop() {
<b class="fc"><i>224</i>&nbsp;        if (DiscoveryNode.isDataNode(settings) || DiscoveryNode.isMasterNode(settings)) {</b>
<b class="fc"><i>225</i>&nbsp;            clusterService.removeApplier(this);</b>
<i>226</i>&nbsp;        }
<b class="fc"><i>227</i>&nbsp;    }</b>
<i>228</i>&nbsp;
<i>229</i>&nbsp;    @Override
<i>230</i>&nbsp;    protected void doClose() {
<b class="fc"><i>231</i>&nbsp;    }</b>
<i>232</i>&nbsp;
<i>233</i>&nbsp;    @Override
<i>234</i>&nbsp;    public synchronized void applyClusterState(final ClusterChangedEvent event) {
<b class="fc"><i>235</i>&nbsp;        if (!lifecycle.started()) {</b>
<b class="nc"><i>236</i>&nbsp;            return;</b>
<i>237</i>&nbsp;        }
<i>238</i>&nbsp;
<b class="fc"><i>239</i>&nbsp;        final ClusterState state = event.state();</b>
<i>240</i>&nbsp;
<i>241</i>&nbsp;        // we need to clean the shards and indices we have on this node, since we
<i>242</i>&nbsp;        // are going to recover them again once state persistence is disabled (no master / not recovered)
<i>243</i>&nbsp;        // TODO: feels hacky, a block disables state persistence, and then we clean the allocated shards, maybe another flag in blocks?
<b class="fc"><i>244</i>&nbsp;        if (state.blocks().disableStatePersistence()) {</b>
<b class="fc"><i>245</i>&nbsp;            for (AllocatedIndex&lt;? extends Shard&gt; indexService : indicesService) {</b>
<b class="nc"><i>246</i>&nbsp;                indicesService.removeIndex(indexService.index(), NO_LONGER_ASSIGNED,</b>
<i>247</i>&nbsp;                    &quot;cleaning index (disabled block persistence)&quot;); // also cleans shards
<b class="nc"><i>248</i>&nbsp;            }</b>
<b class="fc"><i>249</i>&nbsp;            return;</b>
<i>250</i>&nbsp;        }
<i>251</i>&nbsp;
<b class="fc"><i>252</i>&nbsp;        updateFailedShardsCache(state);</b>
<i>253</i>&nbsp;
<b class="fc"><i>254</i>&nbsp;        deleteIndices(event); // also deletes shards of deleted indices</b>
<i>255</i>&nbsp;
<b class="fc"><i>256</i>&nbsp;        removeIndices(event); // also removes shards of removed indices</b>
<i>257</i>&nbsp;
<b class="fc"><i>258</i>&nbsp;        failMissingShards(state);</b>
<i>259</i>&nbsp;
<b class="fc"><i>260</i>&nbsp;        removeShards(state);   // removes any local shards that doesn&#39;t match what the master expects</b>
<i>261</i>&nbsp;
<b class="fc"><i>262</i>&nbsp;        updateIndices(event); // can also fail shards, but these are then guaranteed to be in failedShardsCache</b>
<i>263</i>&nbsp;
<b class="fc"><i>264</i>&nbsp;        createIndices(state);</b>
<i>265</i>&nbsp;
<b class="fc"><i>266</i>&nbsp;        createOrUpdateShards(state);</b>
<b class="fc"><i>267</i>&nbsp;    }</b>
<i>268</i>&nbsp;
<i>269</i>&nbsp;    /**
<i>270</i>&nbsp;     * Removes shard entries from the failed shards cache that are no longer allocated to this node by the master.
<i>271</i>&nbsp;     * Sends shard failures for shards that are marked as actively allocated to this node but don&#39;t actually exist on the node.
<i>272</i>&nbsp;     * Resends shard failures for shards that are still marked as allocated to this node but previously failed.
<i>273</i>&nbsp;     *
<i>274</i>&nbsp;     * @param state new cluster state
<i>275</i>&nbsp;     */
<i>276</i>&nbsp;    private void updateFailedShardsCache(final ClusterState state) {
<b class="fc"><i>277</i>&nbsp;        RoutingNode localRoutingNode = state.getRoutingNodes().node(state.nodes().getLocalNodeId());</b>
<b class="fc"><i>278</i>&nbsp;        if (localRoutingNode == null) {</b>
<b class="nc"><i>279</i>&nbsp;            failedShardsCache.clear();</b>
<b class="nc"><i>280</i>&nbsp;            return;</b>
<i>281</i>&nbsp;        }
<i>282</i>&nbsp;
<b class="fc"><i>283</i>&nbsp;        DiscoveryNode masterNode = state.nodes().getMasterNode();</b>
<i>284</i>&nbsp;
<i>285</i>&nbsp;        // remove items from cache which are not in our routing table anymore and resend failures that have not executed on master yet
<b class="fc"><i>286</i>&nbsp;        for (Iterator&lt;Map.Entry&lt;ShardId, ShardRouting&gt;&gt; iterator = failedShardsCache.entrySet().iterator(); iterator.hasNext(); ) {</b>
<b class="nc"><i>287</i>&nbsp;            ShardRouting failedShardRouting = iterator.next().getValue();</b>
<b class="nc"><i>288</i>&nbsp;            ShardRouting matchedRouting = localRoutingNode.getByShardId(failedShardRouting.shardId());</b>
<b class="nc"><i>289</i>&nbsp;            if (matchedRouting == null || matchedRouting.isSameAllocation(failedShardRouting) == false) {</b>
<b class="nc"><i>290</i>&nbsp;                iterator.remove();</b>
<i>291</i>&nbsp;            } else {
<b class="nc"><i>292</i>&nbsp;                if (masterNode != null) { // TODO: can we remove this? Is resending shard failures the responsibility of shardStateAction?</b>
<b class="nc"><i>293</i>&nbsp;                    String message = &quot;master &quot; + masterNode + &quot; has not removed previously failed shard. resending shard failure&quot;;</b>
<b class="nc"><i>294</i>&nbsp;                    logger.trace(&quot;[{}] re-sending failed shard [{}], reason [{}]&quot;, matchedRouting.shardId(), matchedRouting, message);</b>
<b class="nc"><i>295</i>&nbsp;                    shardStateAction.localShardFailed(matchedRouting, message, null, SHARD_STATE_ACTION_LISTENER, state);</b>
<i>296</i>&nbsp;                }
<i>297</i>&nbsp;            }
<b class="nc"><i>298</i>&nbsp;        }</b>
<b class="fc"><i>299</i>&nbsp;    }</b>
<i>300</i>&nbsp;
<i>301</i>&nbsp;    /**
<i>302</i>&nbsp;     * Deletes indices (with shard data).
<i>303</i>&nbsp;     *
<i>304</i>&nbsp;     * @param event cluster change event
<i>305</i>&nbsp;     */
<i>306</i>&nbsp;    private void deleteIndices(final ClusterChangedEvent event) {
<b class="fc"><i>307</i>&nbsp;        final ClusterState previousState = event.previousState();</b>
<b class="fc"><i>308</i>&nbsp;        final ClusterState state = event.state();</b>
<b class="fc"><i>309</i>&nbsp;        final String localNodeId = state.nodes().getLocalNodeId();</b>
<b class="fc"><i>310</i>&nbsp;        assert localNodeId != null;</b>
<i>311</i>&nbsp;
<b class="fc"><i>312</i>&nbsp;        for (Index index : event.indicesDeleted()) {</b>
<b class="fc"><i>313</i>&nbsp;            if (logger.isDebugEnabled()) {</b>
<b class="nc"><i>314</i>&nbsp;                logger.debug(&quot;[{}] cleaning index, no longer part of the metadata&quot;, index);</b>
<i>315</i>&nbsp;            }
<b class="fc"><i>316</i>&nbsp;            AllocatedIndex&lt;? extends Shard&gt; indexService = indicesService.indexService(index);</b>
<i>317</i>&nbsp;            final IndexSettings indexSettings;
<b class="fc"><i>318</i>&nbsp;            if (indexService != null) {</b>
<b class="fc"><i>319</i>&nbsp;                indexSettings = indexService.getIndexSettings();</b>
<b class="fc"><i>320</i>&nbsp;                indicesService.removeIndex(index, DELETED, &quot;index no longer part of the metadata&quot;);</b>
<b class="nc"><i>321</i>&nbsp;            } else if (previousState.metaData().hasIndex(index.getName())) {</b>
<i>322</i>&nbsp;                // The deleted index was part of the previous cluster state, but not loaded on the local node
<b class="nc"><i>323</i>&nbsp;                final IndexMetaData metaData = previousState.metaData().index(index);</b>
<b class="nc"><i>324</i>&nbsp;                indexSettings = new IndexSettings(metaData, settings);</b>
<b class="nc"><i>325</i>&nbsp;                indicesService.deleteUnassignedIndex(&quot;deleted index was not assigned to local node&quot;, metaData, state);</b>
<b class="nc"><i>326</i>&nbsp;            } else {</b>
<i>327</i>&nbsp;                // The previous cluster state&#39;s metadata also does not contain the index,
<i>328</i>&nbsp;                // which is what happens on node startup when an index was deleted while the
<i>329</i>&nbsp;                // node was not part of the cluster.  In this case, try reading the index
<i>330</i>&nbsp;                // metadata from disk.  If its not there, there is nothing to delete.
<i>331</i>&nbsp;                // First, though, verify the precondition for applying this case by
<i>332</i>&nbsp;                // asserting that the previous cluster state is not initialized/recovered.
<b class="nc"><i>333</i>&nbsp;                assert previousState.blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK);</b>
<b class="nc"><i>334</i>&nbsp;                final IndexMetaData metaData = indicesService.verifyIndexIsDeleted(index, event.state());</b>
<b class="nc"><i>335</i>&nbsp;                if (metaData != null) {</b>
<b class="nc"><i>336</i>&nbsp;                    indexSettings = new IndexSettings(metaData, settings);</b>
<i>337</i>&nbsp;                } else {
<b class="nc"><i>338</i>&nbsp;                    indexSettings = null;</b>
<i>339</i>&nbsp;                }
<i>340</i>&nbsp;            }
<b class="fc"><i>341</i>&nbsp;            if (indexSettings != null) {</b>
<b class="fc"><i>342</i>&nbsp;                threadPool.generic().execute(new AbstractRunnable() {</b>
<i>343</i>&nbsp;                    @Override
<i>344</i>&nbsp;                    public void onFailure(Exception e) {
<b class="nc"><i>345</i>&nbsp;                        logger.warn(() -&gt; new ParameterizedMessage(&quot;[{}] failed to complete pending deletion for index&quot;, index), e);</b>
<b class="nc"><i>346</i>&nbsp;                    }</b>
<i>347</i>&nbsp;
<i>348</i>&nbsp;                    @Override
<i>349</i>&nbsp;                    protected void doRun() throws Exception {
<i>350</i>&nbsp;                        try {
<i>351</i>&nbsp;                            // we are waiting until we can lock the index / all shards on the node and then we ack the delete of the store
<i>352</i>&nbsp;                            // to the master. If we can&#39;t acquire the locks here immediately there might be a shard of this index still
<i>353</i>&nbsp;                            // holding on to the lock due to a &quot;currently canceled recovery&quot; or so. The shard will delete itself BEFORE the
<i>354</i>&nbsp;                            // lock is released so it&#39;s guaranteed to be deleted by the time we get the lock
<b class="fc"><i>355</i>&nbsp;                            indicesService.processPendingDeletes(index, indexSettings, new TimeValue(30, TimeUnit.MINUTES));</b>
<b class="nc"><i>356</i>&nbsp;                        } catch (ShardLockObtainFailedException exc) {</b>
<b class="nc"><i>357</i>&nbsp;                            logger.warn(&quot;[{}] failed to lock all shards for index - timed out after 30 seconds&quot;, index);</b>
<b class="nc"><i>358</i>&nbsp;                        } catch (InterruptedException e) {</b>
<b class="nc"><i>359</i>&nbsp;                            logger.warn(&quot;[{}] failed to lock all shards for index - interrupted&quot;, index);</b>
<b class="fc"><i>360</i>&nbsp;                        }</b>
<b class="fc"><i>361</i>&nbsp;                    }</b>
<i>362</i>&nbsp;                });
<i>363</i>&nbsp;            }
<b class="fc"><i>364</i>&nbsp;        }</b>
<b class="fc"><i>365</i>&nbsp;    }</b>
<i>366</i>&nbsp;
<i>367</i>&nbsp;    /**
<i>368</i>&nbsp;     * Removes indices that have no shards allocated to this node or indices whose state has changed. This does not delete the shard data
<i>369</i>&nbsp;     * as we wait for enough shard copies to exist in the cluster before deleting shard data (triggered by
<i>370</i>&nbsp;     * {@link org.elasticsearch.indices.store.IndicesStore}).
<i>371</i>&nbsp;     *
<i>372</i>&nbsp;     * @param event the cluster changed event
<i>373</i>&nbsp;     */
<i>374</i>&nbsp;    private void removeIndices(final ClusterChangedEvent event) {
<b class="fc"><i>375</i>&nbsp;        final ClusterState state = event.state();</b>
<b class="fc"><i>376</i>&nbsp;        final String localNodeId = state.nodes().getLocalNodeId();</b>
<b class="fc"><i>377</i>&nbsp;        assert localNodeId != null;</b>
<i>378</i>&nbsp;
<b class="fc"><i>379</i>&nbsp;        final Set&lt;Index&gt; indicesWithShards = new HashSet&lt;&gt;();</b>
<b class="fc"><i>380</i>&nbsp;        RoutingNode localRoutingNode = state.getRoutingNodes().node(localNodeId);</b>
<b class="fc"><i>381</i>&nbsp;        if (localRoutingNode != null) { // null e.g. if we are not a data node</b>
<b class="fc"><i>382</i>&nbsp;            for (ShardRouting shardRouting : localRoutingNode) {</b>
<b class="fc"><i>383</i>&nbsp;                indicesWithShards.add(shardRouting.index());</b>
<b class="fc"><i>384</i>&nbsp;            }</b>
<i>385</i>&nbsp;        }
<i>386</i>&nbsp;
<b class="fc"><i>387</i>&nbsp;        for (AllocatedIndex&lt;? extends Shard&gt; indexService : indicesService) {</b>
<b class="fc"><i>388</i>&nbsp;            final Index index = indexService.index();</b>
<b class="fc"><i>389</i>&nbsp;            final IndexMetaData indexMetaData = state.metaData().index(index);</b>
<b class="fc"><i>390</i>&nbsp;            final IndexMetaData existingMetaData = indexService.getIndexSettings().getIndexMetaData();</b>
<i>391</i>&nbsp;
<b class="fc"><i>392</i>&nbsp;            AllocatedIndices.IndexRemovalReason reason = null;</b>
<b class="fc"><i>393</i>&nbsp;            if (indexMetaData != null &amp;&amp; indexMetaData.getState() != existingMetaData.getState()) {</b>
<b class="nc"><i>394</i>&nbsp;                reason = indexMetaData.getState() == IndexMetaData.State.CLOSE ? CLOSED : REOPENED;</b>
<b class="fc"><i>395</i>&nbsp;            } else if (indicesWithShards.contains(index) == false) {</b>
<i>396</i>&nbsp;                // if the cluster change indicates a brand new cluster, we only want
<i>397</i>&nbsp;                // to remove the in-memory structures for the index and not delete the
<i>398</i>&nbsp;                // contents on disk because the index will later be re-imported as a
<i>399</i>&nbsp;                // dangling index
<b class="nc"><i>400</i>&nbsp;                assert indexMetaData != null || event.isNewCluster() :</b>
<i>401</i>&nbsp;                    &quot;index &quot; + index + &quot; does not exist in the cluster state, it should either &quot; +
<i>402</i>&nbsp;                        &quot;have been deleted or the cluster must be new&quot;;
<b class="nc"><i>403</i>&nbsp;                reason = indexMetaData != null &amp;&amp; indexMetaData.getState() == IndexMetaData.State.CLOSE ? CLOSED : NO_LONGER_ASSIGNED;</b>
<i>404</i>&nbsp;            }
<i>405</i>&nbsp;
<b class="fc"><i>406</i>&nbsp;            if (reason != null) {</b>
<b class="nc"><i>407</i>&nbsp;                logger.debug(&quot;{} removing index ({})&quot;, index, reason);</b>
<b class="nc"><i>408</i>&nbsp;                indicesService.removeIndex(index, reason, &quot;removing index (&quot; + reason + &quot;)&quot;);</b>
<i>409</i>&nbsp;            }
<b class="fc"><i>410</i>&nbsp;        }</b>
<b class="fc"><i>411</i>&nbsp;    }</b>
<i>412</i>&nbsp;
<i>413</i>&nbsp;    /**
<i>414</i>&nbsp;     * Notifies master about shards that don&#39;t exist but are supposed to be active on this node.
<i>415</i>&nbsp;     *
<i>416</i>&nbsp;     * @param state new cluster state
<i>417</i>&nbsp;     */
<i>418</i>&nbsp;    private void failMissingShards(final ClusterState state) {
<b class="fc"><i>419</i>&nbsp;        RoutingNode localRoutingNode = state.getRoutingNodes().node(state.nodes().getLocalNodeId());</b>
<b class="fc"><i>420</i>&nbsp;        if (localRoutingNode == null) {</b>
<b class="nc"><i>421</i>&nbsp;            return;</b>
<i>422</i>&nbsp;        }
<b class="fc"><i>423</i>&nbsp;        for (final ShardRouting shardRouting : localRoutingNode) {</b>
<b class="fc"><i>424</i>&nbsp;            ShardId shardId = shardRouting.shardId();</b>
<b class="fc"><i>425</i>&nbsp;            if (shardRouting.initializing() == false &amp;&amp;</b>
<b class="fc"><i>426</i>&nbsp;                failedShardsCache.containsKey(shardId) == false &amp;&amp;</b>
<b class="fc"><i>427</i>&nbsp;                indicesService.getShardOrNull(shardId) == null) {</b>
<i>428</i>&nbsp;                // the master thinks we are active, but we don&#39;t have this shard at all, mark it as failed
<b class="nc"><i>429</i>&nbsp;                sendFailShard(shardRouting, &quot;master marked shard as active, but shard has not been created, mark shard as failed&quot;, null,</b>
<i>430</i>&nbsp;                    state);
<i>431</i>&nbsp;            }
<b class="fc"><i>432</i>&nbsp;        }</b>
<b class="fc"><i>433</i>&nbsp;    }</b>
<i>434</i>&nbsp;
<i>435</i>&nbsp;    /**
<i>436</i>&nbsp;     * Removes shards that are currently loaded by indicesService but have disappeared from the routing table of the current node.
<i>437</i>&nbsp;     * This method does not delete the shard data.
<i>438</i>&nbsp;     *
<i>439</i>&nbsp;     * @param state new cluster state
<i>440</i>&nbsp;     */
<i>441</i>&nbsp;    private void removeShards(final ClusterState state) {
<b class="fc"><i>442</i>&nbsp;        final String localNodeId = state.nodes().getLocalNodeId();</b>
<b class="fc"><i>443</i>&nbsp;        assert localNodeId != null;</b>
<i>444</i>&nbsp;
<i>445</i>&nbsp;        // remove shards based on routing nodes (no deletion of data)
<b class="fc"><i>446</i>&nbsp;        RoutingNode localRoutingNode = state.getRoutingNodes().node(localNodeId);</b>
<b class="fc"><i>447</i>&nbsp;        for (AllocatedIndex&lt;? extends Shard&gt; indexService : indicesService) {</b>
<b class="fc"><i>448</i>&nbsp;            for (Shard shard : indexService) {</b>
<b class="fc"><i>449</i>&nbsp;                ShardRouting currentRoutingEntry = shard.routingEntry();</b>
<b class="fc"><i>450</i>&nbsp;                ShardId shardId = currentRoutingEntry.shardId();</b>
<b class="fc"><i>451</i>&nbsp;                ShardRouting newShardRouting = localRoutingNode == null ? null : localRoutingNode.getByShardId(shardId);</b>
<b class="fc"><i>452</i>&nbsp;                if (newShardRouting == null) {</b>
<i>453</i>&nbsp;                    // we can just remove the shard without cleaning it locally, since we will clean it in IndicesStore
<i>454</i>&nbsp;                    // once all shards are allocated
<b class="nc"><i>455</i>&nbsp;                    logger.debug(&quot;{} removing shard (not allocated)&quot;, shardId);</b>
<b class="nc"><i>456</i>&nbsp;                    indexService.removeShard(shardId.id(), &quot;removing shard (not allocated)&quot;);</b>
<b class="fc"><i>457</i>&nbsp;                } else if (newShardRouting.isSameAllocation(currentRoutingEntry) == false) {</b>
<b class="nc"><i>458</i>&nbsp;                    logger.debug(&quot;{} removing shard (stale allocation id, stale {}, new {})&quot;, shardId,</b>
<i>459</i>&nbsp;                        currentRoutingEntry, newShardRouting);
<b class="nc"><i>460</i>&nbsp;                    indexService.removeShard(shardId.id(), &quot;removing shard (stale copy)&quot;);</b>
<b class="fc"><i>461</i>&nbsp;                } else if (newShardRouting.initializing() &amp;&amp; currentRoutingEntry.active()) {</b>
<i>462</i>&nbsp;                    // this can happen if the node was isolated/gc-ed, rejoins the cluster and a new shard with the same allocation id
<i>463</i>&nbsp;                    // is assigned to it. Batch cluster state processing or if shard fetching completes before the node gets a new cluster
<i>464</i>&nbsp;                    // state may result in a new shard being initialized while having the same allocation id as the currently started shard.
<b class="nc"><i>465</i>&nbsp;                    logger.debug(&quot;{} removing shard (not active, current {}, new {})&quot;, shardId, currentRoutingEntry, newShardRouting);</b>
<b class="nc"><i>466</i>&nbsp;                    indexService.removeShard(shardId.id(), &quot;removing shard (stale copy)&quot;);</b>
<b class="fc"><i>467</i>&nbsp;                } else if (newShardRouting.primary() &amp;&amp; currentRoutingEntry.primary() == false &amp;&amp; newShardRouting.initializing()) {</b>
<b class="nc"><i>468</i>&nbsp;                    assert currentRoutingEntry.initializing() : currentRoutingEntry; // see above if clause</b>
<i>469</i>&nbsp;                    // this can happen when cluster state batching batches activation of the shard, closing an index, reopening it
<i>470</i>&nbsp;                    // and assigning an initializing primary to this node
<b class="nc"><i>471</i>&nbsp;                    logger.debug(&quot;{} removing shard (not active, current {}, new {})&quot;, shardId, currentRoutingEntry, newShardRouting);</b>
<b class="nc"><i>472</i>&nbsp;                    indexService.removeShard(shardId.id(), &quot;removing shard (stale copy)&quot;);</b>
<i>473</i>&nbsp;                }
<b class="fc"><i>474</i>&nbsp;            }</b>
<b class="fc"><i>475</i>&nbsp;        }</b>
<b class="fc"><i>476</i>&nbsp;    }</b>
<i>477</i>&nbsp;
<i>478</i>&nbsp;    private void createIndices(final ClusterState state) {
<i>479</i>&nbsp;        // we only create indices for shards that are allocated
<b class="fc"><i>480</i>&nbsp;        RoutingNode localRoutingNode = state.getRoutingNodes().node(state.nodes().getLocalNodeId());</b>
<b class="fc"><i>481</i>&nbsp;        if (localRoutingNode == null) {</b>
<b class="nc"><i>482</i>&nbsp;            return;</b>
<i>483</i>&nbsp;        }
<i>484</i>&nbsp;        // create map of indices to create with shards to fail if index creation fails
<b class="fc"><i>485</i>&nbsp;        final Map&lt;Index, List&lt;ShardRouting&gt;&gt; indicesToCreate = new HashMap&lt;&gt;();</b>
<b class="fc"><i>486</i>&nbsp;        for (ShardRouting shardRouting : localRoutingNode) {</b>
<b class="fc"><i>487</i>&nbsp;            if (failedShardsCache.containsKey(shardRouting.shardId()) == false) {</b>
<b class="fc"><i>488</i>&nbsp;                final Index index = shardRouting.index();</b>
<b class="fc"><i>489</i>&nbsp;                if (indicesService.indexService(index) == null) {</b>
<b class="fc"><i>490</i>&nbsp;                    indicesToCreate.computeIfAbsent(index, k -&gt; new ArrayList&lt;&gt;()).add(shardRouting);</b>
<i>491</i>&nbsp;                }
<i>492</i>&nbsp;            }
<b class="fc"><i>493</i>&nbsp;        }</b>
<i>494</i>&nbsp;
<b class="fc"><i>495</i>&nbsp;        for (Map.Entry&lt;Index, List&lt;ShardRouting&gt;&gt; entry : indicesToCreate.entrySet()) {</b>
<b class="fc"><i>496</i>&nbsp;            final Index index = entry.getKey();</b>
<b class="fc"><i>497</i>&nbsp;            final IndexMetaData indexMetaData = state.metaData().index(index);</b>
<b class="fc"><i>498</i>&nbsp;            logger.debug(&quot;[{}] creating index&quot;, index);</b>
<i>499</i>&nbsp;
<b class="fc"><i>500</i>&nbsp;            AllocatedIndex&lt;? extends Shard&gt; indexService = null;</b>
<i>501</i>&nbsp;            try {
<b class="fc"><i>502</i>&nbsp;                indexService = indicesService.createIndex(indexMetaData, buildInIndexListener);</b>
<b class="fc"><i>503</i>&nbsp;                if (indexService.updateMapping(null, indexMetaData) &amp;&amp; sendRefreshMapping) {</b>
<b class="nc"><i>504</i>&nbsp;                    nodeMappingRefreshAction.nodeMappingRefresh(state.nodes().getMasterNode(),</b>
<b class="nc"><i>505</i>&nbsp;                        new NodeMappingRefreshAction.NodeMappingRefreshRequest(indexMetaData.getIndex().getName(),</b>
<b class="nc"><i>506</i>&nbsp;                            indexMetaData.getIndexUUID(), state.nodes().getLocalNodeId())</b>
<i>507</i>&nbsp;                    );
<i>508</i>&nbsp;                }
<b class="nc"><i>509</i>&nbsp;            } catch (Exception e) {</b>
<i>510</i>&nbsp;                final String failShardReason;
<b class="nc"><i>511</i>&nbsp;                if (indexService == null) {</b>
<b class="nc"><i>512</i>&nbsp;                    failShardReason = &quot;failed to create index&quot;;</b>
<i>513</i>&nbsp;                } else {
<b class="nc"><i>514</i>&nbsp;                    failShardReason = &quot;failed to update mapping for index&quot;;</b>
<b class="nc"><i>515</i>&nbsp;                    indicesService.removeIndex(index, FAILURE, &quot;removing index (mapping update failed)&quot;);</b>
<i>516</i>&nbsp;                }
<b class="nc"><i>517</i>&nbsp;                for (ShardRouting shardRouting : entry.getValue()) {</b>
<b class="nc"><i>518</i>&nbsp;                    sendFailShard(shardRouting, failShardReason, e, state);</b>
<b class="nc"><i>519</i>&nbsp;                }</b>
<b class="fc"><i>520</i>&nbsp;            }</b>
<b class="fc"><i>521</i>&nbsp;        }</b>
<b class="fc"><i>522</i>&nbsp;    }</b>
<i>523</i>&nbsp;
<i>524</i>&nbsp;    private void updateIndices(ClusterChangedEvent event) {
<b class="fc"><i>525</i>&nbsp;        if (!event.metaDataChanged()) {</b>
<b class="nc"><i>526</i>&nbsp;            return;</b>
<i>527</i>&nbsp;        }
<b class="fc"><i>528</i>&nbsp;        final ClusterState state = event.state();</b>
<b class="fc"><i>529</i>&nbsp;        for (AllocatedIndex&lt;? extends Shard&gt; indexService : indicesService) {</b>
<b class="fc"><i>530</i>&nbsp;            final Index index = indexService.index();</b>
<b class="fc"><i>531</i>&nbsp;            final IndexMetaData currentIndexMetaData = indexService.getIndexSettings().getIndexMetaData();</b>
<b class="fc"><i>532</i>&nbsp;            final IndexMetaData newIndexMetaData = state.metaData().index(index);</b>
<b class="fc"><i>533</i>&nbsp;            assert newIndexMetaData != null : &quot;index &quot; + index + &quot; should have been removed by deleteIndices&quot;;</b>
<b class="fc"><i>534</i>&nbsp;            if (ClusterChangedEvent.indexMetaDataChanged(currentIndexMetaData, newIndexMetaData)) {</b>
<b class="fc"><i>535</i>&nbsp;                String reason = null;</b>
<i>536</i>&nbsp;                try {
<b class="fc"><i>537</i>&nbsp;                    reason = &quot;metadata update failed&quot;;</b>
<i>538</i>&nbsp;                    try {
<b class="fc"><i>539</i>&nbsp;                        indexService.updateMetaData(currentIndexMetaData, newIndexMetaData);</b>
<b class="nc"><i>540</i>&nbsp;                    } catch (Exception e) {</b>
<b class="nc"><i>541</i>&nbsp;                        assert false : e;</b>
<b class="nc"><i>542</i>&nbsp;                        throw e;</b>
<b class="fc"><i>543</i>&nbsp;                    }</b>
<i>544</i>&nbsp;
<b class="fc"><i>545</i>&nbsp;                    reason = &quot;mapping update failed&quot;;</b>
<b class="fc"><i>546</i>&nbsp;                    if (indexService.updateMapping(currentIndexMetaData, newIndexMetaData) &amp;&amp; sendRefreshMapping) {</b>
<b class="nc"><i>547</i>&nbsp;                        nodeMappingRefreshAction.nodeMappingRefresh(state.nodes().getMasterNode(),</b>
<b class="nc"><i>548</i>&nbsp;                            new NodeMappingRefreshAction.NodeMappingRefreshRequest(newIndexMetaData.getIndex().getName(),</b>
<b class="nc"><i>549</i>&nbsp;                                newIndexMetaData.getIndexUUID(), state.nodes().getLocalNodeId())</b>
<i>550</i>&nbsp;                        );
<i>551</i>&nbsp;                    }
<b class="nc"><i>552</i>&nbsp;                } catch (Exception e) {</b>
<b class="nc"><i>553</i>&nbsp;                    indicesService.removeIndex(indexService.index(), FAILURE, &quot;removing index (&quot; + reason + &quot;)&quot;);</b>
<i>554</i>&nbsp;
<i>555</i>&nbsp;                    // fail shards that would be created or updated by createOrUpdateShards
<b class="nc"><i>556</i>&nbsp;                    RoutingNode localRoutingNode = state.getRoutingNodes().node(state.nodes().getLocalNodeId());</b>
<b class="nc"><i>557</i>&nbsp;                    if (localRoutingNode != null) {</b>
<b class="nc"><i>558</i>&nbsp;                        for (final ShardRouting shardRouting : localRoutingNode) {</b>
<b class="nc"><i>559</i>&nbsp;                            if (shardRouting.index().equals(index) &amp;&amp; failedShardsCache.containsKey(shardRouting.shardId()) == false) {</b>
<b class="nc"><i>560</i>&nbsp;                                sendFailShard(shardRouting, &quot;failed to update index (&quot; + reason + &quot;)&quot;, e, state);</b>
<i>561</i>&nbsp;                            }
<b class="nc"><i>562</i>&nbsp;                        }</b>
<i>563</i>&nbsp;                    }
<b class="fc"><i>564</i>&nbsp;                }</b>
<i>565</i>&nbsp;            }
<b class="fc"><i>566</i>&nbsp;        }</b>
<b class="fc"><i>567</i>&nbsp;    }</b>
<i>568</i>&nbsp;
<i>569</i>&nbsp;    private void createOrUpdateShards(final ClusterState state) {
<b class="fc"><i>570</i>&nbsp;        RoutingNode localRoutingNode = state.getRoutingNodes().node(state.nodes().getLocalNodeId());</b>
<b class="fc"><i>571</i>&nbsp;        if (localRoutingNode == null) {</b>
<b class="nc"><i>572</i>&nbsp;            return;</b>
<i>573</i>&nbsp;        }
<i>574</i>&nbsp;
<b class="fc"><i>575</i>&nbsp;        DiscoveryNodes nodes = state.nodes();</b>
<b class="fc"><i>576</i>&nbsp;        RoutingTable routingTable = state.routingTable();</b>
<i>577</i>&nbsp;
<b class="fc"><i>578</i>&nbsp;        for (final ShardRouting shardRouting : localRoutingNode) {</b>
<b class="fc"><i>579</i>&nbsp;            ShardId shardId = shardRouting.shardId();</b>
<b class="fc"><i>580</i>&nbsp;            if (failedShardsCache.containsKey(shardId) == false) {</b>
<b class="fc"><i>581</i>&nbsp;                AllocatedIndex&lt;? extends Shard&gt; indexService = indicesService.indexService(shardId.getIndex());</b>
<b class="fc"><i>582</i>&nbsp;                assert indexService != null : &quot;index &quot; + shardId.getIndex() + &quot; should have been created by createIndices&quot;;</b>
<b class="fc"><i>583</i>&nbsp;                Shard shard = indexService.getShardOrNull(shardId.id());</b>
<b class="fc"><i>584</i>&nbsp;                if (shard == null) {</b>
<b class="fc"><i>585</i>&nbsp;                    assert shardRouting.initializing() : shardRouting + &quot; should have been removed by failMissingShards&quot;;</b>
<b class="fc"><i>586</i>&nbsp;                    createShard(nodes, routingTable, shardRouting, state);</b>
<i>587</i>&nbsp;                } else {
<b class="fc"><i>588</i>&nbsp;                    updateShard(nodes, shardRouting, shard, routingTable, state);</b>
<i>589</i>&nbsp;                }
<i>590</i>&nbsp;            }
<b class="fc"><i>591</i>&nbsp;        }</b>
<b class="fc"><i>592</i>&nbsp;    }</b>
<i>593</i>&nbsp;
<i>594</i>&nbsp;    private void createShard(DiscoveryNodes nodes, RoutingTable routingTable, ShardRouting shardRouting, ClusterState state) {
<b class="fc"><i>595</i>&nbsp;        assert shardRouting.initializing() : &quot;only allow shard creation for initializing shard but was &quot; + shardRouting;</b>
<i>596</i>&nbsp;
<b class="fc"><i>597</i>&nbsp;        DiscoveryNode sourceNode = null;</b>
<b class="fc"><i>598</i>&nbsp;        if (shardRouting.recoverySource().getType() == Type.PEER)  {</b>
<b class="nc"><i>599</i>&nbsp;            sourceNode = findSourceNodeForPeerRecovery(logger, routingTable, nodes, shardRouting);</b>
<b class="nc"><i>600</i>&nbsp;            if (sourceNode == null) {</b>
<b class="nc"><i>601</i>&nbsp;                logger.trace(&quot;ignoring initializing shard {} - no source node can be found.&quot;, shardRouting.shardId());</b>
<b class="nc"><i>602</i>&nbsp;                return;</b>
<i>603</i>&nbsp;            }
<i>604</i>&nbsp;        }
<i>605</i>&nbsp;
<i>606</i>&nbsp;        try {
<b class="fc"><i>607</i>&nbsp;            final long primaryTerm = state.metaData().index(shardRouting.index()).primaryTerm(shardRouting.id());</b>
<b class="fc"><i>608</i>&nbsp;            logger.debug(&quot;{} creating shard with primary term [{}]&quot;, shardRouting.shardId(), primaryTerm);</b>
<b class="fc"><i>609</i>&nbsp;            RecoveryState recoveryState = new RecoveryState(shardRouting, nodes.getLocalNode(), sourceNode);</b>
<b class="fc"><i>610</i>&nbsp;            indicesService.createShard(</b>
<i>611</i>&nbsp;                    shardRouting,
<i>612</i>&nbsp;                    recoveryState,
<i>613</i>&nbsp;                    recoveryTargetService,
<i>614</i>&nbsp;                    new RecoveryListener(shardRouting, primaryTerm),
<i>615</i>&nbsp;                    repositoriesService,
<i>616</i>&nbsp;                    failedShardHandler,
<i>617</i>&nbsp;                    globalCheckpointSyncer,
<i>618</i>&nbsp;                    retentionLeaseSyncer);
<b class="nc"><i>619</i>&nbsp;        } catch (Exception e) {</b>
<b class="nc"><i>620</i>&nbsp;            failAndRemoveShard(shardRouting, true, &quot;failed to create shard&quot;, e, state);</b>
<b class="fc"><i>621</i>&nbsp;        }</b>
<b class="fc"><i>622</i>&nbsp;    }</b>
<i>623</i>&nbsp;
<i>624</i>&nbsp;    private void updateShard(DiscoveryNodes nodes, ShardRouting shardRouting, Shard shard, RoutingTable routingTable,
<i>625</i>&nbsp;                             ClusterState clusterState) {
<b class="fc"><i>626</i>&nbsp;        final ShardRouting currentRoutingEntry = shard.routingEntry();</b>
<b class="fc"><i>627</i>&nbsp;        assert currentRoutingEntry.isSameAllocation(shardRouting) :</b>
<i>628</i>&nbsp;            &quot;local shard has a different allocation id but wasn&#39;t cleaned by removeShards. &quot;
<i>629</i>&nbsp;                + &quot;cluster state: &quot; + shardRouting + &quot; local: &quot; + currentRoutingEntry;
<i>630</i>&nbsp;
<i>631</i>&nbsp;        final long primaryTerm;
<i>632</i>&nbsp;        try {
<b class="fc"><i>633</i>&nbsp;            final IndexMetaData indexMetaData = clusterState.metaData().index(shard.shardId().getIndex());</b>
<b class="fc"><i>634</i>&nbsp;            primaryTerm = indexMetaData.primaryTerm(shard.shardId().id());</b>
<b class="fc"><i>635</i>&nbsp;            final Set&lt;String&gt; inSyncIds = indexMetaData.inSyncAllocationIds(shard.shardId().id());</b>
<b class="fc"><i>636</i>&nbsp;            final IndexShardRoutingTable indexShardRoutingTable = routingTable.shardRoutingTable(shardRouting.shardId());</b>
<b class="fc"><i>637</i>&nbsp;            shard.updateShardState(shardRouting, primaryTerm, primaryReplicaSyncer::resync, clusterState.version(),</b>
<i>638</i>&nbsp;                inSyncIds, indexShardRoutingTable);
<b class="nc"><i>639</i>&nbsp;        } catch (Exception e) {</b>
<b class="nc"><i>640</i>&nbsp;            failAndRemoveShard(shardRouting, true, &quot;failed updating shard routing entry&quot;, e, clusterState);</b>
<b class="nc"><i>641</i>&nbsp;            return;</b>
<b class="fc"><i>642</i>&nbsp;        }</b>
<i>643</i>&nbsp;
<b class="fc"><i>644</i>&nbsp;        final IndexShardState state = shard.state();</b>
<b class="fc"><i>645</i>&nbsp;        if (shardRouting.initializing() &amp;&amp; (state == IndexShardState.STARTED || state == IndexShardState.POST_RECOVERY)) {</b>
<i>646</i>&nbsp;            // the master thinks we are initializing, but we are already started or on POST_RECOVERY and waiting
<i>647</i>&nbsp;            // for master to confirm a shard started message (either master failover, or a cluster event before
<i>648</i>&nbsp;            // we managed to tell the master we started), mark us as started
<b class="nc"><i>649</i>&nbsp;            if (logger.isTraceEnabled()) {</b>
<b class="nc"><i>650</i>&nbsp;                logger.trace(&quot;{} master marked shard as initializing, but shard has state [{}], resending shard started to {}&quot;,</b>
<b class="nc"><i>651</i>&nbsp;                    shardRouting.shardId(), state, nodes.getMasterNode());</b>
<i>652</i>&nbsp;            }
<b class="nc"><i>653</i>&nbsp;            if (nodes.getMasterNode() != null) {</b>
<b class="nc"><i>654</i>&nbsp;                shardStateAction.shardStarted(shardRouting, primaryTerm, &quot;master &quot; + nodes.getMasterNode() +</b>
<i>655</i>&nbsp;                        &quot; marked shard as initializing, but shard state is [&quot; + state + &quot;], mark shard as started&quot;,
<i>656</i>&nbsp;                    SHARD_STATE_ACTION_LISTENER, clusterState);
<i>657</i>&nbsp;            }
<i>658</i>&nbsp;        }
<b class="fc"><i>659</i>&nbsp;    }</b>
<i>660</i>&nbsp;
<i>661</i>&nbsp;    /**
<i>662</i>&nbsp;     * Finds the routing source node for peer recovery, return null if its not found. Note, this method expects the shard
<i>663</i>&nbsp;     * routing to *require* peer recovery, use {@link ShardRouting#recoverySource()} to check if its needed or not.
<i>664</i>&nbsp;     */
<i>665</i>&nbsp;    private static DiscoveryNode findSourceNodeForPeerRecovery(Logger logger, RoutingTable routingTable, DiscoveryNodes nodes,
<i>666</i>&nbsp;                                                               ShardRouting shardRouting) {
<b class="nc"><i>667</i>&nbsp;        DiscoveryNode sourceNode = null;</b>
<b class="nc"><i>668</i>&nbsp;        if (!shardRouting.primary()) {</b>
<b class="nc"><i>669</i>&nbsp;            ShardRouting primary = routingTable.shardRoutingTable(shardRouting.shardId()).primaryShard();</b>
<i>670</i>&nbsp;            // only recover from started primary, if we can&#39;t find one, we will do it next round
<b class="nc"><i>671</i>&nbsp;            if (primary.active()) {</b>
<b class="nc"><i>672</i>&nbsp;                sourceNode = nodes.get(primary.currentNodeId());</b>
<b class="nc"><i>673</i>&nbsp;                if (sourceNode == null) {</b>
<b class="nc"><i>674</i>&nbsp;                    logger.trace(&quot;can&#39;t find replica source node because primary shard {} is assigned to an unknown node.&quot;, primary);</b>
<i>675</i>&nbsp;                }
<i>676</i>&nbsp;            } else {
<b class="nc"><i>677</i>&nbsp;                logger.trace(&quot;can&#39;t find replica source node because primary shard {} is not active.&quot;, primary);</b>
<i>678</i>&nbsp;            }
<b class="nc"><i>679</i>&nbsp;        } else if (shardRouting.relocatingNodeId() != null) {</b>
<b class="nc"><i>680</i>&nbsp;            sourceNode = nodes.get(shardRouting.relocatingNodeId());</b>
<b class="nc"><i>681</i>&nbsp;            if (sourceNode == null) {</b>
<b class="nc"><i>682</i>&nbsp;                logger.trace(&quot;can&#39;t find relocation source node for shard {} because it is assigned to an unknown node [{}].&quot;,</b>
<b class="nc"><i>683</i>&nbsp;                    shardRouting.shardId(), shardRouting.relocatingNodeId());</b>
<i>684</i>&nbsp;            }
<i>685</i>&nbsp;        } else {
<b class="nc"><i>686</i>&nbsp;            throw new IllegalStateException(&quot;trying to find source node for peer recovery when routing state means no peer recovery: &quot; +</b>
<i>687</i>&nbsp;                shardRouting);
<i>688</i>&nbsp;        }
<b class="nc"><i>689</i>&nbsp;        return sourceNode;</b>
<i>690</i>&nbsp;    }
<i>691</i>&nbsp;
<b class="fc"><i>692</i>&nbsp;    private class RecoveryListener implements PeerRecoveryTargetService.RecoveryListener {</b>
<i>693</i>&nbsp;
<i>694</i>&nbsp;        /**
<i>695</i>&nbsp;         * ShardRouting with which the shard was created
<i>696</i>&nbsp;         */
<i>697</i>&nbsp;        private final ShardRouting shardRouting;
<i>698</i>&nbsp;
<i>699</i>&nbsp;        /**
<i>700</i>&nbsp;         * Primary term with which the shard was created
<i>701</i>&nbsp;         */
<i>702</i>&nbsp;        private final long primaryTerm;
<i>703</i>&nbsp;
<b class="fc"><i>704</i>&nbsp;        private RecoveryListener(final ShardRouting shardRouting, final long primaryTerm) {</b>
<b class="fc"><i>705</i>&nbsp;            this.shardRouting = shardRouting;</b>
<b class="fc"><i>706</i>&nbsp;            this.primaryTerm = primaryTerm;</b>
<b class="fc"><i>707</i>&nbsp;        }</b>
<i>708</i>&nbsp;
<i>709</i>&nbsp;        @Override
<i>710</i>&nbsp;        public void onRecoveryDone(final RecoveryState state) {
<b class="fc"><i>711</i>&nbsp;            shardStateAction.shardStarted(shardRouting, primaryTerm, &quot;after &quot; + state.getRecoverySource(), SHARD_STATE_ACTION_LISTENER);</b>
<b class="fc"><i>712</i>&nbsp;        }</b>
<i>713</i>&nbsp;
<i>714</i>&nbsp;        @Override
<i>715</i>&nbsp;        public void onRecoveryFailure(RecoveryState state, RecoveryFailedException e, boolean sendShardFailure) {
<b class="nc"><i>716</i>&nbsp;            handleRecoveryFailure(shardRouting, sendShardFailure, e);</b>
<b class="nc"><i>717</i>&nbsp;        }</b>
<i>718</i>&nbsp;    }
<i>719</i>&nbsp;
<i>720</i>&nbsp;    // package-private for testing
<i>721</i>&nbsp;    synchronized void handleRecoveryFailure(ShardRouting shardRouting, boolean sendShardFailure, Exception failure) {
<b class="nc"><i>722</i>&nbsp;        failAndRemoveShard(shardRouting, sendShardFailure, &quot;failed recovery&quot;, failure, clusterService.state());</b>
<b class="nc"><i>723</i>&nbsp;    }</b>
<i>724</i>&nbsp;
<i>725</i>&nbsp;    private void failAndRemoveShard(ShardRouting shardRouting, boolean sendShardFailure, String message, @Nullable Exception failure,
<i>726</i>&nbsp;                                    ClusterState state) {
<i>727</i>&nbsp;        try {
<b class="nc"><i>728</i>&nbsp;            AllocatedIndex&lt;? extends Shard&gt; indexService = indicesService.indexService(shardRouting.shardId().getIndex());</b>
<b class="nc"><i>729</i>&nbsp;            if (indexService != null) {</b>
<b class="nc"><i>730</i>&nbsp;                Shard shard = indexService.getShardOrNull(shardRouting.shardId().id());</b>
<b class="nc"><i>731</i>&nbsp;                if (shard != null &amp;&amp; shard.routingEntry().isSameAllocation(shardRouting)) {</b>
<b class="nc"><i>732</i>&nbsp;                    indexService.removeShard(shardRouting.shardId().id(), message);</b>
<i>733</i>&nbsp;                }
<i>734</i>&nbsp;            }
<b class="nc"><i>735</i>&nbsp;        } catch (ShardNotFoundException e) {</b>
<i>736</i>&nbsp;            // the node got closed on us, ignore it
<b class="nc"><i>737</i>&nbsp;        } catch (Exception inner) {</b>
<b class="nc"><i>738</i>&nbsp;            inner.addSuppressed(failure);</b>
<b class="nc"><i>739</i>&nbsp;            logger.warn(() -&gt; new ParameterizedMessage(</b>
<i>740</i>&nbsp;                    &quot;[{}][{}] failed to remove shard after failure ([{}])&quot;,
<b class="nc"><i>741</i>&nbsp;                    shardRouting.getIndexName(),</b>
<b class="nc"><i>742</i>&nbsp;                    shardRouting.getId(),</b>
<i>743</i>&nbsp;                    message),
<i>744</i>&nbsp;                inner);
<b class="nc"><i>745</i>&nbsp;        }</b>
<b class="nc"><i>746</i>&nbsp;        if (sendShardFailure) {</b>
<b class="nc"><i>747</i>&nbsp;            sendFailShard(shardRouting, message, failure, state);</b>
<i>748</i>&nbsp;        }
<b class="nc"><i>749</i>&nbsp;    }</b>
<i>750</i>&nbsp;
<i>751</i>&nbsp;    private void sendFailShard(ShardRouting shardRouting, String message, @Nullable Exception failure, ClusterState state) {
<i>752</i>&nbsp;        try {
<b class="nc"><i>753</i>&nbsp;            logger.warn(() -&gt; new ParameterizedMessage(</b>
<b class="nc"><i>754</i>&nbsp;                    &quot;{} marking and sending shard failed due to [{}]&quot;, shardRouting.shardId(), message), failure);</b>
<b class="nc"><i>755</i>&nbsp;            failedShardsCache.put(shardRouting.shardId(), shardRouting);</b>
<b class="nc"><i>756</i>&nbsp;            shardStateAction.localShardFailed(shardRouting, message, failure, SHARD_STATE_ACTION_LISTENER, state);</b>
<b class="nc"><i>757</i>&nbsp;        } catch (Exception inner) {</b>
<b class="nc"><i>758</i>&nbsp;            if (failure != null) inner.addSuppressed(failure);</b>
<b class="nc"><i>759</i>&nbsp;            logger.warn(() -&gt; new ParameterizedMessage(</b>
<i>760</i>&nbsp;                    &quot;[{}][{}] failed to mark shard as failed (because of [{}])&quot;,
<b class="nc"><i>761</i>&nbsp;                    shardRouting.getIndexName(),</b>
<b class="nc"><i>762</i>&nbsp;                    shardRouting.getId(),</b>
<i>763</i>&nbsp;                    message),
<i>764</i>&nbsp;                inner);
<b class="nc"><i>765</i>&nbsp;        }</b>
<b class="nc"><i>766</i>&nbsp;    }</b>
<i>767</i>&nbsp;
<b class="fc"><i>768</i>&nbsp;    private class FailedShardHandler implements Consumer&lt;IndexShard.ShardFailure&gt; {</b>
<i>769</i>&nbsp;        @Override
<i>770</i>&nbsp;        public void accept(final IndexShard.ShardFailure shardFailure) {
<b class="nc"><i>771</i>&nbsp;            final ShardRouting shardRouting = shardFailure.routing;</b>
<b class="nc"><i>772</i>&nbsp;            threadPool.generic().execute(() -&gt; {</b>
<b class="nc"><i>773</i>&nbsp;                synchronized (IndicesClusterStateService.this) {</b>
<b class="nc"><i>774</i>&nbsp;                    failAndRemoveShard(shardRouting, true, &quot;shard failure, reason [&quot; + shardFailure.reason + &quot;]&quot;, shardFailure.cause,</b>
<b class="nc"><i>775</i>&nbsp;                        clusterService.state());</b>
<b class="nc"><i>776</i>&nbsp;                }</b>
<b class="nc"><i>777</i>&nbsp;            });</b>
<b class="nc"><i>778</i>&nbsp;        }</b>
<i>779</i>&nbsp;    }
<i>780</i>&nbsp;
<i>781</i>&nbsp;    public interface Shard {
<i>782</i>&nbsp;
<i>783</i>&nbsp;        /**
<i>784</i>&nbsp;         * Returns the shard id of this shard.
<i>785</i>&nbsp;         */
<i>786</i>&nbsp;        ShardId shardId();
<i>787</i>&nbsp;
<i>788</i>&nbsp;        /**
<i>789</i>&nbsp;         * Returns the latest cluster routing entry received with this shard.
<i>790</i>&nbsp;         */
<i>791</i>&nbsp;        ShardRouting routingEntry();
<i>792</i>&nbsp;
<i>793</i>&nbsp;        /**
<i>794</i>&nbsp;         * Returns the latest internal shard state.
<i>795</i>&nbsp;         */
<i>796</i>&nbsp;        IndexShardState state();
<i>797</i>&nbsp;
<i>798</i>&nbsp;        /**
<i>799</i>&nbsp;         * Returns the recovery state associated with this shard.
<i>800</i>&nbsp;         */
<i>801</i>&nbsp;        RecoveryState recoveryState();
<i>802</i>&nbsp;
<i>803</i>&nbsp;        /**
<i>804</i>&nbsp;         * Updates the shard state based on an incoming cluster state:
<i>805</i>&nbsp;         * - Updates and persists the new routing value.
<i>806</i>&nbsp;         * - Updates the primary term if this shard is a primary.
<i>807</i>&nbsp;         * - Updates the allocation ids that are tracked by the shard if it is a primary.
<i>808</i>&nbsp;         *   See {@link ReplicationTracker#updateFromMaster(long, Set, IndexShardRoutingTable)} for details.
<i>809</i>&nbsp;         *
<i>810</i>&nbsp;         * @param shardRouting                the new routing entry
<i>811</i>&nbsp;         * @param primaryTerm                 the new primary term
<i>812</i>&nbsp;         * @param primaryReplicaSyncer        the primary-replica resync action to trigger when a term is increased on a primary
<i>813</i>&nbsp;         * @param applyingClusterStateVersion the cluster state version being applied when updating the allocation IDs from the master
<i>814</i>&nbsp;         * @param inSyncAllocationIds         the allocation ids of the currently in-sync shard copies
<i>815</i>&nbsp;         * @param routingTable                the shard routing table
<i>816</i>&nbsp;         * @throws IndexShardRelocatedException if shard is marked as relocated and relocation aborted
<i>817</i>&nbsp;         * @throws IOException                  if shard state could not be persisted
<i>818</i>&nbsp;         */
<i>819</i>&nbsp;        void updateShardState(ShardRouting shardRouting,
<i>820</i>&nbsp;                              long primaryTerm,
<i>821</i>&nbsp;                              BiConsumer&lt;IndexShard, ActionListener&lt;ResyncTask&gt;&gt; primaryReplicaSyncer,
<i>822</i>&nbsp;                              long applyingClusterStateVersion,
<i>823</i>&nbsp;                              Set&lt;String&gt; inSyncAllocationIds,
<i>824</i>&nbsp;                              IndexShardRoutingTable routingTable) throws IOException;
<i>825</i>&nbsp;    }
<i>826</i>&nbsp;
<i>827</i>&nbsp;    public interface AllocatedIndex&lt;T extends Shard&gt; extends Iterable&lt;T&gt;, IndexComponent {
<i>828</i>&nbsp;
<i>829</i>&nbsp;        /**
<i>830</i>&nbsp;         * Returns the index settings of this index.
<i>831</i>&nbsp;         */
<i>832</i>&nbsp;        IndexSettings getIndexSettings();
<i>833</i>&nbsp;
<i>834</i>&nbsp;        /**
<i>835</i>&nbsp;         * Updates the metadata of this index. Changes become visible through {@link #getIndexSettings()}.
<i>836</i>&nbsp;         *
<i>837</i>&nbsp;         * @param currentIndexMetaData the current index metadata
<i>838</i>&nbsp;         * @param newIndexMetaData the new index metadata
<i>839</i>&nbsp;         */
<i>840</i>&nbsp;        void updateMetaData(IndexMetaData currentIndexMetaData, IndexMetaData newIndexMetaData);
<i>841</i>&nbsp;
<i>842</i>&nbsp;        /**
<i>843</i>&nbsp;         * Checks if index requires refresh from master.
<i>844</i>&nbsp;         */
<i>845</i>&nbsp;        boolean updateMapping(IndexMetaData currentIndexMetaData, IndexMetaData newIndexMetaData) throws IOException;
<i>846</i>&nbsp;
<i>847</i>&nbsp;        /**
<i>848</i>&nbsp;         * Returns shard with given id.
<i>849</i>&nbsp;         */
<i>850</i>&nbsp;        @Nullable T getShardOrNull(int shardId);
<i>851</i>&nbsp;
<i>852</i>&nbsp;        /**
<i>853</i>&nbsp;         * Removes shard with given id.
<i>854</i>&nbsp;         */
<i>855</i>&nbsp;        void removeShard(int shardId, String message);
<i>856</i>&nbsp;    }
<i>857</i>&nbsp;
<i>858</i>&nbsp;    public interface AllocatedIndices&lt;T extends Shard, U extends AllocatedIndex&lt;T&gt;&gt; extends Iterable&lt;U&gt; {
<i>859</i>&nbsp;
<i>860</i>&nbsp;        /**
<i>861</i>&nbsp;         * Creates a new {@link IndexService} for the given metadata.
<i>862</i>&nbsp;         *
<i>863</i>&nbsp;         * @param indexMetaData          the index metadata to create the index for
<i>864</i>&nbsp;         * @param builtInIndexListener   a list of built-in lifecycle {@link IndexEventListener} that should should be used along side with
<i>865</i>&nbsp;         *                               the per-index listeners
<i>866</i>&nbsp;         * @throws ResourceAlreadyExistsException if the index already exists.
<i>867</i>&nbsp;         */
<i>868</i>&nbsp;        U createIndex(IndexMetaData indexMetaData,
<i>869</i>&nbsp;                      List&lt;IndexEventListener&gt; builtInIndexListener) throws IOException;
<i>870</i>&nbsp;
<i>871</i>&nbsp;        /**
<i>872</i>&nbsp;         * Verify that the contents on disk for the given index is deleted; if not, delete the contents.
<i>873</i>&nbsp;         * This method assumes that an index is already deleted in the cluster state and/or explicitly
<i>874</i>&nbsp;         * through index tombstones.
<i>875</i>&nbsp;         * @param index {@code Index} to make sure its deleted from disk
<i>876</i>&nbsp;         * @param clusterState {@code ClusterState} to ensure the index is not part of it
<i>877</i>&nbsp;         * @return IndexMetaData for the index loaded from disk
<i>878</i>&nbsp;         */
<i>879</i>&nbsp;        IndexMetaData verifyIndexIsDeleted(Index index, ClusterState clusterState);
<i>880</i>&nbsp;
<i>881</i>&nbsp;
<i>882</i>&nbsp;        /**
<i>883</i>&nbsp;         * Deletes an index that is not assigned to this node. This method cleans up all disk folders relating to the index
<i>884</i>&nbsp;         * but does not deal with in-memory structures. For those call {@link #removeIndex(Index, IndexRemovalReason, String)}
<i>885</i>&nbsp;         */
<i>886</i>&nbsp;        void deleteUnassignedIndex(String reason, IndexMetaData metaData, ClusterState clusterState);
<i>887</i>&nbsp;
<i>888</i>&nbsp;        /**
<i>889</i>&nbsp;         * Removes the given index from this service and releases all associated resources. Persistent parts of the index
<i>890</i>&nbsp;         * like the shards files, state and transaction logs are kept around in the case of a disaster recovery.
<i>891</i>&nbsp;         * @param index the index to remove
<i>892</i>&nbsp;         * @param reason the reason to remove the index
<i>893</i>&nbsp;         * @param extraInfo extra information that will be used for logging and reporting
<i>894</i>&nbsp;         */
<i>895</i>&nbsp;        void removeIndex(Index index, IndexRemovalReason reason, String extraInfo);
<i>896</i>&nbsp;
<i>897</i>&nbsp;        /**
<i>898</i>&nbsp;         * Returns an IndexService for the specified index if exists otherwise returns &lt;code&gt;null&lt;/code&gt;.
<i>899</i>&nbsp;         */
<i>900</i>&nbsp;        @Nullable U indexService(Index index);
<i>901</i>&nbsp;
<i>902</i>&nbsp;        /**
<i>903</i>&nbsp;         * Creates a shard for the specified shard routing and starts recovery.
<i>904</i>&nbsp;         *
<i>905</i>&nbsp;         * @param shardRouting           the shard routing
<i>906</i>&nbsp;         * @param recoveryState          the recovery state
<i>907</i>&nbsp;         * @param recoveryTargetService  recovery service for the target
<i>908</i>&nbsp;         * @param recoveryListener       a callback when recovery changes state (finishes or fails)
<i>909</i>&nbsp;         * @param repositoriesService    service responsible for snapshot/restore
<i>910</i>&nbsp;         * @param onShardFailure         a callback when this shard fails
<i>911</i>&nbsp;         * @param globalCheckpointSyncer a callback when this shard syncs the global checkpoint
<i>912</i>&nbsp;         * @param retentionLeaseSyncer   a callback when this shard syncs retention leases
<i>913</i>&nbsp;         * @return a new shard
<i>914</i>&nbsp;         * @throws IOException if an I/O exception occurs when creating the shard
<i>915</i>&nbsp;         */
<i>916</i>&nbsp;        T createShard(
<i>917</i>&nbsp;                ShardRouting shardRouting,
<i>918</i>&nbsp;                RecoveryState recoveryState,
<i>919</i>&nbsp;                PeerRecoveryTargetService recoveryTargetService,
<i>920</i>&nbsp;                PeerRecoveryTargetService.RecoveryListener recoveryListener,
<i>921</i>&nbsp;                RepositoriesService repositoriesService,
<i>922</i>&nbsp;                Consumer&lt;IndexShard.ShardFailure&gt; onShardFailure,
<i>923</i>&nbsp;                Consumer&lt;ShardId&gt; globalCheckpointSyncer,
<i>924</i>&nbsp;                RetentionLeaseSyncer retentionLeaseSyncer) throws IOException;
<i>925</i>&nbsp;
<i>926</i>&nbsp;        /**
<i>927</i>&nbsp;         * Returns shard for the specified id if it exists otherwise returns &lt;code&gt;null&lt;/code&gt;.
<i>928</i>&nbsp;         */
<i>929</i>&nbsp;        default T getShardOrNull(ShardId shardId) {
<b class="fc"><i>930</i>&nbsp;            U indexRef = indexService(shardId.getIndex());</b>
<b class="fc"><i>931</i>&nbsp;            if (indexRef != null) {</b>
<b class="fc"><i>932</i>&nbsp;                return indexRef.getShardOrNull(shardId.id());</b>
<i>933</i>&nbsp;            }
<b class="nc"><i>934</i>&nbsp;            return null;</b>
<i>935</i>&nbsp;        }
<i>936</i>&nbsp;
<i>937</i>&nbsp;        void processPendingDeletes(Index index, IndexSettings indexSettings, TimeValue timeValue)
<i>938</i>&nbsp;            throws IOException, InterruptedException, ShardLockObtainFailedException;
<i>939</i>&nbsp;
<b class="fc"><i>940</i>&nbsp;        enum IndexRemovalReason {</b>
<i>941</i>&nbsp;            /**
<i>942</i>&nbsp;             * Shard of this index were previously assigned to this node but all shards have been relocated.
<i>943</i>&nbsp;             * The index should be removed and all associated resources released. Persistent parts of the index
<i>944</i>&nbsp;             * like the shards files, state and transaction logs are kept around in the case of a disaster recovery.
<i>945</i>&nbsp;             */
<b class="fc"><i>946</i>&nbsp;            NO_LONGER_ASSIGNED,</b>
<i>947</i>&nbsp;            /**
<i>948</i>&nbsp;             * The index is deleted. Persistent parts of the index  like the shards files, state and transaction logs are removed once
<i>949</i>&nbsp;             * all resources are released.
<i>950</i>&nbsp;             */
<b class="fc"><i>951</i>&nbsp;            DELETED,</b>
<i>952</i>&nbsp;
<i>953</i>&nbsp;            /**
<i>954</i>&nbsp;             * The index has been closed. The index should be removed and all associated resources released. Persistent parts of the index
<i>955</i>&nbsp;             * like the shards files, state and transaction logs are kept around in the case of a disaster recovery.
<i>956</i>&nbsp;             */
<b class="fc"><i>957</i>&nbsp;            CLOSED,</b>
<i>958</i>&nbsp;
<i>959</i>&nbsp;            /**
<i>960</i>&nbsp;             * Something around index management has failed and the index should be removed.
<i>961</i>&nbsp;             * Persistent parts of the index like the shards files, state and transaction logs are kept around in the
<i>962</i>&nbsp;             * case of a disaster recovery.
<i>963</i>&nbsp;             */
<b class="fc"><i>964</i>&nbsp;            FAILURE,</b>
<i>965</i>&nbsp;
<i>966</i>&nbsp;            /**
<i>967</i>&nbsp;             * The index has been reopened. The index should be removed and all associated resources released. Persistent parts of the index
<i>968</i>&nbsp;             * like the shards files, state and transaction logs are kept around in the case of a disaster recovery.
<i>969</i>&nbsp;             */
<b class="fc"><i>970</i>&nbsp;            REOPENED,</b>
<i>971</i>&nbsp;        }
<i>972</i>&nbsp;    }
<i>973</i>&nbsp;}
</div>
</div>

<div class="footer">
    
    <div style="float:right;">generated on 2020-02-09 18:45</div>
</div>
</body>
</html>
