


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html id="htmlId">
<head>
  <title>Coverage Report :: Store</title>
  <style type="text/css">
    @import "../../.css/coverage.css";
  </style>
</head>

<body>
<div class="header"></div>

<div class="content">
<div class="breadCrumbs">
    [ <a href="../../index.html">all classes</a> ]
    [ <a href="../index.html">org.elasticsearch.index.store</a> ]
</div>

<h1>Coverage Summary for Class: Store (org.elasticsearch.index.store)</h1>

<table class="coverageStats">

<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">Store</td>
<td class="coverageStat">
  <span class="percent">
    42.3%
  </span>
  <span class="absValue">
    (22/ 52)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    28.6%
  </span>
  <span class="absValue">
    (108/ 377)
  </span>
</td>
</tr>
  <tr>
    <td class="name">Store$1</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (2/ 2)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (3/ 3)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">Store$LuceneVerifyingIndexOutput</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/ 5)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/ 37)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">Store$OnClose</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/ 1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/ 1)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">Store$StoreDirectory</td>
<td class="coverageStat">
  <span class="percent">
    77.8%
  </span>
  <span class="absValue">
    (7/ 9)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    81.2%
  </span>
  <span class="absValue">
    (13/ 16)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">Store$VerifyingIndexInput</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/ 14)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/ 55)
  </span>
</td>
  </tr>
<tr>
  <td class="name"><strong>total</strong></td>
<td class="coverageStat">
  <span class="percent">
    37.3%
  </span>
  <span class="absValue">
    (31/ 83)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    25.4%
  </span>
  <span class="absValue">
    (124/ 489)
  </span>
</td>
</tr>
</table>

<br/>
<br/>


<div class="sourceCode"><i>1</i>&nbsp;/*
<i>2</i>&nbsp; * Licensed to Elasticsearch under one or more contributor
<i>3</i>&nbsp; * license agreements. See the NOTICE file distributed with
<i>4</i>&nbsp; * this work for additional information regarding copyright
<i>5</i>&nbsp; * ownership. Elasticsearch licenses this file to you under
<i>6</i>&nbsp; * the Apache License, Version 2.0 (the &quot;License&quot;); you may
<i>7</i>&nbsp; * not use this file except in compliance with the License.
<i>8</i>&nbsp; * You may obtain a copy of the License at
<i>9</i>&nbsp; *
<i>10</i>&nbsp; *    http://www.apache.org/licenses/LICENSE-2.0
<i>11</i>&nbsp; *
<i>12</i>&nbsp; * Unless required by applicable law or agreed to in writing,
<i>13</i>&nbsp; * software distributed under the License is distributed on an
<i>14</i>&nbsp; * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
<i>15</i>&nbsp; * KIND, either express or implied.  See the License for the
<i>16</i>&nbsp; * specific language governing permissions and limitations
<i>17</i>&nbsp; * under the License.
<i>18</i>&nbsp; */
<i>19</i>&nbsp;
<i>20</i>&nbsp;package org.elasticsearch.index.store;
<i>21</i>&nbsp;
<i>22</i>&nbsp;import org.apache.logging.log4j.Logger;
<i>23</i>&nbsp;import org.apache.logging.log4j.message.ParameterizedMessage;
<i>24</i>&nbsp;import org.apache.lucene.codecs.CodecUtil;
<i>25</i>&nbsp;import org.apache.lucene.index.CheckIndex;
<i>26</i>&nbsp;import org.apache.lucene.index.CorruptIndexException;
<i>27</i>&nbsp;import org.apache.lucene.index.DirectoryReader;
<i>28</i>&nbsp;import org.apache.lucene.index.IndexCommit;
<i>29</i>&nbsp;import org.apache.lucene.index.IndexFileNames;
<i>30</i>&nbsp;import org.apache.lucene.index.IndexFormatTooNewException;
<i>31</i>&nbsp;import org.apache.lucene.index.IndexFormatTooOldException;
<i>32</i>&nbsp;import org.apache.lucene.index.IndexNotFoundException;
<i>33</i>&nbsp;import org.apache.lucene.index.IndexWriter;
<i>34</i>&nbsp;import org.apache.lucene.index.IndexWriterConfig;
<i>35</i>&nbsp;import org.apache.lucene.index.NoMergePolicy;
<i>36</i>&nbsp;import org.apache.lucene.index.SegmentCommitInfo;
<i>37</i>&nbsp;import org.apache.lucene.index.SegmentInfos;
<i>38</i>&nbsp;import org.apache.lucene.store.AlreadyClosedException;
<i>39</i>&nbsp;import org.apache.lucene.store.BufferedChecksum;
<i>40</i>&nbsp;import org.apache.lucene.store.ByteArrayDataInput;
<i>41</i>&nbsp;import org.apache.lucene.store.ChecksumIndexInput;
<i>42</i>&nbsp;import org.apache.lucene.store.Directory;
<i>43</i>&nbsp;import org.apache.lucene.store.FilterDirectory;
<i>44</i>&nbsp;import org.apache.lucene.store.IOContext;
<i>45</i>&nbsp;import org.apache.lucene.store.IndexInput;
<i>46</i>&nbsp;import org.apache.lucene.store.IndexOutput;
<i>47</i>&nbsp;import org.apache.lucene.store.Lock;
<i>48</i>&nbsp;import org.apache.lucene.store.SimpleFSDirectory;
<i>49</i>&nbsp;import org.apache.lucene.util.ArrayUtil;
<i>50</i>&nbsp;import org.apache.lucene.util.BytesRef;
<i>51</i>&nbsp;import org.apache.lucene.util.BytesRefBuilder;
<i>52</i>&nbsp;import org.apache.lucene.util.Version;
<i>53</i>&nbsp;import org.elasticsearch.ExceptionsHelper;
<i>54</i>&nbsp;import org.elasticsearch.common.UUIDs;
<i>55</i>&nbsp;import org.elasticsearch.common.bytes.BytesReference;
<i>56</i>&nbsp;import org.elasticsearch.common.io.Streams;
<i>57</i>&nbsp;import org.elasticsearch.common.io.stream.BytesStreamOutput;
<i>58</i>&nbsp;import org.elasticsearch.common.io.stream.StreamInput;
<i>59</i>&nbsp;import org.elasticsearch.common.io.stream.StreamOutput;
<i>60</i>&nbsp;import org.elasticsearch.common.io.stream.Writeable;
<i>61</i>&nbsp;import org.elasticsearch.common.logging.Loggers;
<i>62</i>&nbsp;import org.elasticsearch.common.lucene.Lucene;
<i>63</i>&nbsp;import org.elasticsearch.common.lucene.store.ByteArrayIndexInput;
<i>64</i>&nbsp;import org.elasticsearch.common.lucene.store.InputStreamIndexInput;
<i>65</i>&nbsp;import org.elasticsearch.common.settings.Setting;
<i>66</i>&nbsp;import org.elasticsearch.common.settings.Setting.Property;
<i>67</i>&nbsp;import org.elasticsearch.common.unit.TimeValue;
<i>68</i>&nbsp;import org.elasticsearch.common.util.concurrent.AbstractRefCounted;
<i>69</i>&nbsp;import org.elasticsearch.common.util.concurrent.RefCounted;
<i>70</i>&nbsp;import org.elasticsearch.common.util.iterable.Iterables;
<i>71</i>&nbsp;import org.elasticsearch.core.internal.io.IOUtils;
<i>72</i>&nbsp;import org.elasticsearch.env.NodeEnvironment;
<i>73</i>&nbsp;import org.elasticsearch.env.ShardLock;
<i>74</i>&nbsp;import org.elasticsearch.env.ShardLockObtainFailedException;
<i>75</i>&nbsp;import org.elasticsearch.index.IndexSettings;
<i>76</i>&nbsp;import org.elasticsearch.index.engine.CombinedDeletionPolicy;
<i>77</i>&nbsp;import org.elasticsearch.index.engine.Engine;
<i>78</i>&nbsp;import org.elasticsearch.index.seqno.SequenceNumbers;
<i>79</i>&nbsp;import org.elasticsearch.index.shard.AbstractIndexShardComponent;
<i>80</i>&nbsp;import org.elasticsearch.index.shard.IndexShard;
<i>81</i>&nbsp;import org.elasticsearch.index.shard.ShardId;
<i>82</i>&nbsp;import org.elasticsearch.index.translog.Translog;
<i>83</i>&nbsp;
<i>84</i>&nbsp;import java.io.Closeable;
<i>85</i>&nbsp;import java.io.EOFException;
<i>86</i>&nbsp;import java.io.FileNotFoundException;
<i>87</i>&nbsp;import java.io.IOException;
<i>88</i>&nbsp;import java.io.InputStream;
<i>89</i>&nbsp;import java.io.PrintStream;
<i>90</i>&nbsp;import java.io.UncheckedIOException;
<i>91</i>&nbsp;import java.nio.file.NoSuchFileException;
<i>92</i>&nbsp;import java.nio.file.Path;
<i>93</i>&nbsp;import java.util.ArrayList;
<i>94</i>&nbsp;import java.util.Collections;
<i>95</i>&nbsp;import java.util.HashMap;
<i>96</i>&nbsp;import java.util.Iterator;
<i>97</i>&nbsp;import java.util.List;
<i>98</i>&nbsp;import java.util.Map;
<i>99</i>&nbsp;import java.util.Set;
<i>100</i>&nbsp;import java.util.Optional;
<i>101</i>&nbsp;import java.util.concurrent.TimeUnit;
<i>102</i>&nbsp;import java.util.concurrent.atomic.AtomicBoolean;
<i>103</i>&nbsp;import java.util.concurrent.locks.ReentrantReadWriteLock;
<i>104</i>&nbsp;import java.util.function.Consumer;
<i>105</i>&nbsp;import java.util.zip.CRC32;
<i>106</i>&nbsp;import java.util.zip.Checksum;
<i>107</i>&nbsp;
<i>108</i>&nbsp;import static java.util.Collections.emptyMap;
<i>109</i>&nbsp;import static java.util.Collections.unmodifiableMap;
<i>110</i>&nbsp;
<i>111</i>&nbsp;/**
<i>112</i>&nbsp; * A Store provides plain access to files written by an elasticsearch index shard. Each shard
<i>113</i>&nbsp; * has a dedicated store that is uses to access Lucene&#39;s Directory which represents the lowest level
<i>114</i>&nbsp; * of file abstraction in Lucene used to read and write Lucene indices.
<i>115</i>&nbsp; * This class also provides access to metadata information like checksums for committed files. A committed
<i>116</i>&nbsp; * file is a file that belongs to a segment written by a Lucene commit. Files that have not been committed
<i>117</i>&nbsp; * ie. created during a merge or a shard refresh / NRT reopen are not considered in the MetadataSnapshot.
<i>118</i>&nbsp; * &lt;p&gt;
<i>119</i>&nbsp; * Note: If you use a store it&#39;s reference count should be increased before using it by calling #incRef and a
<i>120</i>&nbsp; * corresponding #decRef must be called in a try/finally block to release the store again ie.:
<i>121</i>&nbsp; * &lt;pre&gt;
<i>122</i>&nbsp; *      store.incRef();
<i>123</i>&nbsp; *      try {
<i>124</i>&nbsp; *        // use the store...
<i>125</i>&nbsp; *
<i>126</i>&nbsp; *      } finally {
<i>127</i>&nbsp; *          store.decRef();
<i>128</i>&nbsp; *      }
<i>129</i>&nbsp; * &lt;/pre&gt;
<i>130</i>&nbsp; */
<b class="fc"><i>131</i>&nbsp;public class Store extends AbstractIndexShardComponent implements Closeable, RefCounted {</b>
<i>132</i>&nbsp;    /**
<i>133</i>&nbsp;     * This is an escape hatch for lucenes internal optimization that checks if the IndexInput is an instance of ByteBufferIndexInput
<i>134</i>&nbsp;     * and if that&#39;s the case doesn&#39;t load the term dictionary into ram but loads it off disk iff the fields is not an ID like field.
<i>135</i>&nbsp;     * Since this optimization has been added very late in the release processes we add this setting to allow users to opt-out of
<i>136</i>&nbsp;     * this by exploiting lucene internals and wrapping the IndexInput in a simple delegate.
<i>137</i>&nbsp;     */
<b class="fc"><i>138</i>&nbsp;    public static final Setting&lt;Boolean&gt; FORCE_RAM_TERM_DICT = Setting.boolSetting(&quot;index.force_memory_term_dictionary&quot;, false,</b>
<i>139</i>&nbsp;        Property.IndexScope, Property.Deprecated);
<i>140</i>&nbsp;    static final String CODEC = &quot;store&quot;;
<i>141</i>&nbsp;    static final int VERSION_WRITE_THROWABLE= 2; // we write throwable since 2.0
<i>142</i>&nbsp;    static final int VERSION_STACK_TRACE = 1; // we write the stack trace too since 1.4.0
<i>143</i>&nbsp;    static final int VERSION_START = 0;
<i>144</i>&nbsp;    static final int VERSION = VERSION_WRITE_THROWABLE;
<i>145</i>&nbsp;    // public is for test purposes
<i>146</i>&nbsp;    public static final String CORRUPTED = &quot;corrupted_&quot;;
<b class="fc"><i>147</i>&nbsp;    public static final Setting&lt;TimeValue&gt; INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING =</b>
<b class="fc"><i>148</i>&nbsp;        Setting.timeSetting(&quot;index.store.stats_refresh_interval&quot;, TimeValue.timeValueSeconds(10), Property.IndexScope);</b>
<i>149</i>&nbsp;
<b class="fc"><i>150</i>&nbsp;    private final AtomicBoolean isClosed = new AtomicBoolean(false);</b>
<i>151</i>&nbsp;    private final StoreDirectory directory;
<b class="fc"><i>152</i>&nbsp;    private final ReentrantReadWriteLock metadataLock = new ReentrantReadWriteLock();</b>
<i>153</i>&nbsp;    private final ShardLock shardLock;
<i>154</i>&nbsp;    private final OnClose onClose;
<i>155</i>&nbsp;
<b class="fc"><i>156</i>&nbsp;    private final AbstractRefCounted refCounter = new AbstractRefCounted(&quot;store&quot;) {</b>
<i>157</i>&nbsp;        @Override
<i>158</i>&nbsp;        protected void closeInternal() {
<i>159</i>&nbsp;            // close us once we are done
<b class="fc"><i>160</i>&nbsp;            Store.this.closeInternal();</b>
<b class="fc"><i>161</i>&nbsp;        }</b>
<i>162</i>&nbsp;    };
<i>163</i>&nbsp;
<i>164</i>&nbsp;    public Store(ShardId shardId, IndexSettings indexSettings, Directory directory, ShardLock shardLock) {
<b class="nc"><i>165</i>&nbsp;        this(shardId, indexSettings, directory, shardLock, OnClose.EMPTY);</b>
<b class="nc"><i>166</i>&nbsp;    }</b>
<i>167</i>&nbsp;
<i>168</i>&nbsp;    public Store(ShardId shardId, IndexSettings indexSettings, Directory directory, ShardLock shardLock,
<i>169</i>&nbsp;                 OnClose onClose) {
<b class="fc"><i>170</i>&nbsp;        super(shardId, indexSettings);</b>
<b class="fc"><i>171</i>&nbsp;        final TimeValue refreshInterval = indexSettings.getValue(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING);</b>
<b class="fc"><i>172</i>&nbsp;        logger.debug(&quot;store stats are refreshed with refresh_interval [{}]&quot;, refreshInterval);</b>
<b class="fc"><i>173</i>&nbsp;        ByteSizeCachingDirectory sizeCachingDir = new ByteSizeCachingDirectory(directory, refreshInterval);</b>
<b class="fc"><i>174</i>&nbsp;        this.directory = new StoreDirectory(sizeCachingDir, Loggers.getLogger(&quot;index.store.deletes&quot;, shardId));</b>
<b class="fc"><i>175</i>&nbsp;        this.shardLock = shardLock;</b>
<b class="fc"><i>176</i>&nbsp;        this.onClose = onClose;</b>
<i>177</i>&nbsp;
<b class="fc"><i>178</i>&nbsp;        assert onClose != null;</b>
<b class="fc"><i>179</i>&nbsp;        assert shardLock != null;</b>
<b class="fc"><i>180</i>&nbsp;        assert shardLock.getShardId().equals(shardId);</b>
<b class="fc"><i>181</i>&nbsp;    }</b>
<i>182</i>&nbsp;
<i>183</i>&nbsp;    public Directory directory() {
<b class="fc"><i>184</i>&nbsp;        ensureOpen();</b>
<b class="fc"><i>185</i>&nbsp;        return directory;</b>
<i>186</i>&nbsp;    }
<i>187</i>&nbsp;
<i>188</i>&nbsp;    /**
<i>189</i>&nbsp;     * Returns the last committed segments info for this store
<i>190</i>&nbsp;     *
<i>191</i>&nbsp;     * @throws IOException if the index is corrupted or the segments file is not present
<i>192</i>&nbsp;     */
<i>193</i>&nbsp;    public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {
<b class="fc"><i>194</i>&nbsp;        failIfCorrupted();</b>
<i>195</i>&nbsp;        try {
<b class="fc"><i>196</i>&nbsp;            return readSegmentsInfo(null, directory());</b>
<b class="nc"><i>197</i>&nbsp;        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {</b>
<b class="nc"><i>198</i>&nbsp;            markStoreCorrupted(ex);</b>
<b class="nc"><i>199</i>&nbsp;            throw ex;</b>
<i>200</i>&nbsp;        }
<i>201</i>&nbsp;    }
<i>202</i>&nbsp;
<i>203</i>&nbsp;    /**
<i>204</i>&nbsp;     * Returns the segments info for the given commit or for the latest commit if the given commit is &lt;code&gt;null&lt;/code&gt;
<i>205</i>&nbsp;     *
<i>206</i>&nbsp;     * @throws IOException if the index is corrupted or the segments file is not present
<i>207</i>&nbsp;     */
<i>208</i>&nbsp;    private static SegmentInfos readSegmentsInfo(IndexCommit commit, Directory directory) throws IOException {
<b class="fc"><i>209</i>&nbsp;        assert commit == null || commit.getDirectory() == directory;</b>
<i>210</i>&nbsp;        try {
<b class="fc"><i>211</i>&nbsp;            return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit);</b>
<b class="nc"><i>212</i>&nbsp;        } catch (EOFException eof) {</b>
<i>213</i>&nbsp;            // TODO this should be caught by lucene - EOF is almost certainly an index corruption
<b class="nc"><i>214</i>&nbsp;            throw new CorruptIndexException(&quot;Read past EOF while reading segment infos&quot;, &quot;commit(&quot; + commit + &quot;)&quot;, eof);</b>
<b class="fc"><i>215</i>&nbsp;        } catch (IOException exception) {</b>
<b class="fc"><i>216</i>&nbsp;            throw exception; // IOExceptions like too many open files are not necessarily a corruption - just bubble it up</b>
<b class="nc"><i>217</i>&nbsp;        } catch (Exception ex) {</b>
<b class="nc"><i>218</i>&nbsp;            throw new CorruptIndexException(&quot;Hit unexpected exception while reading segment infos&quot;, &quot;commit(&quot; + commit + &quot;)&quot;, ex);</b>
<i>219</i>&nbsp;        }
<i>220</i>&nbsp;
<i>221</i>&nbsp;    }
<i>222</i>&nbsp;
<i>223</i>&nbsp;    /**
<i>224</i>&nbsp;     * Loads the maximum sequence number and local checkpoint from the given Lucene commit point or the latest if not provided.
<i>225</i>&nbsp;     *
<i>226</i>&nbsp;     * @param commit the commit point to load seqno stats, or the last commit in the store if the parameter is null
<i>227</i>&nbsp;     * @return {@link SequenceNumbers.CommitInfo} containing information about the last commit
<i>228</i>&nbsp;     * @throws IOException if an I/O exception occurred reading the latest Lucene commit point from disk
<i>229</i>&nbsp;     */
<i>230</i>&nbsp;    public static SequenceNumbers.CommitInfo loadSeqNoInfo(final IndexCommit commit) throws IOException {
<b class="nc"><i>231</i>&nbsp;        final Map&lt;String, String&gt; userData = commit.getUserData();</b>
<b class="nc"><i>232</i>&nbsp;        return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet());</b>
<i>233</i>&nbsp;    }
<i>234</i>&nbsp;
<i>235</i>&nbsp;    final void ensureOpen() {
<b class="fc"><i>236</i>&nbsp;        if (this.refCounter.refCount() &lt;= 0) {</b>
<b class="nc"><i>237</i>&nbsp;            throw new AlreadyClosedException(&quot;store is already closed&quot;);</b>
<i>238</i>&nbsp;        }
<b class="fc"><i>239</i>&nbsp;    }</b>
<i>240</i>&nbsp;
<i>241</i>&nbsp;    /**
<i>242</i>&nbsp;     * Returns a new MetadataSnapshot for the given commit. If the given commit is &lt;code&gt;null&lt;/code&gt;
<i>243</i>&nbsp;     * the latest commit point is used.
<i>244</i>&nbsp;     *
<i>245</i>&nbsp;     * Note that this method requires the caller verify it has the right to access the store and
<i>246</i>&nbsp;     * no concurrent file changes are happening. If in doubt, you probably want to use one of the following:
<i>247</i>&nbsp;     *
<i>248</i>&nbsp;     * {@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking
<i>249</i>&nbsp;     * {@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard
<i>250</i>&nbsp;     * {@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed
<i>251</i>&nbsp;     * @param commit the index commit to read the snapshot from or &lt;code&gt;null&lt;/code&gt; if the latest snapshot should be read from the
<i>252</i>&nbsp;     *               directory
<i>253</i>&nbsp;     * @throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an
<i>254</i>&nbsp;     *                                    unexpected exception when opening the index reading the segments file.
<i>255</i>&nbsp;     * @throws IndexFormatTooOldException if the lucene index is too old to be opened.
<i>256</i>&nbsp;     * @throws IndexFormatTooNewException if the lucene index is too new to be opened.
<i>257</i>&nbsp;     * @throws FileNotFoundException      if one or more files referenced by a commit are not present.
<i>258</i>&nbsp;     * @throws NoSuchFileException        if one or more files referenced by a commit are not present.
<i>259</i>&nbsp;     * @throws IndexNotFoundException     if the commit point can&#39;t be found in this store
<i>260</i>&nbsp;     */
<i>261</i>&nbsp;    public MetadataSnapshot getMetadata(IndexCommit commit) throws IOException {
<b class="nc"><i>262</i>&nbsp;        return getMetadata(commit, false);</b>
<i>263</i>&nbsp;    }
<i>264</i>&nbsp;
<i>265</i>&nbsp;    /**
<i>266</i>&nbsp;     * Returns a new MetadataSnapshot for the given commit. If the given commit is &lt;code&gt;null&lt;/code&gt;
<i>267</i>&nbsp;     * the latest commit point is used.
<i>268</i>&nbsp;     *
<i>269</i>&nbsp;     * Note that this method requires the caller verify it has the right to access the store and
<i>270</i>&nbsp;     * no concurrent file changes are happening. If in doubt, you probably want to use one of the following:
<i>271</i>&nbsp;     *
<i>272</i>&nbsp;     * {@link #readMetadataSnapshot(Path, ShardId, NodeEnvironment.ShardLocker, Logger)} to read a meta data while locking
<i>273</i>&nbsp;     * {@link IndexShard#snapshotStoreMetadata()} to safely read from an existing shard
<i>274</i>&nbsp;     * {@link IndexShard#acquireLastIndexCommit(boolean)} to get an {@link IndexCommit} which is safe to use but has to be freed
<i>275</i>&nbsp;     *
<i>276</i>&nbsp;     * @param commit the index commit to read the snapshot from or &lt;code&gt;null&lt;/code&gt; if the latest snapshot should be read from the
<i>277</i>&nbsp;     *               directory
<i>278</i>&nbsp;     * @param lockDirectory if &lt;code&gt;true&lt;/code&gt; the index writer lock will be obtained before reading the snapshot. This should
<i>279</i>&nbsp;     *                      only be used if there is no started shard using this store.
<i>280</i>&nbsp;     * @throws CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum mismatch or an
<i>281</i>&nbsp;     *                                    unexpected exception when opening the index reading the segments file.
<i>282</i>&nbsp;     * @throws IndexFormatTooOldException if the lucene index is too old to be opened.
<i>283</i>&nbsp;     * @throws IndexFormatTooNewException if the lucene index is too new to be opened.
<i>284</i>&nbsp;     * @throws FileNotFoundException      if one or more files referenced by a commit are not present.
<i>285</i>&nbsp;     * @throws NoSuchFileException        if one or more files referenced by a commit are not present.
<i>286</i>&nbsp;     * @throws IndexNotFoundException     if the commit point can&#39;t be found in this store
<i>287</i>&nbsp;     */
<i>288</i>&nbsp;    public MetadataSnapshot getMetadata(IndexCommit commit, boolean lockDirectory) throws IOException {
<b class="nc"><i>289</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>290</i>&nbsp;        failIfCorrupted();</b>
<b class="nc"><i>291</i>&nbsp;        assert lockDirectory ? commit == null : true : &quot;IW lock should not be obtained if there is a commit point available&quot;;</b>
<i>292</i>&nbsp;        // if we lock the directory we also acquire the write lock since that makes sure that nobody else tries to lock the IW
<i>293</i>&nbsp;        // on this store at the same time.
<b class="nc"><i>294</i>&nbsp;        java.util.concurrent.locks.Lock lock = lockDirectory ? metadataLock.writeLock() : metadataLock.readLock();</b>
<b class="nc"><i>295</i>&nbsp;        lock.lock();</b>
<b class="nc"><i>296</i>&nbsp;        try (Closeable ignored = lockDirectory ? directory.obtainLock(IndexWriter.WRITE_LOCK_NAME) : () -&gt; {} ) {</b>
<b class="nc"><i>297</i>&nbsp;            return new MetadataSnapshot(commit, directory, logger);</b>
<b class="nc"><i>298</i>&nbsp;        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {</b>
<b class="nc"><i>299</i>&nbsp;            markStoreCorrupted(ex);</b>
<b class="nc"><i>300</i>&nbsp;            throw ex;</b>
<i>301</i>&nbsp;        } finally {
<b class="nc"><i>302</i>&nbsp;            lock.unlock();</b>
<b class="nc"><i>303</i>&nbsp;        }</b>
<i>304</i>&nbsp;    }
<i>305</i>&nbsp;
<i>306</i>&nbsp;    /**
<i>307</i>&nbsp;     * Renames all the given files from the key of the map to the
<i>308</i>&nbsp;     * value of the map. All successfully renamed files are removed from the map in-place.
<i>309</i>&nbsp;     */
<i>310</i>&nbsp;    public void renameTempFilesSafe(Map&lt;String, String&gt; tempFileMap) throws IOException {
<i>311</i>&nbsp;        // this works just like a lucene commit - we rename all temp files and once we successfully
<i>312</i>&nbsp;        // renamed all the segments we rename the commit to ensure we don&#39;t leave half baked commits behind.
<b class="nc"><i>313</i>&nbsp;        final Map.Entry&lt;String, String&gt;[] entries = tempFileMap.entrySet().toArray(new Map.Entry[0]);</b>
<b class="nc"><i>314</i>&nbsp;        ArrayUtil.timSort(entries, (o1, o2) -&gt; {</b>
<b class="nc"><i>315</i>&nbsp;            String left = o1.getValue();</b>
<b class="nc"><i>316</i>&nbsp;            String right = o2.getValue();</b>
<b class="nc"><i>317</i>&nbsp;            if (left.startsWith(IndexFileNames.SEGMENTS) || right.startsWith(IndexFileNames.SEGMENTS)) {</b>
<b class="nc"><i>318</i>&nbsp;                if (left.startsWith(IndexFileNames.SEGMENTS) == false) {</b>
<b class="nc"><i>319</i>&nbsp;                    return -1;</b>
<b class="nc"><i>320</i>&nbsp;                } else if (right.startsWith(IndexFileNames.SEGMENTS) == false) {</b>
<b class="nc"><i>321</i>&nbsp;                    return 1;</b>
<i>322</i>&nbsp;                }
<i>323</i>&nbsp;            }
<b class="nc"><i>324</i>&nbsp;            return left.compareTo(right);</b>
<i>325</i>&nbsp;        });
<b class="nc"><i>326</i>&nbsp;        metadataLock.writeLock().lock();</b>
<i>327</i>&nbsp;        // we make sure that nobody fetches the metadata while we do this rename operation here to ensure we don&#39;t
<i>328</i>&nbsp;        // get exceptions if files are still open.
<b class="nc"><i>329</i>&nbsp;        try (Lock writeLock = directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {</b>
<b class="nc"><i>330</i>&nbsp;            for (Map.Entry&lt;String, String&gt; entry : entries) {</b>
<b class="nc"><i>331</i>&nbsp;                String tempFile = entry.getKey();</b>
<b class="nc"><i>332</i>&nbsp;                String origFile = entry.getValue();</b>
<i>333</i>&nbsp;                // first, go and delete the existing ones
<i>334</i>&nbsp;                try {
<b class="nc"><i>335</i>&nbsp;                    directory.deleteFile(origFile);</b>
<b class="nc"><i>336</i>&nbsp;                } catch (FileNotFoundException | NoSuchFileException e) {</b>
<b class="nc"><i>337</i>&nbsp;                } catch (Exception ex) {</b>
<b class="nc"><i>338</i>&nbsp;                    logger.debug(() -&gt; new ParameterizedMessage(&quot;failed to delete file [{}]&quot;, origFile), ex);</b>
<b class="nc"><i>339</i>&nbsp;                }</b>
<i>340</i>&nbsp;                // now, rename the files... and fail it it won&#39;t work
<b class="nc"><i>341</i>&nbsp;                directory.rename(tempFile, origFile);</b>
<b class="nc"><i>342</i>&nbsp;                final String remove = tempFileMap.remove(tempFile);</b>
<b class="nc"><i>343</i>&nbsp;                assert remove != null;</b>
<i>344</i>&nbsp;            }
<b class="nc"><i>345</i>&nbsp;            directory.syncMetaData();</b>
<b class="nc"><i>346</i>&nbsp;        } finally {</b>
<b class="nc"><i>347</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="nc"><i>348</i>&nbsp;        }</b>
<i>349</i>&nbsp;
<b class="nc"><i>350</i>&nbsp;    }</b>
<i>351</i>&nbsp;
<i>352</i>&nbsp;    /**
<i>353</i>&nbsp;     * Checks and returns the status of the existing index in this store.
<i>354</i>&nbsp;     *
<i>355</i>&nbsp;     * @param out where infoStream messages should go. See {@link CheckIndex#setInfoStream(PrintStream)}
<i>356</i>&nbsp;     */
<i>357</i>&nbsp;    public CheckIndex.Status checkIndex(PrintStream out) throws IOException {
<b class="nc"><i>358</i>&nbsp;        metadataLock.writeLock().lock();</b>
<b class="nc"><i>359</i>&nbsp;        try (CheckIndex checkIndex = new CheckIndex(directory)) {</b>
<b class="nc"><i>360</i>&nbsp;            checkIndex.setInfoStream(out);</b>
<b class="nc"><i>361</i>&nbsp;            return checkIndex.checkIndex();</b>
<b class="nc"><i>362</i>&nbsp;        } finally {</b>
<b class="nc"><i>363</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="nc"><i>364</i>&nbsp;        }</b>
<i>365</i>&nbsp;    }
<i>366</i>&nbsp;
<i>367</i>&nbsp;    public StoreStats stats() throws IOException {
<b class="nc"><i>368</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>369</i>&nbsp;        return new StoreStats(directory.estimateSize());</b>
<i>370</i>&nbsp;    }
<i>371</i>&nbsp;
<i>372</i>&nbsp;    /**
<i>373</i>&nbsp;     * Increments the refCount of this Store instance.  RefCounts are used to determine when a
<i>374</i>&nbsp;     * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a
<i>375</i>&nbsp;     * corresponding {@link #decRef}, in a finally clause; otherwise the store may never be closed.  Note that
<i>376</i>&nbsp;     * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link
<i>377</i>&nbsp;     * #decRef} has been called for all outstanding references.
<i>378</i>&nbsp;     * &lt;p&gt;
<i>379</i>&nbsp;     * Note: Close can safely be called multiple times.
<i>380</i>&nbsp;     *
<i>381</i>&nbsp;     * @throws AlreadyClosedException iff the reference counter can not be incremented.
<i>382</i>&nbsp;     * @see #decRef
<i>383</i>&nbsp;     * @see #tryIncRef()
<i>384</i>&nbsp;     */
<i>385</i>&nbsp;    @Override
<i>386</i>&nbsp;    public final void incRef() {
<b class="fc"><i>387</i>&nbsp;        refCounter.incRef();</b>
<b class="fc"><i>388</i>&nbsp;    }</b>
<i>389</i>&nbsp;
<i>390</i>&nbsp;    /**
<i>391</i>&nbsp;     * Tries to increment the refCount of this Store instance. This method will return {@code true} iff the refCount was
<i>392</i>&nbsp;     * incremented successfully otherwise {@code false}. RefCounts are used to determine when a
<i>393</i>&nbsp;     * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a
<i>394</i>&nbsp;     * corresponding {@link #decRef}, in a finally clause; otherwise the store may never be closed.  Note that
<i>395</i>&nbsp;     * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link
<i>396</i>&nbsp;     * #decRef} has been called for all outstanding references.
<i>397</i>&nbsp;     * &lt;p&gt;
<i>398</i>&nbsp;     * Note: Close can safely be called multiple times.
<i>399</i>&nbsp;     *
<i>400</i>&nbsp;     * @see #decRef()
<i>401</i>&nbsp;     * @see #incRef()
<i>402</i>&nbsp;     */
<i>403</i>&nbsp;    @Override
<i>404</i>&nbsp;    public final boolean tryIncRef() {
<b class="fc"><i>405</i>&nbsp;        return refCounter.tryIncRef();</b>
<i>406</i>&nbsp;    }
<i>407</i>&nbsp;
<i>408</i>&nbsp;    /**
<i>409</i>&nbsp;     * Decreases the refCount of this Store instance. If the refCount drops to 0, then this
<i>410</i>&nbsp;     * store is closed.
<i>411</i>&nbsp;     *
<i>412</i>&nbsp;     * @see #incRef
<i>413</i>&nbsp;     */
<i>414</i>&nbsp;    @Override
<i>415</i>&nbsp;    public final void decRef() {
<b class="fc"><i>416</i>&nbsp;        refCounter.decRef();</b>
<b class="fc"><i>417</i>&nbsp;    }</b>
<i>418</i>&nbsp;
<i>419</i>&nbsp;    @Override
<i>420</i>&nbsp;    public void close() {
<i>421</i>&nbsp;
<b class="fc"><i>422</i>&nbsp;        if (isClosed.compareAndSet(false, true)) {</b>
<i>423</i>&nbsp;            // only do this once!
<b class="fc"><i>424</i>&nbsp;            decRef();</b>
<b class="fc"><i>425</i>&nbsp;            logger.debug(&quot;store reference count on close: {}&quot;, refCounter.refCount());</b>
<i>426</i>&nbsp;        }
<b class="fc"><i>427</i>&nbsp;    }</b>
<i>428</i>&nbsp;
<i>429</i>&nbsp;    private void closeInternal() {
<i>430</i>&nbsp;        // Leverage try-with-resources to close the shard lock for us
<b class="fc"><i>431</i>&nbsp;        try (Closeable c = shardLock) {</b>
<i>432</i>&nbsp;            try {
<b class="fc"><i>433</i>&nbsp;                directory.innerClose(); // this closes the distributorDirectory as well</b>
<i>434</i>&nbsp;            } finally {
<b class="fc"><i>435</i>&nbsp;                onClose.accept(shardLock);</b>
<b class="fc"><i>436</i>&nbsp;            }</b>
<b class="fc"><i>437</i>&nbsp;        } catch (IOException e) {</b>
<b class="nc"><i>438</i>&nbsp;            throw new UncheckedIOException(e);</b>
<b class="fc"><i>439</i>&nbsp;        }</b>
<b class="fc"><i>440</i>&nbsp;    }</b>
<i>441</i>&nbsp;
<i>442</i>&nbsp;    /**
<i>443</i>&nbsp;     * Reads a MetadataSnapshot from the given index locations or returns an empty snapshot if it can&#39;t be read.
<i>444</i>&nbsp;     *
<i>445</i>&nbsp;     * @throws IOException if the index we try to read is corrupted
<i>446</i>&nbsp;     */
<i>447</i>&nbsp;    public static MetadataSnapshot readMetadataSnapshot(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,
<i>448</i>&nbsp;                                                        Logger logger) throws IOException {
<b class="nc"><i>449</i>&nbsp;        try (ShardLock lock = shardLocker.lock(shardId, &quot;read metadata snapshot&quot;, TimeUnit.SECONDS.toMillis(5));</b>
<b class="nc"><i>450</i>&nbsp;             Directory dir = new SimpleFSDirectory(indexLocation)) {</b>
<b class="nc"><i>451</i>&nbsp;            failIfCorrupted(dir, shardId);</b>
<b class="nc"><i>452</i>&nbsp;            return new MetadataSnapshot(null, dir, logger);</b>
<b class="nc"><i>453</i>&nbsp;        } catch (IndexNotFoundException ex) {</b>
<i>454</i>&nbsp;            // that&#39;s fine - happens all the time no need to log
<b class="nc"><i>455</i>&nbsp;        } catch (FileNotFoundException | NoSuchFileException ex) {</b>
<b class="nc"><i>456</i>&nbsp;            logger.info(&quot;Failed to open / find files while reading metadata snapshot&quot;, ex);</b>
<b class="nc"><i>457</i>&nbsp;        } catch (ShardLockObtainFailedException ex) {</b>
<b class="nc"><i>458</i>&nbsp;            logger.info(() -&gt; new ParameterizedMessage(&quot;{}: failed to obtain shard lock&quot;, shardId), ex);</b>
<b class="nc"><i>459</i>&nbsp;        }</b>
<b class="nc"><i>460</i>&nbsp;        return MetadataSnapshot.EMPTY;</b>
<i>461</i>&nbsp;    }
<i>462</i>&nbsp;
<i>463</i>&nbsp;    /**
<i>464</i>&nbsp;     * Tries to open an index for the given location. This includes reading the
<i>465</i>&nbsp;     * segment infos and possible corruption markers. If the index can not
<i>466</i>&nbsp;     * be opened, an exception is thrown
<i>467</i>&nbsp;     */
<i>468</i>&nbsp;    public static void tryOpenIndex(Path indexLocation, ShardId shardId, NodeEnvironment.ShardLocker shardLocker,
<i>469</i>&nbsp;                                        Logger logger) throws IOException, ShardLockObtainFailedException {
<b class="nc"><i>470</i>&nbsp;        try (ShardLock lock = shardLocker.lock(shardId, &quot;open index&quot;, TimeUnit.SECONDS.toMillis(5));</b>
<b class="nc"><i>471</i>&nbsp;             Directory dir = new SimpleFSDirectory(indexLocation)) {</b>
<b class="nc"><i>472</i>&nbsp;            failIfCorrupted(dir, shardId);</b>
<b class="nc"><i>473</i>&nbsp;            SegmentInfos segInfo = Lucene.readSegmentInfos(dir);</b>
<b class="nc"><i>474</i>&nbsp;            logger.trace(&quot;{} loaded segment info [{}]&quot;, shardId, segInfo);</b>
<b class="nc"><i>475</i>&nbsp;        }</b>
<b class="nc"><i>476</i>&nbsp;    }</b>
<i>477</i>&nbsp;
<i>478</i>&nbsp;    /**
<i>479</i>&nbsp;     * The returned IndexOutput validates the files checksum.
<i>480</i>&nbsp;     * &lt;p&gt;
<i>481</i>&nbsp;     * Note: Checksums are calculated by default since version 4.8.0. This method only adds the
<i>482</i>&nbsp;     * verification against the checksum in the given metadata and does not add any significant overhead.
<i>483</i>&nbsp;     */
<i>484</i>&nbsp;    public IndexOutput createVerifyingOutput(String fileName, final StoreFileMetaData metadata,
<i>485</i>&nbsp;                                                final IOContext context) throws IOException {
<b class="nc"><i>486</i>&nbsp;        IndexOutput output = directory().createOutput(fileName, context);</b>
<b class="nc"><i>487</i>&nbsp;        boolean success = false;</b>
<i>488</i>&nbsp;        try {
<b class="nc"><i>489</i>&nbsp;            assert metadata.writtenBy() != null;</b>
<b class="nc"><i>490</i>&nbsp;            output = new LuceneVerifyingIndexOutput(metadata, output);</b>
<b class="nc"><i>491</i>&nbsp;            success = true;</b>
<i>492</i>&nbsp;        } finally {
<b class="nc"><i>493</i>&nbsp;            if (success == false) {</b>
<b class="nc"><i>494</i>&nbsp;                IOUtils.closeWhileHandlingException(output);</b>
<i>495</i>&nbsp;            }
<b class="nc"><i>496</i>&nbsp;        }</b>
<b class="nc"><i>497</i>&nbsp;        return output;</b>
<i>498</i>&nbsp;    }
<i>499</i>&nbsp;
<i>500</i>&nbsp;    public static void verify(IndexOutput output) throws IOException {
<b class="nc"><i>501</i>&nbsp;        if (output instanceof VerifyingIndexOutput) {</b>
<b class="nc"><i>502</i>&nbsp;            ((VerifyingIndexOutput) output).verify();</b>
<i>503</i>&nbsp;        }
<b class="nc"><i>504</i>&nbsp;    }</b>
<i>505</i>&nbsp;
<i>506</i>&nbsp;    public IndexInput openVerifyingInput(String filename, IOContext context, StoreFileMetaData metadata) throws IOException {
<b class="nc"><i>507</i>&nbsp;        assert metadata.writtenBy() != null;</b>
<b class="nc"><i>508</i>&nbsp;        return new VerifyingIndexInput(directory().openInput(filename, context));</b>
<i>509</i>&nbsp;    }
<i>510</i>&nbsp;
<i>511</i>&nbsp;    public static void verify(IndexInput input) throws IOException {
<b class="nc"><i>512</i>&nbsp;        if (input instanceof VerifyingIndexInput) {</b>
<b class="nc"><i>513</i>&nbsp;            ((VerifyingIndexInput) input).verify();</b>
<i>514</i>&nbsp;        }
<b class="nc"><i>515</i>&nbsp;    }</b>
<i>516</i>&nbsp;
<i>517</i>&nbsp;    public boolean checkIntegrityNoException(StoreFileMetaData md) {
<b class="nc"><i>518</i>&nbsp;        return checkIntegrityNoException(md, directory());</b>
<i>519</i>&nbsp;    }
<i>520</i>&nbsp;
<i>521</i>&nbsp;    public static boolean checkIntegrityNoException(StoreFileMetaData md, Directory directory) {
<i>522</i>&nbsp;        try {
<b class="nc"><i>523</i>&nbsp;            checkIntegrity(md, directory);</b>
<b class="nc"><i>524</i>&nbsp;            return true;</b>
<b class="nc"><i>525</i>&nbsp;        } catch (IOException e) {</b>
<b class="nc"><i>526</i>&nbsp;            return false;</b>
<i>527</i>&nbsp;        }
<i>528</i>&nbsp;    }
<i>529</i>&nbsp;
<i>530</i>&nbsp;    public static void checkIntegrity(final StoreFileMetaData md, final Directory directory) throws IOException {
<b class="nc"><i>531</i>&nbsp;        try (IndexInput input = directory.openInput(md.name(), IOContext.READONCE)) {</b>
<b class="nc"><i>532</i>&nbsp;            if (input.length() != md.length()) { // first check the length no matter how old this file is</b>
<b class="nc"><i>533</i>&nbsp;                throw new CorruptIndexException(&quot;expected length=&quot; + md.length() + &quot; != actual length: &quot; + input.length() +</b>
<i>534</i>&nbsp;                    &quot; : file truncated?&quot;, input);
<i>535</i>&nbsp;            }
<i>536</i>&nbsp;            // throw exception if the file is corrupt
<b class="nc"><i>537</i>&nbsp;            String checksum = Store.digestToString(CodecUtil.checksumEntireFile(input));</b>
<i>538</i>&nbsp;            // throw exception if metadata is inconsistent
<b class="nc"><i>539</i>&nbsp;            if (!checksum.equals(md.checksum())) {</b>
<b class="nc"><i>540</i>&nbsp;                throw new CorruptIndexException(&quot;inconsistent metadata: lucene checksum=&quot; + checksum +</b>
<b class="nc"><i>541</i>&nbsp;                        &quot;, metadata checksum=&quot; + md.checksum(), input);</b>
<i>542</i>&nbsp;            }
<b class="nc"><i>543</i>&nbsp;        }</b>
<b class="nc"><i>544</i>&nbsp;    }</b>
<i>545</i>&nbsp;
<i>546</i>&nbsp;    public boolean isMarkedCorrupted() throws IOException {
<b class="nc"><i>547</i>&nbsp;        ensureOpen();</b>
<i>548</i>&nbsp;        /* marking a store as corrupted is basically adding a _corrupted to all
<i>549</i>&nbsp;         * the files. This prevent
<i>550</i>&nbsp;         */
<b class="nc"><i>551</i>&nbsp;        final String[] files = directory().listAll();</b>
<b class="nc"><i>552</i>&nbsp;        for (String file : files) {</b>
<b class="nc"><i>553</i>&nbsp;            if (file.startsWith(CORRUPTED)) {</b>
<b class="nc"><i>554</i>&nbsp;                return true;</b>
<i>555</i>&nbsp;            }
<i>556</i>&nbsp;        }
<b class="nc"><i>557</i>&nbsp;        return false;</b>
<i>558</i>&nbsp;    }
<i>559</i>&nbsp;
<i>560</i>&nbsp;    /**
<i>561</i>&nbsp;     * Deletes all corruption markers from this store.
<i>562</i>&nbsp;     */
<i>563</i>&nbsp;    public void removeCorruptionMarker() throws IOException {
<b class="nc"><i>564</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>565</i>&nbsp;        final Directory directory = directory();</b>
<b class="nc"><i>566</i>&nbsp;        IOException firstException = null;</b>
<b class="nc"><i>567</i>&nbsp;        final String[] files = directory.listAll();</b>
<b class="nc"><i>568</i>&nbsp;        for (String file : files) {</b>
<b class="nc"><i>569</i>&nbsp;            if (file.startsWith(CORRUPTED)) {</b>
<i>570</i>&nbsp;                try {
<b class="nc"><i>571</i>&nbsp;                    directory.deleteFile(file);</b>
<b class="nc"><i>572</i>&nbsp;                } catch (IOException ex) {</b>
<b class="nc"><i>573</i>&nbsp;                    if (firstException == null) {</b>
<b class="nc"><i>574</i>&nbsp;                        firstException = ex;</b>
<i>575</i>&nbsp;                    } else {
<b class="nc"><i>576</i>&nbsp;                        firstException.addSuppressed(ex);</b>
<i>577</i>&nbsp;                    }
<b class="nc"><i>578</i>&nbsp;                }</b>
<i>579</i>&nbsp;            }
<i>580</i>&nbsp;        }
<b class="nc"><i>581</i>&nbsp;        if (firstException != null) {</b>
<b class="nc"><i>582</i>&nbsp;            throw firstException;</b>
<i>583</i>&nbsp;        }
<b class="nc"><i>584</i>&nbsp;    }</b>
<i>585</i>&nbsp;
<i>586</i>&nbsp;    public void failIfCorrupted() throws IOException {
<b class="fc"><i>587</i>&nbsp;        ensureOpen();</b>
<b class="fc"><i>588</i>&nbsp;        failIfCorrupted(directory, shardId);</b>
<b class="fc"><i>589</i>&nbsp;    }</b>
<i>590</i>&nbsp;
<i>591</i>&nbsp;    private static void failIfCorrupted(Directory directory, ShardId shardId) throws IOException {
<b class="fc"><i>592</i>&nbsp;        final String[] files = directory.listAll();</b>
<b class="fc"><i>593</i>&nbsp;        List&lt;CorruptIndexException&gt; ex = new ArrayList&lt;&gt;();</b>
<b class="fc"><i>594</i>&nbsp;        for (String file : files) {</b>
<b class="fc"><i>595</i>&nbsp;            if (file.startsWith(CORRUPTED)) {</b>
<b class="nc"><i>596</i>&nbsp;                try (ChecksumIndexInput input = directory.openChecksumInput(file, IOContext.READONCE)) {</b>
<b class="nc"><i>597</i>&nbsp;                    int version = CodecUtil.checkHeader(input, CODEC, VERSION_START, VERSION);</b>
<i>598</i>&nbsp;
<b class="nc"><i>599</i>&nbsp;                    if (version == VERSION_WRITE_THROWABLE) {</b>
<b class="nc"><i>600</i>&nbsp;                        final int size = input.readVInt();</b>
<b class="nc"><i>601</i>&nbsp;                        final byte[] buffer = new byte[size];</b>
<b class="nc"><i>602</i>&nbsp;                        input.readBytes(buffer, 0, buffer.length);</b>
<b class="nc"><i>603</i>&nbsp;                        StreamInput in = StreamInput.wrap(buffer);</b>
<b class="nc"><i>604</i>&nbsp;                        Exception t = in.readException();</b>
<b class="nc"><i>605</i>&nbsp;                        if (t instanceof CorruptIndexException) {</b>
<b class="nc"><i>606</i>&nbsp;                            ex.add((CorruptIndexException) t);</b>
<i>607</i>&nbsp;                        } else {
<b class="nc"><i>608</i>&nbsp;                            ex.add(new CorruptIndexException(t.getMessage(), &quot;preexisting_corruption&quot;, t));</b>
<i>609</i>&nbsp;                        }
<b class="nc"><i>610</i>&nbsp;                    } else {</b>
<b class="nc"><i>611</i>&nbsp;                        assert version == VERSION_START || version == VERSION_STACK_TRACE;</b>
<b class="nc"><i>612</i>&nbsp;                        String msg = input.readString();</b>
<b class="nc"><i>613</i>&nbsp;                        StringBuilder builder = new StringBuilder(shardId.toString());</b>
<b class="nc"><i>614</i>&nbsp;                        builder.append(&quot; Preexisting corrupted index [&quot;);</b>
<b class="nc"><i>615</i>&nbsp;                        builder.append(file).append(&quot;] caused by: &quot;);</b>
<b class="nc"><i>616</i>&nbsp;                        builder.append(msg);</b>
<b class="nc"><i>617</i>&nbsp;                        if (version == VERSION_STACK_TRACE) {</b>
<b class="nc"><i>618</i>&nbsp;                            builder.append(System.lineSeparator());</b>
<b class="nc"><i>619</i>&nbsp;                            builder.append(input.readString());</b>
<i>620</i>&nbsp;                        }
<b class="nc"><i>621</i>&nbsp;                        ex.add(new CorruptIndexException(builder.toString(), &quot;preexisting_corruption&quot;));</b>
<i>622</i>&nbsp;                    }
<b class="nc"><i>623</i>&nbsp;                    CodecUtil.checkFooter(input);</b>
<b class="nc"><i>624</i>&nbsp;                }</b>
<i>625</i>&nbsp;            }
<i>626</i>&nbsp;        }
<b class="fc"><i>627</i>&nbsp;        if (ex.isEmpty() == false) {</b>
<b class="nc"><i>628</i>&nbsp;            ExceptionsHelper.rethrowAndSuppress(ex);</b>
<i>629</i>&nbsp;        }
<b class="fc"><i>630</i>&nbsp;    }</b>
<i>631</i>&nbsp;
<i>632</i>&nbsp;    /**
<i>633</i>&nbsp;     * This method deletes every file in this store that is not contained in the given source meta data or is a
<i>634</i>&nbsp;     * legacy checksum file. After the delete it pulls the latest metadata snapshot from the store and compares it
<i>635</i>&nbsp;     * to the given snapshot. If the snapshots are inconsistent an illegal state exception is thrown.
<i>636</i>&nbsp;     *
<i>637</i>&nbsp;     * @param reason         the reason for this cleanup operation logged for each deleted file
<i>638</i>&nbsp;     * @param sourceMetaData the metadata used for cleanup. all files in this metadata should be kept around.
<i>639</i>&nbsp;     * @throws IOException           if an IOException occurs
<i>640</i>&nbsp;     * @throws IllegalStateException if the latest snapshot in this store differs from the given one after the cleanup.
<i>641</i>&nbsp;     */
<i>642</i>&nbsp;    public void cleanupAndVerify(String reason, MetadataSnapshot sourceMetaData) throws IOException {
<b class="nc"><i>643</i>&nbsp;        metadataLock.writeLock().lock();</b>
<b class="nc"><i>644</i>&nbsp;        try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {</b>
<b class="nc"><i>645</i>&nbsp;            for (String existingFile : directory.listAll()) {</b>
<b class="nc"><i>646</i>&nbsp;                if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {</b>
<i>647</i>&nbsp;                    // don&#39;t delete snapshot file, or the checksums file (note, this is extra protection since the Store won&#39;t delete
<i>648</i>&nbsp;                    // checksum)
<b class="nc"><i>649</i>&nbsp;                    continue;</b>
<i>650</i>&nbsp;                }
<i>651</i>&nbsp;                try {
<b class="nc"><i>652</i>&nbsp;                    directory.deleteFile(reason, existingFile);</b>
<i>653</i>&nbsp;                    // FNF should not happen since we hold a write lock?
<b class="nc"><i>654</i>&nbsp;                } catch (IOException ex) {</b>
<b class="nc"><i>655</i>&nbsp;                    if (existingFile.startsWith(IndexFileNames.SEGMENTS)</b>
<b class="nc"><i>656</i>&nbsp;                            || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN)</b>
<b class="nc"><i>657</i>&nbsp;                            || existingFile.startsWith(CORRUPTED)) {</b>
<i>658</i>&nbsp;                        // TODO do we need to also fail this if we can&#39;t delete the pending commit file?
<i>659</i>&nbsp;                        // if one of those files can&#39;t be deleted we better fail the cleanup otherwise we might leave an old commit
<i>660</i>&nbsp;                        // point around?
<b class="nc"><i>661</i>&nbsp;                        throw new IllegalStateException(&quot;Can&#39;t delete &quot; + existingFile + &quot; - cleanup failed&quot;, ex);</b>
<i>662</i>&nbsp;                    }
<b class="nc"><i>663</i>&nbsp;                    logger.debug(() -&gt; new ParameterizedMessage(&quot;failed to delete file [{}]&quot;, existingFile), ex);</b>
<i>664</i>&nbsp;                    // ignore, we don&#39;t really care, will get deleted later on
<b class="nc"><i>665</i>&nbsp;                }</b>
<i>666</i>&nbsp;            }
<b class="nc"><i>667</i>&nbsp;            directory.syncMetaData();</b>
<b class="nc"><i>668</i>&nbsp;            final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null);</b>
<b class="nc"><i>669</i>&nbsp;            verifyAfterCleanup(sourceMetaData, metadataOrEmpty);</b>
<b class="nc"><i>670</i>&nbsp;        } finally {</b>
<b class="nc"><i>671</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="nc"><i>672</i>&nbsp;        }</b>
<b class="nc"><i>673</i>&nbsp;    }</b>
<i>674</i>&nbsp;
<i>675</i>&nbsp;    // pkg private for testing
<i>676</i>&nbsp;    final void verifyAfterCleanup(MetadataSnapshot sourceMetaData, MetadataSnapshot targetMetaData) {
<b class="nc"><i>677</i>&nbsp;        final RecoveryDiff recoveryDiff = targetMetaData.recoveryDiff(sourceMetaData);</b>
<b class="nc"><i>678</i>&nbsp;        if (recoveryDiff.identical.size() != recoveryDiff.size()) {</b>
<b class="nc"><i>679</i>&nbsp;            if (recoveryDiff.missing.isEmpty()) {</b>
<b class="nc"><i>680</i>&nbsp;                for (StoreFileMetaData meta : recoveryDiff.different) {</b>
<b class="nc"><i>681</i>&nbsp;                    StoreFileMetaData local = targetMetaData.get(meta.name());</b>
<b class="nc"><i>682</i>&nbsp;                    StoreFileMetaData remote = sourceMetaData.get(meta.name());</b>
<i>683</i>&nbsp;                    // if we have different files then they must have no checksums; otherwise something went wrong during recovery.
<i>684</i>&nbsp;                    // we have that problem when we have an empty index is only a segments_1 file so we can&#39;t tell if it&#39;s a Lucene 4.8 file
<i>685</i>&nbsp;                    // and therefore no checksum is included. That isn&#39;t a problem since we simply copy it over anyway but those files
<i>686</i>&nbsp;                    // come out as different in the diff. That&#39;s why we have to double check here again if the rest of it matches.
<i>687</i>&nbsp;
<i>688</i>&nbsp;                    // all is fine this file is just part of a commit or a segment that is different
<b class="nc"><i>689</i>&nbsp;                    if (local.isSame(remote) == false) {</b>
<b class="nc"><i>690</i>&nbsp;                        logger.debug(&quot;Files are different on the recovery target: {} &quot;, recoveryDiff);</b>
<b class="nc"><i>691</i>&nbsp;                        throw new IllegalStateException(&quot;local version: &quot; + local + &quot; is different from remote version after recovery: &quot; +</b>
<i>692</i>&nbsp;                            remote, null);
<i>693</i>&nbsp;                    }
<b class="nc"><i>694</i>&nbsp;                }</b>
<i>695</i>&nbsp;            } else {
<b class="nc"><i>696</i>&nbsp;                logger.debug(&quot;Files are missing on the recovery target: {} &quot;, recoveryDiff);</b>
<b class="nc"><i>697</i>&nbsp;                throw new IllegalStateException(&quot;Files are missing on the recovery target: [different=&quot;</b>
<i>698</i>&nbsp;                        + recoveryDiff.different + &quot;, missing=&quot; + recoveryDiff.missing + &#39;]&#39;, null);
<i>699</i>&nbsp;            }
<i>700</i>&nbsp;        }
<b class="nc"><i>701</i>&nbsp;    }</b>
<i>702</i>&nbsp;
<i>703</i>&nbsp;    /**
<i>704</i>&nbsp;     * Returns the current reference count.
<i>705</i>&nbsp;     */
<i>706</i>&nbsp;    public int refCount() {
<b class="nc"><i>707</i>&nbsp;        return refCounter.refCount();</b>
<i>708</i>&nbsp;    }
<i>709</i>&nbsp;
<b class="fc"><i>710</i>&nbsp;    static final class StoreDirectory extends FilterDirectory {</b>
<i>711</i>&nbsp;
<i>712</i>&nbsp;        private final Logger deletesLogger;
<i>713</i>&nbsp;
<i>714</i>&nbsp;        StoreDirectory(ByteSizeCachingDirectory delegateDirectory, Logger deletesLogger) {
<b class="fc"><i>715</i>&nbsp;            super(delegateDirectory);</b>
<b class="fc"><i>716</i>&nbsp;            this.deletesLogger = deletesLogger;</b>
<b class="fc"><i>717</i>&nbsp;        }</b>
<i>718</i>&nbsp;
<i>719</i>&nbsp;        /** Estimate the cumulative size of all files in this directory in bytes. */
<i>720</i>&nbsp;        long estimateSize() throws IOException {
<b class="nc"><i>721</i>&nbsp;            return ((ByteSizeCachingDirectory) getDelegate()).estimateSizeInBytes();</b>
<i>722</i>&nbsp;        }
<i>723</i>&nbsp;
<i>724</i>&nbsp;        @Override
<i>725</i>&nbsp;        public void close() {
<b class="nc"><i>726</i>&nbsp;            assert false : &quot;Nobody should close this directory except of the Store itself&quot;;</b>
<b class="nc"><i>727</i>&nbsp;        }</b>
<i>728</i>&nbsp;
<i>729</i>&nbsp;        public void deleteFile(String msg, String name) throws IOException {
<b class="fc"><i>730</i>&nbsp;            deletesLogger.trace(&quot;{}: delete file {}&quot;, msg, name);</b>
<b class="fc"><i>731</i>&nbsp;            super.deleteFile(name);</b>
<b class="fc"><i>732</i>&nbsp;        }</b>
<i>733</i>&nbsp;
<i>734</i>&nbsp;        @Override
<i>735</i>&nbsp;        public void deleteFile(String name) throws IOException {
<b class="fc"><i>736</i>&nbsp;            deleteFile(&quot;StoreDirectory.deleteFile&quot;, name);</b>
<b class="fc"><i>737</i>&nbsp;        }</b>
<i>738</i>&nbsp;
<i>739</i>&nbsp;        private void innerClose() throws IOException {
<b class="fc"><i>740</i>&nbsp;            super.close();</b>
<b class="fc"><i>741</i>&nbsp;        }</b>
<i>742</i>&nbsp;
<i>743</i>&nbsp;        @Override
<i>744</i>&nbsp;        public String toString() {
<b class="fc"><i>745</i>&nbsp;            return &quot;store(&quot; + in.toString() + &quot;)&quot;;</b>
<i>746</i>&nbsp;        }
<i>747</i>&nbsp;
<i>748</i>&nbsp;        @Override
<i>749</i>&nbsp;        public Set&lt;String&gt; getPendingDeletions() throws IOException {
<i>750</i>&nbsp;            // FilterDirectory.getPendingDeletions does not delegate, working around it here.
<i>751</i>&nbsp;            // to be removed once fixed in FilterDirectory.
<b class="fc"><i>752</i>&nbsp;            return unwrap(this).getPendingDeletions();</b>
<i>753</i>&nbsp;        }
<i>754</i>&nbsp;    }
<i>755</i>&nbsp;
<i>756</i>&nbsp;    /**
<i>757</i>&nbsp;     * Represents a snapshot of the current directory build from the latest Lucene commit.
<i>758</i>&nbsp;     * Only files that are part of the last commit are considered in this datastructure.
<i>759</i>&nbsp;     * For backwards compatibility the snapshot might include legacy checksums that
<i>760</i>&nbsp;     * are derived from a dedicated checksum file written by older elasticsearch version pre 1.3
<i>761</i>&nbsp;     * &lt;p&gt;
<i>762</i>&nbsp;     * Note: This class will ignore the {@code segments.gen} file since it&#39;s optional and might
<i>763</i>&nbsp;     * change concurrently for safety reasons.
<i>764</i>&nbsp;     *
<i>765</i>&nbsp;     * @see StoreFileMetaData
<i>766</i>&nbsp;     */
<i>767</i>&nbsp;    public static final class MetadataSnapshot implements Iterable&lt;StoreFileMetaData&gt;, Writeable {
<i>768</i>&nbsp;        private final Map&lt;String, StoreFileMetaData&gt; metadata;
<i>769</i>&nbsp;
<i>770</i>&nbsp;        public static final MetadataSnapshot EMPTY = new MetadataSnapshot();
<i>771</i>&nbsp;
<i>772</i>&nbsp;        private final Map&lt;String, String&gt; commitUserData;
<i>773</i>&nbsp;
<i>774</i>&nbsp;        private final long numDocs;
<i>775</i>&nbsp;
<i>776</i>&nbsp;        public MetadataSnapshot(Map&lt;String, StoreFileMetaData&gt; metadata, Map&lt;String, String&gt; commitUserData, long numDocs) {
<i>777</i>&nbsp;            this.metadata = metadata;
<i>778</i>&nbsp;            this.commitUserData = commitUserData;
<i>779</i>&nbsp;            this.numDocs = numDocs;
<i>780</i>&nbsp;        }
<i>781</i>&nbsp;
<i>782</i>&nbsp;        MetadataSnapshot() {
<i>783</i>&nbsp;            metadata = emptyMap();
<i>784</i>&nbsp;            commitUserData = emptyMap();
<i>785</i>&nbsp;            numDocs = 0;
<i>786</i>&nbsp;        }
<i>787</i>&nbsp;
<i>788</i>&nbsp;        MetadataSnapshot(IndexCommit commit, Directory directory, Logger logger) throws IOException {
<i>789</i>&nbsp;            LoadedMetadata loadedMetadata = loadMetadata(commit, directory, logger);
<i>790</i>&nbsp;            metadata = loadedMetadata.fileMetadata;
<i>791</i>&nbsp;            commitUserData = loadedMetadata.userData;
<i>792</i>&nbsp;            numDocs = loadedMetadata.numDocs;
<i>793</i>&nbsp;            assert metadata.isEmpty() || numSegmentFiles() == 1 : &quot;numSegmentFiles: &quot; + numSegmentFiles();
<i>794</i>&nbsp;        }
<i>795</i>&nbsp;
<i>796</i>&nbsp;        /**
<i>797</i>&nbsp;         * Read from a stream.
<i>798</i>&nbsp;         */
<i>799</i>&nbsp;        public MetadataSnapshot(StreamInput in) throws IOException {
<i>800</i>&nbsp;            final int size = in.readVInt();
<i>801</i>&nbsp;            Map&lt;String, StoreFileMetaData&gt; metadata = new HashMap&lt;&gt;();
<i>802</i>&nbsp;            for (int i = 0; i &lt; size; i++) {
<i>803</i>&nbsp;                StoreFileMetaData meta = new StoreFileMetaData(in);
<i>804</i>&nbsp;                metadata.put(meta.name(), meta);
<i>805</i>&nbsp;            }
<i>806</i>&nbsp;            Map&lt;String, String&gt; commitUserData = new HashMap&lt;&gt;();
<i>807</i>&nbsp;            int num = in.readVInt();
<i>808</i>&nbsp;            for (int i = num; i &gt; 0; i--) {
<i>809</i>&nbsp;                commitUserData.put(in.readString(), in.readString());
<i>810</i>&nbsp;            }
<i>811</i>&nbsp;
<i>812</i>&nbsp;            this.metadata = unmodifiableMap(metadata);
<i>813</i>&nbsp;            this.commitUserData = unmodifiableMap(commitUserData);
<i>814</i>&nbsp;            this.numDocs = in.readLong();
<i>815</i>&nbsp;            assert metadata.isEmpty() || numSegmentFiles() == 1 : &quot;numSegmentFiles: &quot; + numSegmentFiles();
<i>816</i>&nbsp;        }
<i>817</i>&nbsp;
<i>818</i>&nbsp;        @Override
<i>819</i>&nbsp;        public void writeTo(StreamOutput out) throws IOException {
<i>820</i>&nbsp;            out.writeVInt(this.metadata.size());
<i>821</i>&nbsp;            for (StoreFileMetaData meta : this) {
<i>822</i>&nbsp;                meta.writeTo(out);
<i>823</i>&nbsp;            }
<i>824</i>&nbsp;            out.writeVInt(commitUserData.size());
<i>825</i>&nbsp;            for (Map.Entry&lt;String, String&gt; entry : commitUserData.entrySet()) {
<i>826</i>&nbsp;                out.writeString(entry.getKey());
<i>827</i>&nbsp;                out.writeString(entry.getValue());
<i>828</i>&nbsp;            }
<i>829</i>&nbsp;            out.writeLong(numDocs);
<i>830</i>&nbsp;        }
<i>831</i>&nbsp;
<i>832</i>&nbsp;        /**
<i>833</i>&nbsp;         * Returns the number of documents in this store snapshot
<i>834</i>&nbsp;         */
<i>835</i>&nbsp;        public long getNumDocs() {
<i>836</i>&nbsp;            return numDocs;
<i>837</i>&nbsp;        }
<i>838</i>&nbsp;
<i>839</i>&nbsp;        static class LoadedMetadata {
<i>840</i>&nbsp;            final Map&lt;String, StoreFileMetaData&gt; fileMetadata;
<i>841</i>&nbsp;            final Map&lt;String, String&gt; userData;
<i>842</i>&nbsp;            final long numDocs;
<i>843</i>&nbsp;
<i>844</i>&nbsp;            LoadedMetadata(Map&lt;String, StoreFileMetaData&gt; fileMetadata, Map&lt;String, String&gt; userData, long numDocs) {
<i>845</i>&nbsp;                this.fileMetadata = fileMetadata;
<i>846</i>&nbsp;                this.userData = userData;
<i>847</i>&nbsp;                this.numDocs = numDocs;
<i>848</i>&nbsp;            }
<i>849</i>&nbsp;        }
<i>850</i>&nbsp;
<i>851</i>&nbsp;        static LoadedMetadata loadMetadata(IndexCommit commit, Directory directory, Logger logger) throws IOException {
<i>852</i>&nbsp;            long numDocs;
<i>853</i>&nbsp;            Map&lt;String, StoreFileMetaData&gt; builder = new HashMap&lt;&gt;();
<i>854</i>&nbsp;            Map&lt;String, String&gt; commitUserDataBuilder = new HashMap&lt;&gt;();
<i>855</i>&nbsp;            try {
<i>856</i>&nbsp;                final SegmentInfos segmentCommitInfos = Store.readSegmentsInfo(commit, directory);
<i>857</i>&nbsp;                numDocs = Lucene.getNumDocs(segmentCommitInfos);
<i>858</i>&nbsp;                commitUserDataBuilder.putAll(segmentCommitInfos.getUserData());
<i>859</i>&nbsp;                // we don&#39;t know which version was used to write so we take the max version.
<i>860</i>&nbsp;                Version maxVersion = segmentCommitInfos.getMinSegmentLuceneVersion();
<i>861</i>&nbsp;                for (SegmentCommitInfo info : segmentCommitInfos) {
<i>862</i>&nbsp;                    final Version version = info.info.getVersion();
<i>863</i>&nbsp;                    if (version == null) {
<i>864</i>&nbsp;                        // version is written since 3.1+: we should have already hit IndexFormatTooOld.
<i>865</i>&nbsp;                        throw new IllegalArgumentException(&quot;expected valid version value: &quot; + info.info.toString());
<i>866</i>&nbsp;                    }
<i>867</i>&nbsp;                    if (version.onOrAfter(maxVersion)) {
<i>868</i>&nbsp;                        maxVersion = version;
<i>869</i>&nbsp;                    }
<i>870</i>&nbsp;                    for (String file : info.files()) {
<i>871</i>&nbsp;                        checksumFromLuceneFile(directory, file, builder, logger, version,
<i>872</i>&nbsp;                            SEGMENT_INFO_EXTENSION.equals(IndexFileNames.getExtension(file)));
<i>873</i>&nbsp;                    }
<i>874</i>&nbsp;                }
<i>875</i>&nbsp;                if (maxVersion == null) {
<i>876</i>&nbsp;                    maxVersion = org.elasticsearch.Version.CURRENT.minimumIndexCompatibilityVersion().luceneVersion;
<i>877</i>&nbsp;                }
<i>878</i>&nbsp;                final String segmentsFile = segmentCommitInfos.getSegmentsFileName();
<i>879</i>&nbsp;                checksumFromLuceneFile(directory, segmentsFile, builder, logger, maxVersion, true);
<i>880</i>&nbsp;            } catch (CorruptIndexException | IndexNotFoundException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
<i>881</i>&nbsp;                // we either know the index is corrupted or it&#39;s just not there
<i>882</i>&nbsp;                throw ex;
<i>883</i>&nbsp;            } catch (Exception ex) {
<i>884</i>&nbsp;                try {
<i>885</i>&nbsp;                    // Lucene checks the checksum after it tries to lookup the codec etc.
<i>886</i>&nbsp;                    // in that case we might get only IAE or similar exceptions while we are really corrupt...
<i>887</i>&nbsp;                    // TODO we should check the checksum in lucene if we hit an exception
<i>888</i>&nbsp;                    logger.warn(() -&gt;
<i>889</i>&nbsp;                        new ParameterizedMessage(&quot;failed to build store metadata. checking segment info integrity &quot; +
<i>890</i>&nbsp;                            &quot;(with commit [{}])&quot;, commit == null ? &quot;no&quot; : &quot;yes&quot;), ex);
<i>891</i>&nbsp;                    Lucene.checkSegmentInfoIntegrity(directory);
<i>892</i>&nbsp;                } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException cex) {
<i>893</i>&nbsp;                    cex.addSuppressed(ex);
<i>894</i>&nbsp;                    throw cex;
<i>895</i>&nbsp;                } catch (Exception inner) {
<i>896</i>&nbsp;                    inner.addSuppressed(ex);
<i>897</i>&nbsp;                    throw inner;
<i>898</i>&nbsp;                }
<i>899</i>&nbsp;                throw ex;
<i>900</i>&nbsp;            }
<i>901</i>&nbsp;            return new LoadedMetadata(unmodifiableMap(builder), unmodifiableMap(commitUserDataBuilder), numDocs);
<i>902</i>&nbsp;        }
<i>903</i>&nbsp;
<i>904</i>&nbsp;        private static void checksumFromLuceneFile(Directory directory, String file, Map&lt;String, StoreFileMetaData&gt; builder,
<i>905</i>&nbsp;                Logger logger, Version version, boolean readFileAsHash) throws IOException {
<i>906</i>&nbsp;            final String checksum;
<i>907</i>&nbsp;            final BytesRefBuilder fileHash = new BytesRefBuilder();
<i>908</i>&nbsp;            try (IndexInput in = directory.openInput(file, IOContext.READONCE)) {
<i>909</i>&nbsp;                final long length;
<i>910</i>&nbsp;                try {
<i>911</i>&nbsp;                    length = in.length();
<i>912</i>&nbsp;                    if (length &lt; CodecUtil.footerLength()) {
<i>913</i>&nbsp;                        // truncated files trigger IAE if we seek negative... these files are really corrupted though
<i>914</i>&nbsp;                        throw new CorruptIndexException(&quot;Can&#39;t retrieve checksum from file: &quot; + file + &quot; file length must be &gt;= &quot; +
<i>915</i>&nbsp;                            CodecUtil.footerLength() + &quot; but was: &quot; + in.length(), in);
<i>916</i>&nbsp;                    }
<i>917</i>&nbsp;                    if (readFileAsHash) {
<i>918</i>&nbsp;                        // additional safety we checksum the entire file we read the hash for...
<i>919</i>&nbsp;                        final VerifyingIndexInput verifyingIndexInput = new VerifyingIndexInput(in);
<i>920</i>&nbsp;                        hashFile(fileHash, new InputStreamIndexInput(verifyingIndexInput, length), length);
<i>921</i>&nbsp;                        checksum = digestToString(verifyingIndexInput.verify());
<i>922</i>&nbsp;                    } else {
<i>923</i>&nbsp;                        checksum = digestToString(CodecUtil.retrieveChecksum(in));
<i>924</i>&nbsp;                    }
<i>925</i>&nbsp;
<i>926</i>&nbsp;                } catch (Exception ex) {
<i>927</i>&nbsp;                    logger.debug(() -&gt; new ParameterizedMessage(&quot;Can retrieve checksum from file [{}]&quot;, file), ex);
<i>928</i>&nbsp;                    throw ex;
<i>929</i>&nbsp;                }
<i>930</i>&nbsp;                builder.put(file, new StoreFileMetaData(file, length, checksum, version, fileHash.get()));
<i>931</i>&nbsp;            }
<i>932</i>&nbsp;        }
<i>933</i>&nbsp;
<i>934</i>&nbsp;        /**
<i>935</i>&nbsp;         * Computes a strong hash value for small files. Note that this method should only be used for files &amp;lt; 1MB
<i>936</i>&nbsp;         */
<i>937</i>&nbsp;        public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {
<i>938</i>&nbsp;            final int len = (int) Math.min(1024 * 1024, size); // for safety we limit this to 1MB
<i>939</i>&nbsp;            fileHash.grow(len);
<i>940</i>&nbsp;            fileHash.setLength(len);
<i>941</i>&nbsp;            final int readBytes = Streams.readFully(in, fileHash.bytes(), 0, len);
<i>942</i>&nbsp;            assert readBytes == len : Integer.toString(readBytes) + &quot; != &quot; + Integer.toString(len);
<i>943</i>&nbsp;            assert fileHash.length() == len : Integer.toString(fileHash.length()) + &quot; != &quot; + Integer.toString(len);
<i>944</i>&nbsp;        }
<i>945</i>&nbsp;
<i>946</i>&nbsp;        @Override
<i>947</i>&nbsp;        public Iterator&lt;StoreFileMetaData&gt; iterator() {
<i>948</i>&nbsp;            return metadata.values().iterator();
<i>949</i>&nbsp;        }
<i>950</i>&nbsp;
<i>951</i>&nbsp;        public StoreFileMetaData get(String name) {
<i>952</i>&nbsp;            return metadata.get(name);
<i>953</i>&nbsp;        }
<i>954</i>&nbsp;
<i>955</i>&nbsp;        public Map&lt;String, StoreFileMetaData&gt; asMap() {
<i>956</i>&nbsp;            return metadata;
<i>957</i>&nbsp;        }
<i>958</i>&nbsp;
<i>959</i>&nbsp;        private static final String DEL_FILE_EXTENSION = &quot;del&quot;; // legacy delete file
<i>960</i>&nbsp;        private static final String LIV_FILE_EXTENSION = &quot;liv&quot;; // lucene 5 delete file
<i>961</i>&nbsp;        private static final String SEGMENT_INFO_EXTENSION = &quot;si&quot;;
<i>962</i>&nbsp;
<i>963</i>&nbsp;        /**
<i>964</i>&nbsp;         * Returns a diff between the two snapshots that can be used for recovery. The given snapshot is treated as the
<i>965</i>&nbsp;         * recovery target and this snapshot as the source. The returned diff will hold a list of files that are:
<i>966</i>&nbsp;         * &lt;ul&gt;
<i>967</i>&nbsp;         * &lt;li&gt;identical: they exist in both snapshots and they can be considered the same ie. they don&#39;t need to be recovered&lt;/li&gt;
<i>968</i>&nbsp;         * &lt;li&gt;different: they exist in both snapshots but their they are not identical&lt;/li&gt;
<i>969</i>&nbsp;         * &lt;li&gt;missing: files that exist in the source but not in the target&lt;/li&gt;
<i>970</i>&nbsp;         * &lt;/ul&gt;
<i>971</i>&nbsp;         * This method groups file into per-segment files and per-commit files. A file is treated as
<i>972</i>&nbsp;         * identical if and on if all files in it&#39;s group are identical. On a per-segment level files for a segment are treated
<i>973</i>&nbsp;         * as identical iff:
<i>974</i>&nbsp;         * &lt;ul&gt;
<i>975</i>&nbsp;         * &lt;li&gt;all files in this segment have the same checksum&lt;/li&gt;
<i>976</i>&nbsp;         * &lt;li&gt;all files in this segment have the same length&lt;/li&gt;
<i>977</i>&nbsp;         * &lt;li&gt;the segments {@code .si} files hashes are byte-identical Note: This is a using a perfect hash function,
<i>978</i>&nbsp;         * The metadata transfers the {@code .si} file content as it&#39;s hash&lt;/li&gt;
<i>979</i>&nbsp;         * &lt;/ul&gt;
<i>980</i>&nbsp;         * &lt;p&gt;
<i>981</i>&nbsp;         * The {@code .si} file contains a lot of diagnostics including a timestamp etc. in the future there might be
<i>982</i>&nbsp;         * unique segment identifiers in there hardening this method further.
<i>983</i>&nbsp;         * &lt;p&gt;
<i>984</i>&nbsp;         * The per-commit files handles very similar. A commit is composed of the {@code segments_N} files as well as generational files
<i>985</i>&nbsp;         * like deletes ({@code _x_y.del}) or field-info ({@code _x_y.fnm}) files. On a per-commit level files for a commit are treated
<i>986</i>&nbsp;         * as identical iff:
<i>987</i>&nbsp;         * &lt;ul&gt;
<i>988</i>&nbsp;         * &lt;li&gt;all files belonging to this commit have the same checksum&lt;/li&gt;
<i>989</i>&nbsp;         * &lt;li&gt;all files belonging to this commit have the same length&lt;/li&gt;
<i>990</i>&nbsp;         * &lt;li&gt;the segments file {@code segments_N} files hashes are byte-identical Note: This is a using a perfect hash function,
<i>991</i>&nbsp;         * The metadata transfers the {@code segments_N} file content as it&#39;s hash&lt;/li&gt;
<i>992</i>&nbsp;         * &lt;/ul&gt;
<i>993</i>&nbsp;         * &lt;p&gt;
<i>994</i>&nbsp;         * NOTE: this diff will not contain the {@code segments.gen} file. This file is omitted on recovery.
<i>995</i>&nbsp;         */
<i>996</i>&nbsp;        public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {
<i>997</i>&nbsp;            final List&lt;StoreFileMetaData&gt; identical = new ArrayList&lt;&gt;();
<i>998</i>&nbsp;            final List&lt;StoreFileMetaData&gt; different = new ArrayList&lt;&gt;();
<i>999</i>&nbsp;            final List&lt;StoreFileMetaData&gt; missing = new ArrayList&lt;&gt;();
<i>1000</i>&nbsp;            final Map&lt;String, List&lt;StoreFileMetaData&gt;&gt; perSegment = new HashMap&lt;&gt;();
<i>1001</i>&nbsp;            final List&lt;StoreFileMetaData&gt; perCommitStoreFiles = new ArrayList&lt;&gt;();
<i>1002</i>&nbsp;
<i>1003</i>&nbsp;            for (StoreFileMetaData meta : this) {
<i>1004</i>&nbsp;                if (IndexFileNames.OLD_SEGMENTS_GEN.equals(meta.name())) { // legacy
<i>1005</i>&nbsp;                    continue; // we don&#39;t need that file at all
<i>1006</i>&nbsp;                }
<i>1007</i>&nbsp;                final String segmentId = IndexFileNames.parseSegmentName(meta.name());
<i>1008</i>&nbsp;                final String extension = IndexFileNames.getExtension(meta.name());
<i>1009</i>&nbsp;                if (IndexFileNames.SEGMENTS.equals(segmentId) ||
<i>1010</i>&nbsp;                        DEL_FILE_EXTENSION.equals(extension) || LIV_FILE_EXTENSION.equals(extension)) {
<i>1011</i>&nbsp;                    // only treat del files as per-commit files fnm files are generational but only for upgradable DV
<i>1012</i>&nbsp;                    perCommitStoreFiles.add(meta);
<i>1013</i>&nbsp;                } else {
<i>1014</i>&nbsp;                    perSegment.computeIfAbsent(segmentId, k -&gt; new ArrayList&lt;&gt;()).add(meta);
<i>1015</i>&nbsp;                }
<i>1016</i>&nbsp;            }
<i>1017</i>&nbsp;            final ArrayList&lt;StoreFileMetaData&gt; identicalFiles = new ArrayList&lt;&gt;();
<i>1018</i>&nbsp;            for (List&lt;StoreFileMetaData&gt; segmentFiles : Iterables.concat(perSegment.values(), Collections.singleton(perCommitStoreFiles))) {
<i>1019</i>&nbsp;                identicalFiles.clear();
<i>1020</i>&nbsp;                boolean consistent = true;
<i>1021</i>&nbsp;                for (StoreFileMetaData meta : segmentFiles) {
<i>1022</i>&nbsp;                    StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot.get(meta.name());
<i>1023</i>&nbsp;                    if (storeFileMetaData == null) {
<i>1024</i>&nbsp;                        consistent = false;
<i>1025</i>&nbsp;                        missing.add(meta);
<i>1026</i>&nbsp;                    } else if (storeFileMetaData.isSame(meta) == false) {
<i>1027</i>&nbsp;                        consistent = false;
<i>1028</i>&nbsp;                        different.add(meta);
<i>1029</i>&nbsp;                    } else {
<i>1030</i>&nbsp;                        identicalFiles.add(meta);
<i>1031</i>&nbsp;                    }
<i>1032</i>&nbsp;                }
<i>1033</i>&nbsp;                if (consistent) {
<i>1034</i>&nbsp;                    identical.addAll(identicalFiles);
<i>1035</i>&nbsp;                } else {
<i>1036</i>&nbsp;                    // make sure all files are added - this can happen if only the deletes are different
<i>1037</i>&nbsp;                    different.addAll(identicalFiles);
<i>1038</i>&nbsp;                }
<i>1039</i>&nbsp;            }
<i>1040</i>&nbsp;            RecoveryDiff recoveryDiff = new RecoveryDiff(Collections.unmodifiableList(identical),
<i>1041</i>&nbsp;                Collections.unmodifiableList(different), Collections.unmodifiableList(missing));
<i>1042</i>&nbsp;            assert recoveryDiff.size() == this.metadata.size() - (metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) ? 1 : 0)
<i>1043</i>&nbsp;                    : &quot;some files are missing recoveryDiff size: [&quot; + recoveryDiff.size() + &quot;] metadata size: [&quot; +
<i>1044</i>&nbsp;                      this.metadata.size() + &quot;] contains  segments.gen: [&quot; + metadata.containsKey(IndexFileNames.OLD_SEGMENTS_GEN) + &quot;]&quot;;
<i>1045</i>&nbsp;            return recoveryDiff;
<i>1046</i>&nbsp;        }
<i>1047</i>&nbsp;
<i>1048</i>&nbsp;        /**
<i>1049</i>&nbsp;         * Returns the number of files in this snapshot
<i>1050</i>&nbsp;         */
<i>1051</i>&nbsp;        public int size() {
<i>1052</i>&nbsp;            return metadata.size();
<i>1053</i>&nbsp;        }
<i>1054</i>&nbsp;
<i>1055</i>&nbsp;        public Map&lt;String, String&gt; getCommitUserData() {
<i>1056</i>&nbsp;            return commitUserData;
<i>1057</i>&nbsp;        }
<i>1058</i>&nbsp;
<i>1059</i>&nbsp;        /**
<i>1060</i>&nbsp;         * returns the history uuid the store points at, or null if nonexistent.
<i>1061</i>&nbsp;         */
<i>1062</i>&nbsp;        public String getHistoryUUID() {
<i>1063</i>&nbsp;            return commitUserData.get(Engine.HISTORY_UUID_KEY);
<i>1064</i>&nbsp;        }
<i>1065</i>&nbsp;
<i>1066</i>&nbsp;        /**
<i>1067</i>&nbsp;         * Returns true iff this metadata contains the given file.
<i>1068</i>&nbsp;         */
<i>1069</i>&nbsp;        public boolean contains(String existingFile) {
<i>1070</i>&nbsp;            return metadata.containsKey(existingFile);
<i>1071</i>&nbsp;        }
<i>1072</i>&nbsp;
<i>1073</i>&nbsp;        /**
<i>1074</i>&nbsp;         * Returns the segments file that this metadata snapshot represents or null if the snapshot is empty.
<i>1075</i>&nbsp;         */
<i>1076</i>&nbsp;        public StoreFileMetaData getSegmentsFile() {
<i>1077</i>&nbsp;            for (StoreFileMetaData file : this) {
<i>1078</i>&nbsp;                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {
<i>1079</i>&nbsp;                    return file;
<i>1080</i>&nbsp;                }
<i>1081</i>&nbsp;            }
<i>1082</i>&nbsp;            assert metadata.isEmpty();
<i>1083</i>&nbsp;            return null;
<i>1084</i>&nbsp;        }
<i>1085</i>&nbsp;
<i>1086</i>&nbsp;        private int numSegmentFiles() { // only for asserts
<i>1087</i>&nbsp;            int count = 0;
<i>1088</i>&nbsp;            for (StoreFileMetaData file : this) {
<i>1089</i>&nbsp;                if (file.name().startsWith(IndexFileNames.SEGMENTS)) {
<i>1090</i>&nbsp;                    count++;
<i>1091</i>&nbsp;                }
<i>1092</i>&nbsp;            }
<i>1093</i>&nbsp;            return count;
<i>1094</i>&nbsp;        }
<i>1095</i>&nbsp;
<i>1096</i>&nbsp;        /**
<i>1097</i>&nbsp;         * Returns the sync id of the commit point that this MetadataSnapshot represents.
<i>1098</i>&nbsp;         *
<i>1099</i>&nbsp;         * @return sync id if exists, else null
<i>1100</i>&nbsp;         */
<i>1101</i>&nbsp;        public String getSyncId() {
<i>1102</i>&nbsp;            return commitUserData.get(Engine.SYNC_COMMIT_ID);
<i>1103</i>&nbsp;        }
<i>1104</i>&nbsp;    }
<i>1105</i>&nbsp;
<i>1106</i>&nbsp;    /**
<i>1107</i>&nbsp;     * A class representing the diff between a recovery source and recovery target
<i>1108</i>&nbsp;     *
<i>1109</i>&nbsp;     * @see MetadataSnapshot#recoveryDiff(org.elasticsearch.index.store.Store.MetadataSnapshot)
<i>1110</i>&nbsp;     */
<i>1111</i>&nbsp;    public static final class RecoveryDiff {
<i>1112</i>&nbsp;        /**
<i>1113</i>&nbsp;         * Files that exist in both snapshots and they can be considered the same ie. they don&#39;t need to be recovered
<i>1114</i>&nbsp;         */
<i>1115</i>&nbsp;        public final List&lt;StoreFileMetaData&gt; identical;
<i>1116</i>&nbsp;        /**
<i>1117</i>&nbsp;         * Files that exist in both snapshots but their they are not identical
<i>1118</i>&nbsp;         */
<i>1119</i>&nbsp;        public final List&lt;StoreFileMetaData&gt; different;
<i>1120</i>&nbsp;        /**
<i>1121</i>&nbsp;         * Files that exist in the source but not in the target
<i>1122</i>&nbsp;         */
<i>1123</i>&nbsp;        public final List&lt;StoreFileMetaData&gt; missing;
<i>1124</i>&nbsp;
<i>1125</i>&nbsp;        RecoveryDiff(List&lt;StoreFileMetaData&gt; identical, List&lt;StoreFileMetaData&gt; different, List&lt;StoreFileMetaData&gt; missing) {
<i>1126</i>&nbsp;            this.identical = identical;
<i>1127</i>&nbsp;            this.different = different;
<i>1128</i>&nbsp;            this.missing = missing;
<i>1129</i>&nbsp;        }
<i>1130</i>&nbsp;
<i>1131</i>&nbsp;        /**
<i>1132</i>&nbsp;         * Returns the sum of the files in this diff.
<i>1133</i>&nbsp;         */
<i>1134</i>&nbsp;        public int size() {
<i>1135</i>&nbsp;            return identical.size() + different.size() + missing.size();
<i>1136</i>&nbsp;        }
<i>1137</i>&nbsp;
<i>1138</i>&nbsp;        @Override
<i>1139</i>&nbsp;        public String toString() {
<i>1140</i>&nbsp;            return &quot;RecoveryDiff{&quot; +
<i>1141</i>&nbsp;                    &quot;identical=&quot; + identical +
<i>1142</i>&nbsp;                    &quot;, different=&quot; + different +
<i>1143</i>&nbsp;                    &quot;, missing=&quot; + missing +
<i>1144</i>&nbsp;                    &#39;}&#39;;
<i>1145</i>&nbsp;        }
<i>1146</i>&nbsp;    }
<i>1147</i>&nbsp;
<i>1148</i>&nbsp;
<i>1149</i>&nbsp;    /**
<i>1150</i>&nbsp;     * Returns true if the file is auto-generated by the store and shouldn&#39;t be deleted during cleanup.
<i>1151</i>&nbsp;     * This includes write lock and checksum files
<i>1152</i>&nbsp;     */
<i>1153</i>&nbsp;    public static boolean isAutogenerated(String name) {
<b class="nc"><i>1154</i>&nbsp;        return IndexWriter.WRITE_LOCK_NAME.equals(name);</b>
<i>1155</i>&nbsp;    }
<i>1156</i>&nbsp;
<i>1157</i>&nbsp;    /**
<i>1158</i>&nbsp;     * Produces a string representation of the given digest value.
<i>1159</i>&nbsp;     */
<i>1160</i>&nbsp;    public static String digestToString(long digest) {
<b class="nc"><i>1161</i>&nbsp;        return Long.toString(digest, Character.MAX_RADIX);</b>
<i>1162</i>&nbsp;    }
<i>1163</i>&nbsp;
<i>1164</i>&nbsp;
<i>1165</i>&nbsp;    static class LuceneVerifyingIndexOutput extends VerifyingIndexOutput {
<i>1166</i>&nbsp;
<i>1167</i>&nbsp;        private final StoreFileMetaData metadata;
<i>1168</i>&nbsp;        private long writtenBytes;
<i>1169</i>&nbsp;        private final long checksumPosition;
<i>1170</i>&nbsp;        private String actualChecksum;
<b class="nc"><i>1171</i>&nbsp;        private final byte[] footerChecksum = new byte[8]; // this holds the actual footer checksum data written by to this output</b>
<i>1172</i>&nbsp;
<i>1173</i>&nbsp;        LuceneVerifyingIndexOutput(StoreFileMetaData metadata, IndexOutput out) {
<b class="nc"><i>1174</i>&nbsp;            super(out);</b>
<b class="nc"><i>1175</i>&nbsp;            this.metadata = metadata;</b>
<b class="nc"><i>1176</i>&nbsp;            checksumPosition = metadata.length() - 8; // the last 8 bytes are the checksum - we store it in footerChecksum</b>
<b class="nc"><i>1177</i>&nbsp;        }</b>
<i>1178</i>&nbsp;
<i>1179</i>&nbsp;        @Override
<i>1180</i>&nbsp;        public void verify() throws IOException {
<b class="nc"><i>1181</i>&nbsp;            String footerDigest = null;</b>
<b class="nc"><i>1182</i>&nbsp;            if (metadata.checksum().equals(actualChecksum) &amp;&amp; writtenBytes == metadata.length()) {</b>
<b class="nc"><i>1183</i>&nbsp;                ByteArrayIndexInput indexInput = new ByteArrayIndexInput(&quot;checksum&quot;, this.footerChecksum);</b>
<b class="nc"><i>1184</i>&nbsp;                footerDigest = digestToString(indexInput.readLong());</b>
<b class="nc"><i>1185</i>&nbsp;                if (metadata.checksum().equals(footerDigest)) {</b>
<b class="nc"><i>1186</i>&nbsp;                    return;</b>
<i>1187</i>&nbsp;                }
<i>1188</i>&nbsp;            }
<b class="nc"><i>1189</i>&nbsp;            throw new CorruptIndexException(&quot;verification failed (hardware problem?) : expected=&quot; + metadata.checksum() +</b>
<i>1190</i>&nbsp;                    &quot; actual=&quot; + actualChecksum + &quot; footer=&quot; + footerDigest +&quot; writtenLength=&quot; + writtenBytes + &quot; expectedLength=&quot; +
<b class="nc"><i>1191</i>&nbsp;                    metadata.length() + &quot; (resource=&quot; + metadata.toString() + &quot;)&quot;, &quot;VerifyingIndexOutput(&quot; + metadata.name() + &quot;)&quot;);</b>
<i>1192</i>&nbsp;        }
<i>1193</i>&nbsp;
<i>1194</i>&nbsp;        @Override
<i>1195</i>&nbsp;        public void writeByte(byte b) throws IOException {
<b class="nc"><i>1196</i>&nbsp;            final long writtenBytes = this.writtenBytes++;</b>
<b class="nc"><i>1197</i>&nbsp;            if (writtenBytes &gt;= checksumPosition) { // we are writing parts of the checksum....</b>
<b class="nc"><i>1198</i>&nbsp;                if (writtenBytes == checksumPosition) {</b>
<b class="nc"><i>1199</i>&nbsp;                    readAndCompareChecksum();</b>
<i>1200</i>&nbsp;                }
<b class="nc"><i>1201</i>&nbsp;                final int index = Math.toIntExact(writtenBytes - checksumPosition);</b>
<b class="nc"><i>1202</i>&nbsp;                if (index &lt; footerChecksum.length) {</b>
<b class="nc"><i>1203</i>&nbsp;                    footerChecksum[index] = b;</b>
<b class="nc"><i>1204</i>&nbsp;                    if (index == footerChecksum.length-1) {</b>
<b class="nc"><i>1205</i>&nbsp;                        verify(); // we have recorded the entire checksum</b>
<i>1206</i>&nbsp;                    }
<i>1207</i>&nbsp;                } else {
<b class="nc"><i>1208</i>&nbsp;                    verify(); // fail if we write more than expected</b>
<b class="nc"><i>1209</i>&nbsp;                    throw new AssertionError(&quot;write past EOF expected length: &quot; + metadata.length() +</b>
<i>1210</i>&nbsp;                        &quot; writtenBytes: &quot; + writtenBytes);
<i>1211</i>&nbsp;                }
<i>1212</i>&nbsp;            }
<b class="nc"><i>1213</i>&nbsp;            out.writeByte(b);</b>
<b class="nc"><i>1214</i>&nbsp;        }</b>
<i>1215</i>&nbsp;
<i>1216</i>&nbsp;        private void readAndCompareChecksum() throws IOException {
<b class="nc"><i>1217</i>&nbsp;            actualChecksum = digestToString(getChecksum());</b>
<b class="nc"><i>1218</i>&nbsp;            if (!metadata.checksum().equals(actualChecksum)) {</b>
<b class="nc"><i>1219</i>&nbsp;                throw new CorruptIndexException(&quot;checksum failed (hardware problem?) : expected=&quot; + metadata.checksum() +</b>
<i>1220</i>&nbsp;                        &quot; actual=&quot; + actualChecksum +
<b class="nc"><i>1221</i>&nbsp;                        &quot; (resource=&quot; + metadata.toString() + &quot;)&quot;, &quot;VerifyingIndexOutput(&quot; + metadata.name() + &quot;)&quot;);</b>
<i>1222</i>&nbsp;            }
<b class="nc"><i>1223</i>&nbsp;        }</b>
<i>1224</i>&nbsp;
<i>1225</i>&nbsp;        @Override
<i>1226</i>&nbsp;        public void writeBytes(byte[] b, int offset, int length) throws IOException {
<b class="nc"><i>1227</i>&nbsp;            if (writtenBytes + length &gt; checksumPosition) {</b>
<b class="nc"><i>1228</i>&nbsp;                for (int i = 0; i &lt; length; i++) { // don&#39;t optimze writing the last block of bytes</b>
<b class="nc"><i>1229</i>&nbsp;                    writeByte(b[offset+i]);</b>
<i>1230</i>&nbsp;                }
<i>1231</i>&nbsp;            } else {
<b class="nc"><i>1232</i>&nbsp;                out.writeBytes(b, offset, length);</b>
<b class="nc"><i>1233</i>&nbsp;                writtenBytes += length;</b>
<i>1234</i>&nbsp;            }
<b class="nc"><i>1235</i>&nbsp;        }</b>
<i>1236</i>&nbsp;    }
<i>1237</i>&nbsp;
<i>1238</i>&nbsp;    /**
<i>1239</i>&nbsp;     * Index input that calculates checksum as data is read from the input.
<i>1240</i>&nbsp;     * &lt;p&gt;
<i>1241</i>&nbsp;     * This class supports random access (it is possible to seek backward and forward) in order to accommodate retry
<i>1242</i>&nbsp;     * mechanism that is used in some repository plugins (S3 for example). However, the checksum is only calculated on
<i>1243</i>&nbsp;     * the first read. All consecutive reads of the same data are not used to calculate the checksum.
<i>1244</i>&nbsp;     */
<b class="nc"><i>1245</i>&nbsp;    static class VerifyingIndexInput extends ChecksumIndexInput {</b>
<i>1246</i>&nbsp;        private final IndexInput input;
<i>1247</i>&nbsp;        private final Checksum digest;
<i>1248</i>&nbsp;        private final long checksumPosition;
<b class="nc"><i>1249</i>&nbsp;        private final byte[] checksum = new byte[8];</b>
<b class="nc"><i>1250</i>&nbsp;        private long verifiedPosition = 0;</b>
<i>1251</i>&nbsp;
<i>1252</i>&nbsp;        VerifyingIndexInput(IndexInput input) {
<b class="nc"><i>1253</i>&nbsp;            this(input, new BufferedChecksum(new CRC32()));</b>
<b class="nc"><i>1254</i>&nbsp;        }</b>
<i>1255</i>&nbsp;
<i>1256</i>&nbsp;        VerifyingIndexInput(IndexInput input, Checksum digest) {
<b class="nc"><i>1257</i>&nbsp;            super(&quot;VerifyingIndexInput(&quot; + input + &quot;)&quot;);</b>
<b class="nc"><i>1258</i>&nbsp;            this.input = input;</b>
<b class="nc"><i>1259</i>&nbsp;            this.digest = digest;</b>
<b class="nc"><i>1260</i>&nbsp;            checksumPosition = input.length() - 8;</b>
<b class="nc"><i>1261</i>&nbsp;        }</b>
<i>1262</i>&nbsp;
<i>1263</i>&nbsp;        @Override
<i>1264</i>&nbsp;        public byte readByte() throws IOException {
<b class="nc"><i>1265</i>&nbsp;            long pos = input.getFilePointer();</b>
<b class="nc"><i>1266</i>&nbsp;            final byte b = input.readByte();</b>
<b class="nc"><i>1267</i>&nbsp;            pos++;</b>
<b class="nc"><i>1268</i>&nbsp;            if (pos &gt; verifiedPosition) {</b>
<b class="nc"><i>1269</i>&nbsp;                if (pos &lt;= checksumPosition) {</b>
<b class="nc"><i>1270</i>&nbsp;                    digest.update(b);</b>
<i>1271</i>&nbsp;                } else {
<b class="nc"><i>1272</i>&nbsp;                    checksum[(int) (pos - checksumPosition - 1)] = b;</b>
<i>1273</i>&nbsp;                }
<b class="nc"><i>1274</i>&nbsp;                verifiedPosition = pos;</b>
<i>1275</i>&nbsp;            }
<b class="nc"><i>1276</i>&nbsp;            return b;</b>
<i>1277</i>&nbsp;        }
<i>1278</i>&nbsp;
<i>1279</i>&nbsp;        @Override
<i>1280</i>&nbsp;        public void readBytes(byte[] b, int offset, int len)
<i>1281</i>&nbsp;                throws IOException {
<b class="nc"><i>1282</i>&nbsp;            long pos = input.getFilePointer();</b>
<b class="nc"><i>1283</i>&nbsp;            input.readBytes(b, offset, len);</b>
<b class="nc"><i>1284</i>&nbsp;            if (pos + len &gt; verifiedPosition) {</b>
<i>1285</i>&nbsp;                // Conversion to int is safe here because (verifiedPosition - pos) can be at most len, which is integer
<b class="nc"><i>1286</i>&nbsp;                int alreadyVerified = (int) Math.max(0, verifiedPosition - pos);</b>
<b class="nc"><i>1287</i>&nbsp;                if (pos &lt; checksumPosition) {</b>
<b class="nc"><i>1288</i>&nbsp;                    if (pos + len &lt; checksumPosition) {</b>
<b class="nc"><i>1289</i>&nbsp;                        digest.update(b, offset + alreadyVerified, len - alreadyVerified);</b>
<i>1290</i>&nbsp;                    } else {
<b class="nc"><i>1291</i>&nbsp;                        int checksumOffset = (int) (checksumPosition - pos);</b>
<b class="nc"><i>1292</i>&nbsp;                        if (checksumOffset - alreadyVerified &gt; 0) {</b>
<b class="nc"><i>1293</i>&nbsp;                            digest.update(b, offset + alreadyVerified, checksumOffset - alreadyVerified);</b>
<i>1294</i>&nbsp;                        }
<b class="nc"><i>1295</i>&nbsp;                        System.arraycopy(b, offset + checksumOffset, checksum, 0, len - checksumOffset);</b>
<b class="nc"><i>1296</i>&nbsp;                    }</b>
<i>1297</i>&nbsp;                } else {
<i>1298</i>&nbsp;                    // Conversion to int is safe here because checksumPosition is (file length - 8) so
<i>1299</i>&nbsp;                    // (pos - checksumPosition) cannot be bigger than 8 unless we are reading after the end of file
<b class="nc"><i>1300</i>&nbsp;                    assert pos - checksumPosition &lt; 8;</b>
<b class="nc"><i>1301</i>&nbsp;                    System.arraycopy(b, offset, checksum, (int) (pos - checksumPosition), len);</b>
<i>1302</i>&nbsp;                }
<b class="nc"><i>1303</i>&nbsp;                verifiedPosition = pos + len;</b>
<i>1304</i>&nbsp;            }
<b class="nc"><i>1305</i>&nbsp;        }</b>
<i>1306</i>&nbsp;
<i>1307</i>&nbsp;        @Override
<i>1308</i>&nbsp;        public long getChecksum() {
<b class="nc"><i>1309</i>&nbsp;            return digest.getValue();</b>
<i>1310</i>&nbsp;        }
<i>1311</i>&nbsp;
<i>1312</i>&nbsp;        @Override
<i>1313</i>&nbsp;        public void seek(long pos) throws IOException {
<b class="nc"><i>1314</i>&nbsp;            if (pos &lt; verifiedPosition) {</b>
<i>1315</i>&nbsp;                // going within verified region - just seek there
<b class="nc"><i>1316</i>&nbsp;                input.seek(pos);</b>
<i>1317</i>&nbsp;            } else {
<b class="nc"><i>1318</i>&nbsp;                if (verifiedPosition &gt; getFilePointer()) {</b>
<i>1319</i>&nbsp;                    // portion of the skip region is verified and portion is not
<i>1320</i>&nbsp;                    // skipping the verified portion
<b class="nc"><i>1321</i>&nbsp;                    input.seek(verifiedPosition);</b>
<i>1322</i>&nbsp;                    // and checking unverified
<b class="nc"><i>1323</i>&nbsp;                    skipBytes(pos - verifiedPosition);</b>
<i>1324</i>&nbsp;                } else {
<b class="nc"><i>1325</i>&nbsp;                    skipBytes(pos - getFilePointer());</b>
<i>1326</i>&nbsp;                }
<i>1327</i>&nbsp;            }
<b class="nc"><i>1328</i>&nbsp;        }</b>
<i>1329</i>&nbsp;
<i>1330</i>&nbsp;        @Override
<i>1331</i>&nbsp;        public void close() throws IOException {
<b class="nc"><i>1332</i>&nbsp;            input.close();</b>
<b class="nc"><i>1333</i>&nbsp;        }</b>
<i>1334</i>&nbsp;
<i>1335</i>&nbsp;        @Override
<i>1336</i>&nbsp;        public long getFilePointer() {
<b class="nc"><i>1337</i>&nbsp;            return input.getFilePointer();</b>
<i>1338</i>&nbsp;        }
<i>1339</i>&nbsp;
<i>1340</i>&nbsp;        @Override
<i>1341</i>&nbsp;        public long length() {
<b class="nc"><i>1342</i>&nbsp;            return input.length();</b>
<i>1343</i>&nbsp;        }
<i>1344</i>&nbsp;
<i>1345</i>&nbsp;        @Override
<i>1346</i>&nbsp;        public IndexInput clone() {
<b class="nc"><i>1347</i>&nbsp;            throw new UnsupportedOperationException();</b>
<i>1348</i>&nbsp;        }
<i>1349</i>&nbsp;
<i>1350</i>&nbsp;        @Override
<i>1351</i>&nbsp;        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {
<b class="nc"><i>1352</i>&nbsp;            throw new UnsupportedOperationException();</b>
<i>1353</i>&nbsp;        }
<i>1354</i>&nbsp;
<i>1355</i>&nbsp;        public long getStoredChecksum() {
<b class="nc"><i>1356</i>&nbsp;            return new ByteArrayDataInput(checksum).readLong();</b>
<i>1357</i>&nbsp;        }
<i>1358</i>&nbsp;
<i>1359</i>&nbsp;        public long verify() throws CorruptIndexException {
<b class="nc"><i>1360</i>&nbsp;            long storedChecksum = getStoredChecksum();</b>
<b class="nc"><i>1361</i>&nbsp;            if (getChecksum() == storedChecksum) {</b>
<b class="nc"><i>1362</i>&nbsp;                return storedChecksum;</b>
<i>1363</i>&nbsp;            }
<b class="nc"><i>1364</i>&nbsp;            throw new CorruptIndexException(&quot;verification failed : calculated=&quot; + Store.digestToString(getChecksum()) +</b>
<b class="nc"><i>1365</i>&nbsp;                    &quot; stored=&quot; + Store.digestToString(storedChecksum), this);</b>
<i>1366</i>&nbsp;        }
<i>1367</i>&nbsp;
<i>1368</i>&nbsp;    }
<i>1369</i>&nbsp;
<i>1370</i>&nbsp;    public void deleteQuiet(String... files) {
<b class="nc"><i>1371</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>1372</i>&nbsp;        StoreDirectory directory = this.directory;</b>
<b class="nc"><i>1373</i>&nbsp;        for (String file : files) {</b>
<i>1374</i>&nbsp;            try {
<b class="nc"><i>1375</i>&nbsp;               directory.deleteFile(&quot;Store.deleteQuiet&quot;, file);</b>
<b class="nc"><i>1376</i>&nbsp;            } catch (Exception ex) {</b>
<i>1377</i>&nbsp;                // ignore :(
<b class="nc"><i>1378</i>&nbsp;            }</b>
<i>1379</i>&nbsp;        }
<b class="nc"><i>1380</i>&nbsp;    }</b>
<i>1381</i>&nbsp;
<i>1382</i>&nbsp;    /**
<i>1383</i>&nbsp;     * Marks this store as corrupted. This method writes a {@code corrupted_${uuid}} file containing the given exception
<i>1384</i>&nbsp;     * message. If a store contains a {@code corrupted_${uuid}} file {@link #isMarkedCorrupted()} will return &lt;code&gt;true&lt;/code&gt;.
<i>1385</i>&nbsp;     */
<i>1386</i>&nbsp;    public void markStoreCorrupted(IOException exception) throws IOException {
<b class="nc"><i>1387</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>1388</i>&nbsp;        if (!isMarkedCorrupted()) {</b>
<b class="nc"><i>1389</i>&nbsp;            String uuid = CORRUPTED + UUIDs.randomBase64UUID();</b>
<b class="nc"><i>1390</i>&nbsp;            try (IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {</b>
<b class="nc"><i>1391</i>&nbsp;                CodecUtil.writeHeader(output, CODEC, VERSION);</b>
<b class="nc"><i>1392</i>&nbsp;                BytesStreamOutput out = new BytesStreamOutput();</b>
<b class="nc"><i>1393</i>&nbsp;                out.writeException(exception);</b>
<b class="nc"><i>1394</i>&nbsp;                BytesReference bytes = out.bytes();</b>
<b class="nc"><i>1395</i>&nbsp;                output.writeVInt(bytes.length());</b>
<b class="nc"><i>1396</i>&nbsp;                BytesRef ref = bytes.toBytesRef();</b>
<b class="nc"><i>1397</i>&nbsp;                output.writeBytes(ref.bytes, ref.offset, ref.length);</b>
<b class="nc"><i>1398</i>&nbsp;                CodecUtil.writeFooter(output);</b>
<b class="nc"><i>1399</i>&nbsp;            } catch (IOException ex) {</b>
<b class="nc"><i>1400</i>&nbsp;                logger.warn(&quot;Can&#39;t mark store as corrupted&quot;, ex);</b>
<b class="nc"><i>1401</i>&nbsp;            }</b>
<b class="nc"><i>1402</i>&nbsp;            directory().sync(Collections.singleton(uuid));</b>
<i>1403</i>&nbsp;        }
<b class="nc"><i>1404</i>&nbsp;    }</b>
<i>1405</i>&nbsp;
<i>1406</i>&nbsp;    /**
<i>1407</i>&nbsp;     * A listener that is executed once the store is closed and all references to it are released
<i>1408</i>&nbsp;     */
<i>1409</i>&nbsp;    public interface OnClose extends Consumer&lt;ShardLock&gt; {
<b class="nc"><i>1410</i>&nbsp;        OnClose EMPTY = new OnClose() {</b>
<i>1411</i>&nbsp;            /**
<i>1412</i>&nbsp;             * This method is called while the provided {@link org.elasticsearch.env.ShardLock} is held.
<i>1413</i>&nbsp;             * This method is only called once after all resources for a store are released.
<i>1414</i>&nbsp;             */
<i>1415</i>&nbsp;            @Override
<i>1416</i>&nbsp;            public void accept(ShardLock Lock) {
<i>1417</i>&nbsp;            }
<i>1418</i>&nbsp;        };
<i>1419</i>&nbsp;    }
<i>1420</i>&nbsp;
<i>1421</i>&nbsp;    /**
<i>1422</i>&nbsp;     * creates an empty lucene index and a corresponding empty translog. Any existing data will be deleted.
<i>1423</i>&nbsp;     */
<i>1424</i>&nbsp;    public void createEmpty(Version luceneVersion) throws IOException {
<b class="fc"><i>1425</i>&nbsp;        metadataLock.writeLock().lock();</b>
<b class="fc"><i>1426</i>&nbsp;        try (IndexWriter writer = newEmptyIndexWriter(directory, luceneVersion)) {</b>
<b class="fc"><i>1427</i>&nbsp;            final Map&lt;String, String&gt; map = new HashMap&lt;&gt;();</b>
<b class="fc"><i>1428</i>&nbsp;            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID());</b>
<b class="fc"><i>1429</i>&nbsp;            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED));</b>
<b class="fc"><i>1430</i>&nbsp;            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED));</b>
<b class="fc"><i>1431</i>&nbsp;            map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, &quot;-1&quot;);</b>
<b class="fc"><i>1432</i>&nbsp;            updateCommitData(writer, map);</b>
<b class="fc"><i>1433</i>&nbsp;        } finally {</b>
<b class="fc"><i>1434</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="fc"><i>1435</i>&nbsp;        }</b>
<b class="fc"><i>1436</i>&nbsp;    }</b>
<i>1437</i>&nbsp;
<i>1438</i>&nbsp;
<i>1439</i>&nbsp;    /**
<i>1440</i>&nbsp;     * Marks an existing lucene index with a new history uuid.
<i>1441</i>&nbsp;     * This is used to make sure no existing shard will recovery from this index using ops based recovery.
<i>1442</i>&nbsp;     */
<i>1443</i>&nbsp;    public void bootstrapNewHistory() throws IOException {
<b class="nc"><i>1444</i>&nbsp;        metadataLock.writeLock().lock();</b>
<i>1445</i>&nbsp;        try {
<b class="nc"><i>1446</i>&nbsp;            Map&lt;String, String&gt; userData = readLastCommittedSegmentsInfo().getUserData();</b>
<b class="nc"><i>1447</i>&nbsp;            final long maxSeqNo = Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO));</b>
<b class="nc"><i>1448</i>&nbsp;            final long localCheckpoint = Long.parseLong(userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));</b>
<b class="nc"><i>1449</i>&nbsp;            bootstrapNewHistory(localCheckpoint, maxSeqNo);</b>
<i>1450</i>&nbsp;        } finally {
<b class="nc"><i>1451</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="nc"><i>1452</i>&nbsp;        }</b>
<b class="nc"><i>1453</i>&nbsp;    }</b>
<i>1454</i>&nbsp;
<i>1455</i>&nbsp;    /**
<i>1456</i>&nbsp;     * Marks an existing lucene index with a new history uuid and sets the given local checkpoint
<i>1457</i>&nbsp;     * as well as the maximum sequence number.
<i>1458</i>&nbsp;     * This is used to make sure no existing shard will recover from this index using ops based recovery.
<i>1459</i>&nbsp;     * @see SequenceNumbers#LOCAL_CHECKPOINT_KEY
<i>1460</i>&nbsp;     * @see SequenceNumbers#MAX_SEQ_NO
<i>1461</i>&nbsp;     */
<i>1462</i>&nbsp;    public void bootstrapNewHistory(long localCheckpoint, long maxSeqNo) throws IOException {
<b class="nc"><i>1463</i>&nbsp;        metadataLock.writeLock().lock();</b>
<b class="nc"><i>1464</i>&nbsp;        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {</b>
<b class="nc"><i>1465</i>&nbsp;            final Map&lt;String, String&gt; map = new HashMap&lt;&gt;();</b>
<b class="nc"><i>1466</i>&nbsp;            map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID());</b>
<b class="nc"><i>1467</i>&nbsp;            map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(localCheckpoint));</b>
<b class="nc"><i>1468</i>&nbsp;            map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(maxSeqNo));</b>
<b class="nc"><i>1469</i>&nbsp;            updateCommitData(writer, map);</b>
<b class="nc"><i>1470</i>&nbsp;        } finally {</b>
<b class="nc"><i>1471</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="nc"><i>1472</i>&nbsp;        }</b>
<b class="nc"><i>1473</i>&nbsp;    }</b>
<i>1474</i>&nbsp;
<i>1475</i>&nbsp;    /**
<i>1476</i>&nbsp;     * Force bakes the given translog generation as recovery information in the lucene index. This is
<i>1477</i>&nbsp;     * used when recovering from a snapshot or peer file based recovery where a new empty translog is
<i>1478</i>&nbsp;     * created and the existing lucene index needs should be changed to use it.
<i>1479</i>&nbsp;     */
<i>1480</i>&nbsp;    public void associateIndexWithNewTranslog(final String translogUUID) throws IOException {
<b class="fc"><i>1481</i>&nbsp;        metadataLock.writeLock().lock();</b>
<b class="fc"><i>1482</i>&nbsp;        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {</b>
<b class="fc"><i>1483</i>&nbsp;            if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {</b>
<b class="nc"><i>1484</i>&nbsp;                throw new IllegalArgumentException(&quot;a new translog uuid can&#39;t be equal to existing one. got [&quot; + translogUUID + &quot;]&quot;);</b>
<i>1485</i>&nbsp;            }
<b class="fc"><i>1486</i>&nbsp;            final Map&lt;String, String&gt; map = new HashMap&lt;&gt;();</b>
<b class="fc"><i>1487</i>&nbsp;            map.put(Translog.TRANSLOG_GENERATION_KEY, &quot;1&quot;);</b>
<b class="fc"><i>1488</i>&nbsp;            map.put(Translog.TRANSLOG_UUID_KEY, translogUUID);</b>
<b class="fc"><i>1489</i>&nbsp;            updateCommitData(writer, map);</b>
<b class="fc"><i>1490</i>&nbsp;        } finally {</b>
<b class="fc"><i>1491</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="fc"><i>1492</i>&nbsp;        }</b>
<b class="fc"><i>1493</i>&nbsp;    }</b>
<i>1494</i>&nbsp;
<i>1495</i>&nbsp;
<i>1496</i>&nbsp;    /**
<i>1497</i>&nbsp;     * Checks that the Lucene index contains a history uuid marker. If not, a new one is generated and committed.
<i>1498</i>&nbsp;     */
<i>1499</i>&nbsp;    public void ensureIndexHasHistoryUUID() throws IOException {
<b class="nc"><i>1500</i>&nbsp;        metadataLock.writeLock().lock();</b>
<b class="nc"><i>1501</i>&nbsp;        try (IndexWriter writer = newAppendingIndexWriter(directory, null)) {</b>
<b class="nc"><i>1502</i>&nbsp;            final Map&lt;String, String&gt; userData = getUserData(writer);</b>
<b class="nc"><i>1503</i>&nbsp;            if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {</b>
<b class="nc"><i>1504</i>&nbsp;                updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()));</b>
<i>1505</i>&nbsp;            }
<b class="nc"><i>1506</i>&nbsp;        } finally {</b>
<b class="nc"><i>1507</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="nc"><i>1508</i>&nbsp;        }</b>
<b class="nc"><i>1509</i>&nbsp;    }</b>
<i>1510</i>&nbsp;
<i>1511</i>&nbsp;    /**
<i>1512</i>&nbsp;     * Keeping existing unsafe commits when opening an engine can be problematic because these commits are not safe
<i>1513</i>&nbsp;     * at the recovering time but they can suddenly become safe in the future.
<i>1514</i>&nbsp;     * The following issues can happen if unsafe commits are kept oninit.
<i>1515</i>&nbsp;     * &lt;p&gt;
<i>1516</i>&nbsp;     * 1. Replica can use unsafe commit in peer-recovery. This happens when a replica with a safe commit c1(max_seqno=1)
<i>1517</i>&nbsp;     * and an unsafe commit c2(max_seqno=2) recovers from a primary with c1(max_seqno=1). If a new document(seqno=2)
<i>1518</i>&nbsp;     * is added without flushing, the global checkpoint is advanced to 2; and the replica recovers again, it will use
<i>1519</i>&nbsp;     * the unsafe commit c2(max_seqno=2 at most gcp=2) as the starting commit for sequenced-based recovery even the
<i>1520</i>&nbsp;     * commit c2 contains a stale operation and the document(with seqno=2) will not be replicated to the replica.
<i>1521</i>&nbsp;     * &lt;p&gt;
<i>1522</i>&nbsp;     * 2. Min translog gen for recovery can go backwards in peer-recovery. This happens when are replica with a safe commit
<i>1523</i>&nbsp;     * c1(local_checkpoint=1, recovery_translog_gen=1) and an unsafe commit c2(local_checkpoint=2, recovery_translog_gen=2).
<i>1524</i>&nbsp;     * The replica recovers from a primary, and keeps c2 as the last commit, then sets last_translog_gen to 2. Flushing a new
<i>1525</i>&nbsp;     * commit on the replica will cause exception as the new last commit c3 will have recovery_translog_gen=1. The recovery
<i>1526</i>&nbsp;     * translog generation of a commit is calculated based on the current local checkpoint. The local checkpoint of c3 is 1
<i>1527</i>&nbsp;     * while the local checkpoint of c2 is 2.
<i>1528</i>&nbsp;     * &lt;p&gt;
<i>1529</i>&nbsp;     * 3. Commit without translog can be used in recovery. An old index, which was created before multiple-commits is introduced
<i>1530</i>&nbsp;     * (v6.2), may not have a safe commit. If that index has a snapshotted commit without translog and an unsafe commit,
<i>1531</i>&nbsp;     * the policy can consider the snapshotted commit as a safe commit for recovery even the commit does not have translog.
<i>1532</i>&nbsp;     */
<i>1533</i>&nbsp;    public void trimUnsafeCommits(final long lastSyncedGlobalCheckpoint, final long minRetainedTranslogGen,
<i>1534</i>&nbsp;                                  final org.elasticsearch.Version indexVersionCreated) throws IOException {
<b class="fc"><i>1535</i>&nbsp;        metadataLock.writeLock().lock();</b>
<i>1536</i>&nbsp;        try {
<b class="fc"><i>1537</i>&nbsp;            final List&lt;IndexCommit&gt; existingCommits = DirectoryReader.listCommits(directory);</b>
<b class="fc"><i>1538</i>&nbsp;            if (existingCommits.isEmpty()) {</b>
<b class="nc"><i>1539</i>&nbsp;                throw new IllegalArgumentException(&quot;No index found to trim&quot;);</b>
<i>1540</i>&nbsp;            }
<b class="fc"><i>1541</i>&nbsp;            final IndexCommit lastIndexCommitCommit = existingCommits.get(existingCommits.size() - 1);</b>
<b class="fc"><i>1542</i>&nbsp;            final String translogUUID = lastIndexCommitCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY);</b>
<i>1543</i>&nbsp;            final IndexCommit startingIndexCommit;
<i>1544</i>&nbsp;            // We may not have a safe commit if an index was create before v6.2; and if there is a snapshotted commit whose translog
<i>1545</i>&nbsp;            // are not retained but max_seqno is at most the global checkpoint, we may mistakenly select it as a starting commit.
<i>1546</i>&nbsp;            // To avoid this issue, we only select index commits whose translog are fully retained.
<b class="fc"><i>1547</i>&nbsp;            if (indexVersionCreated.before(org.elasticsearch.Version.V_6_2_0)) {</b>
<b class="nc"><i>1548</i>&nbsp;                final List&lt;IndexCommit&gt; recoverableCommits = new ArrayList&lt;&gt;();</b>
<b class="nc"><i>1549</i>&nbsp;                for (IndexCommit commit : existingCommits) {</b>
<b class="nc"><i>1550</i>&nbsp;                    if (minRetainedTranslogGen &lt;= Long.parseLong(commit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY))) {</b>
<b class="nc"><i>1551</i>&nbsp;                        recoverableCommits.add(commit);</b>
<i>1552</i>&nbsp;                    }
<b class="nc"><i>1553</i>&nbsp;                }</b>
<b class="nc"><i>1554</i>&nbsp;                assert recoverableCommits.isEmpty() == false : &quot;No commit point with translog found; &quot; +</b>
<i>1555</i>&nbsp;                    &quot;commits [&quot; + existingCommits + &quot;], minRetainedTranslogGen [&quot; + minRetainedTranslogGen + &quot;]&quot;;
<b class="nc"><i>1556</i>&nbsp;                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(recoverableCommits, lastSyncedGlobalCheckpoint);</b>
<b class="nc"><i>1557</i>&nbsp;            } else {</b>
<i>1558</i>&nbsp;                // TODO: Asserts the starting commit is a safe commit once peer-recovery sets global checkpoint.
<b class="fc"><i>1559</i>&nbsp;                startingIndexCommit = CombinedDeletionPolicy.findSafeCommitPoint(existingCommits, lastSyncedGlobalCheckpoint);</b>
<i>1560</i>&nbsp;            }
<i>1561</i>&nbsp;
<b class="fc"><i>1562</i>&nbsp;            if (translogUUID.equals(startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY)) == false) {</b>
<b class="nc"><i>1563</i>&nbsp;                throw new IllegalStateException(&quot;starting commit translog uuid [&quot;</b>
<b class="nc"><i>1564</i>&nbsp;                    + startingIndexCommit.getUserData().get(Translog.TRANSLOG_UUID_KEY) + &quot;] is not equal to last commit&#39;s translog uuid [&quot;</b>
<i>1565</i>&nbsp;                    + translogUUID + &quot;]&quot;);
<i>1566</i>&nbsp;            }
<b class="fc"><i>1567</i>&nbsp;            if (startingIndexCommit.equals(lastIndexCommitCommit) == false) {</b>
<b class="nc"><i>1568</i>&nbsp;                try (IndexWriter writer = newAppendingIndexWriter(directory, startingIndexCommit)) {</b>
<i>1569</i>&nbsp;                    // this achieves two things:
<i>1570</i>&nbsp;                    // - by committing a new commit based on the starting commit, it make sure the starting commit will be opened
<i>1571</i>&nbsp;                    // - deletes any other commit (by lucene standard deletion policy)
<i>1572</i>&nbsp;                    //
<i>1573</i>&nbsp;                    // note that we can&#39;t just use IndexCommit.delete() as we really want to make sure that those files won&#39;t be used
<i>1574</i>&nbsp;                    // even if a virus scanner causes the files not to be used.
<i>1575</i>&nbsp;
<i>1576</i>&nbsp;                    // The new commit will use segment files from the starting commit but userData from the last commit by default.
<i>1577</i>&nbsp;                    // Thus, we need to manually set the userData from the starting commit to the new commit.
<b class="nc"><i>1578</i>&nbsp;                    writer.setLiveCommitData(startingIndexCommit.getUserData().entrySet());</b>
<b class="nc"><i>1579</i>&nbsp;                    writer.commit();</b>
<b class="nc"><i>1580</i>&nbsp;                }</b>
<i>1581</i>&nbsp;            }
<i>1582</i>&nbsp;        } finally {
<b class="fc"><i>1583</i>&nbsp;            metadataLock.writeLock().unlock();</b>
<b class="fc"><i>1584</i>&nbsp;        }</b>
<b class="fc"><i>1585</i>&nbsp;    }</b>
<i>1586</i>&nbsp;
<i>1587</i>&nbsp;    /**
<i>1588</i>&nbsp;     * Returns a {@link org.elasticsearch.index.seqno.SequenceNumbers.CommitInfo} of the safe commit if exists.
<i>1589</i>&nbsp;     */
<i>1590</i>&nbsp;    public Optional&lt;SequenceNumbers.CommitInfo&gt; findSafeIndexCommit(long globalCheckpoint) throws IOException {
<b class="nc"><i>1591</i>&nbsp;        final List&lt;IndexCommit&gt; commits = DirectoryReader.listCommits(directory);</b>
<b class="nc"><i>1592</i>&nbsp;        assert commits.isEmpty() == false : &quot;no commit found&quot;;</b>
<b class="nc"><i>1593</i>&nbsp;        final IndexCommit safeCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint);</b>
<b class="nc"><i>1594</i>&nbsp;        final SequenceNumbers.CommitInfo commitInfo = SequenceNumbers.loadSeqNoInfoFromLuceneCommit(safeCommit.getUserData().entrySet());</b>
<i>1595</i>&nbsp;        // all operations of the safe commit must be at most the global checkpoint.
<b class="nc"><i>1596</i>&nbsp;        if (commitInfo.maxSeqNo &lt;= globalCheckpoint) {</b>
<b class="nc"><i>1597</i>&nbsp;            return Optional.of(commitInfo);</b>
<i>1598</i>&nbsp;        } else {
<b class="nc"><i>1599</i>&nbsp;            return Optional.empty();</b>
<i>1600</i>&nbsp;        }
<i>1601</i>&nbsp;    }
<i>1602</i>&nbsp;
<i>1603</i>&nbsp;    private static void updateCommitData(IndexWriter writer, Map&lt;String, String&gt; keysToUpdate) throws IOException {
<b class="fc"><i>1604</i>&nbsp;        final Map&lt;String, String&gt; userData = getUserData(writer);</b>
<b class="fc"><i>1605</i>&nbsp;        userData.putAll(keysToUpdate);</b>
<b class="fc"><i>1606</i>&nbsp;        writer.setLiveCommitData(userData.entrySet());</b>
<b class="fc"><i>1607</i>&nbsp;        writer.commit();</b>
<b class="fc"><i>1608</i>&nbsp;    }</b>
<i>1609</i>&nbsp;
<i>1610</i>&nbsp;    private static Map&lt;String, String&gt; getUserData(IndexWriter writer) {
<b class="fc"><i>1611</i>&nbsp;        final Map&lt;String, String&gt; userData = new HashMap&lt;&gt;();</b>
<b class="fc"><i>1612</i>&nbsp;        writer.getLiveCommitData().forEach(e -&gt; userData.put(e.getKey(), e.getValue()));</b>
<b class="fc"><i>1613</i>&nbsp;        return userData;</b>
<i>1614</i>&nbsp;    }
<i>1615</i>&nbsp;
<i>1616</i>&nbsp;    private static IndexWriter newAppendingIndexWriter(final Directory dir, final IndexCommit commit) throws IOException {
<b class="fc"><i>1617</i>&nbsp;        IndexWriterConfig iwc = newIndexWriterConfig()</b>
<b class="fc"><i>1618</i>&nbsp;            .setIndexCommit(commit)</b>
<b class="fc"><i>1619</i>&nbsp;            .setOpenMode(IndexWriterConfig.OpenMode.APPEND);</b>
<b class="fc"><i>1620</i>&nbsp;        return new IndexWriter(dir, iwc);</b>
<i>1621</i>&nbsp;    }
<i>1622</i>&nbsp;
<i>1623</i>&nbsp;    private static IndexWriter newEmptyIndexWriter(final Directory dir, final Version luceneVersion) throws IOException {
<b class="fc"><i>1624</i>&nbsp;        IndexWriterConfig iwc = newIndexWriterConfig()</b>
<b class="fc"><i>1625</i>&nbsp;            .setOpenMode(IndexWriterConfig.OpenMode.CREATE)</b>
<b class="fc"><i>1626</i>&nbsp;            .setIndexCreatedVersionMajor(luceneVersion.major);</b>
<b class="fc"><i>1627</i>&nbsp;        return new IndexWriter(dir, iwc);</b>
<i>1628</i>&nbsp;    }
<i>1629</i>&nbsp;
<i>1630</i>&nbsp;    private static IndexWriterConfig newIndexWriterConfig() {
<b class="fc"><i>1631</i>&nbsp;        return new IndexWriterConfig(null)</b>
<b class="fc"><i>1632</i>&nbsp;                .setSoftDeletesField(Lucene.SOFT_DELETES_FIELD)</b>
<b class="fc"><i>1633</i>&nbsp;                .setCommitOnClose(false)</b>
<i>1634</i>&nbsp;                // we don&#39;t want merges to happen here - we call maybe merge on the engine
<i>1635</i>&nbsp;                // later once we stared it up otherwise we would need to wait for it here
<i>1636</i>&nbsp;                // we also don&#39;t specify a codec here and merges should use the engines for this index
<b class="fc"><i>1637</i>&nbsp;                .setMergePolicy(NoMergePolicy.INSTANCE);</b>
<i>1638</i>&nbsp;    }
<i>1639</i>&nbsp;}
</div>
</div>

<div class="footer">
    
    <div style="float:right;">generated on 2020-02-09 18:45</div>
</div>
</body>
</html>
