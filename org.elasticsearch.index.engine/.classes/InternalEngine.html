


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html id="htmlId">
<head>
  <title>Coverage Report :: InternalEngine</title>
  <style type="text/css">
    @import "../../.css/coverage.css";
  </style>
</head>

<body>
<div class="header"></div>

<div class="content">
<div class="breadCrumbs">
    [ <a href="../../index.html">all classes</a> ]
    [ <a href="../index.html">org.elasticsearch.index.engine</a> ]
</div>

<h1>Coverage Summary for Class: InternalEngine (org.elasticsearch.index.engine)</h1>

<table class="coverageStats">

<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">InternalEngine</td>
<td class="coverageStat">
  <span class="percent">
    29.2%
  </span>
  <span class="absValue">
    (40/ 137)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    21.2%
  </span>
  <span class="absValue">
    (264/ 1247)
  </span>
</td>
</tr>
  <tr>
    <td class="name">InternalEngine$1</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (1/ 1)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (2/ 2)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">InternalEngine$AssertingIndexWriter</td>
<td class="coverageStat">
  <span class="percent">
    25%
  </span>
  <span class="absValue">
    (2/ 8)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    25%
  </span>
  <span class="absValue">
    (4/ 16)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">InternalEngine$EngineMergeScheduler</td>
<td class="coverageStat">
  <span class="percent">
    25%
  </span>
  <span class="absValue">
    (1/ 4)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    20%
  </span>
  <span class="absValue">
    (5/ 25)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">InternalEngine$ExternalReaderManager</td>
<td class="coverageStat">
  <span class="percent">
    83.3%
  </span>
  <span class="absValue">
    (5/ 6)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    83.3%
  </span>
  <span class="absValue">
    (20/ 24)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">InternalEngine$LastRefreshedCheckpointListener</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (5/ 5)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    91.7%
  </span>
  <span class="absValue">
    (11/ 12)
  </span>
</td>
  </tr>
  <tr>
    <td class="name">InternalEngine$RefreshWarmerListener</td>
<td class="coverageStat">
  <span class="percent">
    100%
  </span>
  <span class="absValue">
    (2/ 2)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    75%
  </span>
  <span class="absValue">
    (9/ 12)
  </span>
</td>
  </tr>
<tr>
  <td class="name"><strong>total</strong></td>
<td class="coverageStat">
  <span class="percent">
    34.4%
  </span>
  <span class="absValue">
    (56/ 163)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    23.5%
  </span>
  <span class="absValue">
    (315/ 1338)
  </span>
</td>
</tr>
</table>

<br/>
<br/>


<div class="sourceCode"><i>1</i>&nbsp;/*
<i>2</i>&nbsp; * Licensed to Elasticsearch under one or more contributor
<i>3</i>&nbsp; * license agreements. See the NOTICE file distributed with
<i>4</i>&nbsp; * this work for additional information regarding copyright
<i>5</i>&nbsp; * ownership. Elasticsearch licenses this file to you under
<i>6</i>&nbsp; * the Apache License, Version 2.0 (the &quot;License&quot;); you may
<i>7</i>&nbsp; * not use this file except in compliance with the License.
<i>8</i>&nbsp; * You may obtain a copy of the License at
<i>9</i>&nbsp; *
<i>10</i>&nbsp; *    http://www.apache.org/licenses/LICENSE-2.0
<i>11</i>&nbsp; *
<i>12</i>&nbsp; * Unless required by applicable law or agreed to in writing,
<i>13</i>&nbsp; * software distributed under the License is distributed on an
<i>14</i>&nbsp; * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
<i>15</i>&nbsp; * KIND, either express or implied.  See the License for the
<i>16</i>&nbsp; * specific language governing permissions and limitations
<i>17</i>&nbsp; * under the License.
<i>18</i>&nbsp; */
<i>19</i>&nbsp;
<i>20</i>&nbsp;package org.elasticsearch.index.engine;
<i>21</i>&nbsp;
<i>22</i>&nbsp;import org.apache.logging.log4j.Logger;
<i>23</i>&nbsp;import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader;
<i>24</i>&nbsp;import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader.FSTLoadMode;
<i>25</i>&nbsp;import org.apache.lucene.document.Field;
<i>26</i>&nbsp;import org.apache.lucene.document.LongPoint;
<i>27</i>&nbsp;import org.apache.lucene.document.NumericDocValuesField;
<i>28</i>&nbsp;import org.apache.lucene.index.DirectoryReader;
<i>29</i>&nbsp;import org.apache.lucene.index.IndexCommit;
<i>30</i>&nbsp;import org.apache.lucene.index.IndexReader;
<i>31</i>&nbsp;import org.apache.lucene.index.IndexWriter;
<i>32</i>&nbsp;import org.apache.lucene.index.IndexWriterConfig;
<i>33</i>&nbsp;import org.apache.lucene.index.IndexableField;
<i>34</i>&nbsp;import org.apache.lucene.index.LeafReaderContext;
<i>35</i>&nbsp;import org.apache.lucene.index.LiveIndexWriterConfig;
<i>36</i>&nbsp;import org.apache.lucene.index.MergePolicy;
<i>37</i>&nbsp;import org.apache.lucene.index.SegmentCommitInfo;
<i>38</i>&nbsp;import org.apache.lucene.index.SegmentInfos;
<i>39</i>&nbsp;import org.apache.lucene.index.SoftDeletesRetentionMergePolicy;
<i>40</i>&nbsp;import org.apache.lucene.index.Term;
<i>41</i>&nbsp;import org.apache.lucene.search.DocIdSetIterator;
<i>42</i>&nbsp;import org.apache.lucene.search.IndexSearcher;
<i>43</i>&nbsp;import org.apache.lucene.search.Query;
<i>44</i>&nbsp;import org.apache.lucene.search.ReferenceManager;
<i>45</i>&nbsp;import org.apache.lucene.search.ScoreMode;
<i>46</i>&nbsp;import org.apache.lucene.search.Scorer;
<i>47</i>&nbsp;import org.apache.lucene.search.TermQuery;
<i>48</i>&nbsp;import org.apache.lucene.search.Weight;
<i>49</i>&nbsp;import org.apache.lucene.store.AlreadyClosedException;
<i>50</i>&nbsp;import org.apache.lucene.store.Directory;
<i>51</i>&nbsp;import org.apache.lucene.store.FilterDirectory;
<i>52</i>&nbsp;import org.apache.lucene.store.LockObtainFailedException;
<i>53</i>&nbsp;import org.apache.lucene.store.MMapDirectory;
<i>54</i>&nbsp;import org.apache.lucene.util.BytesRef;
<i>55</i>&nbsp;import org.apache.lucene.util.InfoStream;
<i>56</i>&nbsp;import org.elasticsearch.Assertions;
<i>57</i>&nbsp;import org.elasticsearch.ExceptionsHelper;
<i>58</i>&nbsp;import org.elasticsearch.action.index.IndexRequest;
<i>59</i>&nbsp;import org.elasticsearch.common.Nullable;
<i>60</i>&nbsp;import org.elasticsearch.common.SuppressForbidden;
<i>61</i>&nbsp;import org.elasticsearch.common.lease.Releasable;
<i>62</i>&nbsp;import org.elasticsearch.common.lucene.LoggerInfoStream;
<i>63</i>&nbsp;import org.elasticsearch.common.lucene.Lucene;
<i>64</i>&nbsp;import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
<i>65</i>&nbsp;import org.elasticsearch.common.lucene.uid.Versions;
<i>66</i>&nbsp;import org.elasticsearch.common.lucene.uid.VersionsAndSeqNoResolver;
<i>67</i>&nbsp;import org.elasticsearch.common.lucene.uid.VersionsAndSeqNoResolver.DocIdAndSeqNo;
<i>68</i>&nbsp;import org.elasticsearch.common.metrics.CounterMetric;
<i>69</i>&nbsp;import org.elasticsearch.common.unit.ByteSizeValue;
<i>70</i>&nbsp;import org.elasticsearch.common.unit.TimeValue;
<i>71</i>&nbsp;import org.elasticsearch.common.util.concurrent.AbstractRunnable;
<i>72</i>&nbsp;import org.elasticsearch.common.util.concurrent.KeyedLock;
<i>73</i>&nbsp;import org.elasticsearch.common.util.concurrent.ReleasableLock;
<i>74</i>&nbsp;import org.elasticsearch.core.internal.io.IOUtils;
<i>75</i>&nbsp;import org.elasticsearch.index.IndexSettings;
<i>76</i>&nbsp;import org.elasticsearch.index.VersionType;
<i>77</i>&nbsp;import org.elasticsearch.index.fieldvisitor.IdOnlyFieldVisitor;
<i>78</i>&nbsp;import org.elasticsearch.index.mapper.IdFieldMapper;
<i>79</i>&nbsp;import org.elasticsearch.index.mapper.MapperService;
<i>80</i>&nbsp;import org.elasticsearch.index.mapper.ParseContext;
<i>81</i>&nbsp;import org.elasticsearch.index.mapper.ParsedDocument;
<i>82</i>&nbsp;import org.elasticsearch.index.mapper.SeqNoFieldMapper;
<i>83</i>&nbsp;import org.elasticsearch.index.mapper.SourceFieldMapper;
<i>84</i>&nbsp;import org.elasticsearch.index.mapper.Uid;
<i>85</i>&nbsp;import org.elasticsearch.index.merge.MergeStats;
<i>86</i>&nbsp;import org.elasticsearch.index.merge.OnGoingMerge;
<i>87</i>&nbsp;import org.elasticsearch.index.seqno.LocalCheckpointTracker;
<i>88</i>&nbsp;import org.elasticsearch.index.seqno.SeqNoStats;
<i>89</i>&nbsp;import org.elasticsearch.index.seqno.SequenceNumbers;
<i>90</i>&nbsp;import org.elasticsearch.index.shard.ElasticsearchMergePolicy;
<i>91</i>&nbsp;import org.elasticsearch.index.shard.ShardId;
<i>92</i>&nbsp;import org.elasticsearch.index.store.FsDirectoryFactory;
<i>93</i>&nbsp;import org.elasticsearch.index.store.Store;
<i>94</i>&nbsp;import org.elasticsearch.index.translog.Translog;
<i>95</i>&nbsp;import org.elasticsearch.index.translog.TranslogConfig;
<i>96</i>&nbsp;import org.elasticsearch.index.translog.TranslogCorruptedException;
<i>97</i>&nbsp;import org.elasticsearch.index.translog.TranslogDeletionPolicy;
<i>98</i>&nbsp;import org.elasticsearch.index.translog.TranslogStats;
<i>99</i>&nbsp;import org.elasticsearch.threadpool.ThreadPool;
<i>100</i>&nbsp;
<i>101</i>&nbsp;import java.io.Closeable;
<i>102</i>&nbsp;import java.io.IOException;
<i>103</i>&nbsp;import java.nio.file.Path;
<i>104</i>&nbsp;import java.util.Arrays;
<i>105</i>&nbsp;import java.util.Collections;
<i>106</i>&nbsp;import java.util.HashMap;
<i>107</i>&nbsp;import java.util.List;
<i>108</i>&nbsp;import java.util.Locale;
<i>109</i>&nbsp;import java.util.Map;
<i>110</i>&nbsp;import java.util.Objects;
<i>111</i>&nbsp;import java.util.Optional;
<i>112</i>&nbsp;import java.util.Set;
<i>113</i>&nbsp;import java.util.concurrent.CountDownLatch;
<i>114</i>&nbsp;import java.util.concurrent.atomic.AtomicBoolean;
<i>115</i>&nbsp;import java.util.concurrent.atomic.AtomicInteger;
<i>116</i>&nbsp;import java.util.concurrent.atomic.AtomicLong;
<i>117</i>&nbsp;import java.util.concurrent.locks.Lock;
<i>118</i>&nbsp;import java.util.concurrent.locks.ReentrantLock;
<i>119</i>&nbsp;import java.util.function.BiConsumer;
<i>120</i>&nbsp;import java.util.function.BiFunction;
<i>121</i>&nbsp;import java.util.function.LongConsumer;
<i>122</i>&nbsp;import java.util.function.LongSupplier;
<i>123</i>&nbsp;import java.util.stream.Collectors;
<i>124</i>&nbsp;import java.util.stream.Stream;
<i>125</i>&nbsp;
<b class="fc"><i>126</i>&nbsp;public class InternalEngine extends Engine {</b>
<i>127</i>&nbsp;
<i>128</i>&nbsp;    /**
<i>129</i>&nbsp;     * When we last pruned expired tombstones from versionMap.deletes:
<i>130</i>&nbsp;     */
<i>131</i>&nbsp;    private volatile long lastDeleteVersionPruneTimeMSec;
<i>132</i>&nbsp;
<i>133</i>&nbsp;    private final Translog translog;
<i>134</i>&nbsp;    private final ElasticsearchConcurrentMergeScheduler mergeScheduler;
<i>135</i>&nbsp;
<i>136</i>&nbsp;    private final IndexWriter indexWriter;
<i>137</i>&nbsp;
<i>138</i>&nbsp;    private final ExternalReaderManager externalReaderManager;
<i>139</i>&nbsp;    private final ElasticsearchReaderManager internalReaderManager;
<i>140</i>&nbsp;
<b class="fc"><i>141</i>&nbsp;    private final Lock flushLock = new ReentrantLock();</b>
<b class="fc"><i>142</i>&nbsp;    private final ReentrantLock optimizeLock = new ReentrantLock();</b>
<i>143</i>&nbsp;
<i>144</i>&nbsp;    // A uid (in the form of BytesRef) to the version map
<i>145</i>&nbsp;    // we use the hashed variant since we iterate over it and check removal and additions on existing keys
<b class="fc"><i>146</i>&nbsp;    private final LiveVersionMap versionMap = new LiveVersionMap();</b>
<i>147</i>&nbsp;
<i>148</i>&nbsp;    private volatile SegmentInfos lastCommittedSegmentInfos;
<i>149</i>&nbsp;
<i>150</i>&nbsp;    private final IndexThrottle throttle;
<i>151</i>&nbsp;
<i>152</i>&nbsp;    private final LocalCheckpointTracker localCheckpointTracker;
<i>153</i>&nbsp;
<i>154</i>&nbsp;    private final CombinedDeletionPolicy combinedDeletionPolicy;
<i>155</i>&nbsp;
<i>156</i>&nbsp;    // How many callers are currently requesting index throttling.  Currently there are only two situations where we do this: when merges
<i>157</i>&nbsp;    // are falling behind and when writing indexing buffer to disk is too slow.  When this is 0, there is no throttling, else we throttling
<i>158</i>&nbsp;    // incoming indexing ops to a single thread:
<b class="fc"><i>159</i>&nbsp;    private final AtomicInteger throttleRequestCount = new AtomicInteger();</b>
<b class="fc"><i>160</i>&nbsp;    private final AtomicBoolean pendingTranslogRecovery = new AtomicBoolean(false);</b>
<b class="fc"><i>161</i>&nbsp;    private final AtomicLong maxUnsafeAutoIdTimestamp = new AtomicLong(-1);</b>
<b class="fc"><i>162</i>&nbsp;    private final AtomicLong maxSeenAutoIdTimestamp = new AtomicLong(-1);</b>
<i>163</i>&nbsp;    // max_seq_no_of_updates_or_deletes tracks the max seq_no of update or delete operations that have been processed in this engine.
<i>164</i>&nbsp;    // An index request is considered as an update if it overwrites existing documents with the same docId in the Lucene index.
<i>165</i>&nbsp;    // The value of this marker never goes backwards, and is tracked/updated differently on primary and replica.
<i>166</i>&nbsp;    private final AtomicLong maxSeqNoOfUpdatesOrDeletes;
<b class="fc"><i>167</i>&nbsp;    private final CounterMetric numVersionLookups = new CounterMetric();</b>
<b class="fc"><i>168</i>&nbsp;    private final CounterMetric numIndexVersionsLookups = new CounterMetric();</b>
<i>169</i>&nbsp;    // Lucene operations since this engine was opened - not include operations from existing segments.
<b class="fc"><i>170</i>&nbsp;    private final CounterMetric numDocDeletes = new CounterMetric();</b>
<b class="fc"><i>171</i>&nbsp;    private final CounterMetric numDocAppends = new CounterMetric();</b>
<b class="fc"><i>172</i>&nbsp;    private final CounterMetric numDocUpdates = new CounterMetric();</b>
<b class="fc"><i>173</i>&nbsp;    private final NumericDocValuesField softDeletesField = Lucene.newSoftDeletesField();</b>
<i>174</i>&nbsp;    private final boolean softDeleteEnabled;
<i>175</i>&nbsp;    private final SoftDeletesPolicy softDeletesPolicy;
<i>176</i>&nbsp;    private final LastRefreshedCheckpointListener lastRefreshedCheckpointListener;
<i>177</i>&nbsp;
<b class="fc"><i>178</i>&nbsp;    private final AtomicBoolean trackTranslogLocation = new AtomicBoolean(false);</b>
<b class="fc"><i>179</i>&nbsp;    private final KeyedLock&lt;Long&gt; noOpKeyedLock = new KeyedLock&lt;&gt;();</b>
<b class="fc"><i>180</i>&nbsp;    private final AtomicBoolean shouldPeriodicallyFlushAfterBigMerge = new AtomicBoolean(false);</b>
<i>181</i>&nbsp;
<i>182</i>&nbsp;    @Nullable
<i>183</i>&nbsp;    private final String historyUUID;
<i>184</i>&nbsp;
<i>185</i>&nbsp;    public InternalEngine(EngineConfig engineConfig) {
<b class="fc"><i>186</i>&nbsp;        this(engineConfig, LocalCheckpointTracker::new);</b>
<b class="fc"><i>187</i>&nbsp;    }</b>
<i>188</i>&nbsp;
<i>189</i>&nbsp;    InternalEngine(
<i>190</i>&nbsp;            final EngineConfig engineConfig,
<i>191</i>&nbsp;            final BiFunction&lt;Long, Long, LocalCheckpointTracker&gt; localCheckpointTrackerSupplier) {
<b class="fc"><i>192</i>&nbsp;        super(engineConfig);</b>
<b class="fc"><i>193</i>&nbsp;        if (engineConfig.isAutoGeneratedIDsOptimizationEnabled() == false) {</b>
<b class="nc"><i>194</i>&nbsp;            updateAutoIdTimestamp(Long.MAX_VALUE, true);</b>
<i>195</i>&nbsp;        }
<b class="fc"><i>196</i>&nbsp;        final TranslogDeletionPolicy translogDeletionPolicy = new TranslogDeletionPolicy(</b>
<b class="fc"><i>197</i>&nbsp;                engineConfig.getIndexSettings().getTranslogRetentionSize().getBytes(),</b>
<b class="fc"><i>198</i>&nbsp;                engineConfig.getIndexSettings().getTranslogRetentionAge().getMillis(),</b>
<b class="fc"><i>199</i>&nbsp;                engineConfig.getIndexSettings().getTranslogRetentionTotalFiles()</b>
<i>200</i>&nbsp;        );
<b class="fc"><i>201</i>&nbsp;        store.incRef();</b>
<b class="fc"><i>202</i>&nbsp;        IndexWriter writer = null;</b>
<b class="fc"><i>203</i>&nbsp;        Translog translog = null;</b>
<b class="fc"><i>204</i>&nbsp;        ExternalReaderManager externalReaderManager = null;</b>
<b class="fc"><i>205</i>&nbsp;        ElasticsearchReaderManager internalReaderManager = null;</b>
<b class="fc"><i>206</i>&nbsp;        EngineMergeScheduler scheduler = null;</b>
<b class="fc"><i>207</i>&nbsp;        boolean success = false;</b>
<i>208</i>&nbsp;        try {
<b class="fc"><i>209</i>&nbsp;            this.lastDeleteVersionPruneTimeMSec = engineConfig.getThreadPool().relativeTimeInMillis();</b>
<b class="fc"><i>210</i>&nbsp;            mergeScheduler = scheduler = new EngineMergeScheduler(engineConfig.getShardId(), engineConfig.getIndexSettings());</b>
<b class="fc"><i>211</i>&nbsp;            throttle = new IndexThrottle();</b>
<i>212</i>&nbsp;            try {
<b class="fc"><i>213</i>&nbsp;                trimUnsafeCommits(engineConfig);</b>
<b class="fc"><i>214</i>&nbsp;                translog = openTranslog(engineConfig, translogDeletionPolicy, engineConfig.getGlobalCheckpointSupplier(),</b>
<i>215</i>&nbsp;                    seqNo -&gt; {
<b class="nc"><i>216</i>&nbsp;                        final LocalCheckpointTracker tracker = getLocalCheckpointTracker();</b>
<b class="nc"><i>217</i>&nbsp;                        assert tracker != null || getTranslog().isOpen() == false;</b>
<b class="nc"><i>218</i>&nbsp;                        if (tracker != null) {</b>
<b class="nc"><i>219</i>&nbsp;                            tracker.markSeqNoAsPersisted(seqNo);</b>
<i>220</i>&nbsp;                        }
<b class="nc"><i>221</i>&nbsp;                    });</b>
<b class="fc"><i>222</i>&nbsp;                assert translog.getGeneration() != null;</b>
<b class="fc"><i>223</i>&nbsp;                this.translog = translog;</b>
<b class="fc"><i>224</i>&nbsp;                this.softDeleteEnabled = engineConfig.getIndexSettings().isSoftDeleteEnabled();</b>
<b class="fc"><i>225</i>&nbsp;                this.softDeletesPolicy = newSoftDeletesPolicy();</b>
<b class="fc"><i>226</i>&nbsp;                this.combinedDeletionPolicy =</b>
<b class="fc"><i>227</i>&nbsp;                    new CombinedDeletionPolicy(logger, translogDeletionPolicy, softDeletesPolicy, translog::getLastSyncedGlobalCheckpoint);</b>
<b class="fc"><i>228</i>&nbsp;                this.localCheckpointTracker = createLocalCheckpointTracker(localCheckpointTrackerSupplier);</b>
<b class="fc"><i>229</i>&nbsp;                writer = createWriter();</b>
<b class="fc"><i>230</i>&nbsp;                bootstrapAppendOnlyInfoFromWriter(writer);</b>
<b class="fc"><i>231</i>&nbsp;                historyUUID = loadHistoryUUID(writer);</b>
<b class="fc"><i>232</i>&nbsp;                indexWriter = writer;</b>
<b class="nc"><i>233</i>&nbsp;            } catch (IOException | TranslogCorruptedException e) {</b>
<b class="nc"><i>234</i>&nbsp;                throw new EngineCreationFailureException(shardId, &quot;failed to create engine&quot;, e);</b>
<b class="nc"><i>235</i>&nbsp;            } catch (AssertionError e) {</b>
<i>236</i>&nbsp;                // IndexWriter throws AssertionError on init, if asserts are enabled, if any files don&#39;t exist, but tests that
<i>237</i>&nbsp;                // randomly throw FNFE/NSFE can also hit this:
<b class="nc"><i>238</i>&nbsp;                if (ExceptionsHelper.stackTrace(e).contains(&quot;org.apache.lucene.index.IndexWriter.filesExist&quot;)) {</b>
<b class="nc"><i>239</i>&nbsp;                    throw new EngineCreationFailureException(shardId, &quot;failed to create engine&quot;, e);</b>
<i>240</i>&nbsp;                } else {
<b class="nc"><i>241</i>&nbsp;                    throw e;</b>
<i>242</i>&nbsp;                }
<b class="fc"><i>243</i>&nbsp;            }</b>
<b class="fc"><i>244</i>&nbsp;            externalReaderManager = createReaderManager(new RefreshWarmerListener(logger, isClosed, engineConfig));</b>
<b class="fc"><i>245</i>&nbsp;            internalReaderManager = externalReaderManager.internalReaderManager;</b>
<b class="fc"><i>246</i>&nbsp;            this.internalReaderManager = internalReaderManager;</b>
<b class="fc"><i>247</i>&nbsp;            this.externalReaderManager = externalReaderManager;</b>
<b class="fc"><i>248</i>&nbsp;            internalReaderManager.addListener(versionMap);</b>
<b class="fc"><i>249</i>&nbsp;            assert pendingTranslogRecovery.get() == false : &quot;translog recovery can&#39;t be pending before we set it&quot;;</b>
<i>250</i>&nbsp;            // don&#39;t allow commits until we are done with recovering
<b class="fc"><i>251</i>&nbsp;            pendingTranslogRecovery.set(true);</b>
<b class="fc"><i>252</i>&nbsp;            for (ReferenceManager.RefreshListener listener: engineConfig.getExternalRefreshListener()) {</b>
<b class="fc"><i>253</i>&nbsp;                this.externalReaderManager.addListener(listener);</b>
<b class="fc"><i>254</i>&nbsp;            }</b>
<b class="fc"><i>255</i>&nbsp;            for (ReferenceManager.RefreshListener listener: engineConfig.getInternalRefreshListener()) {</b>
<b class="fc"><i>256</i>&nbsp;                this.internalReaderManager.addListener(listener);</b>
<b class="fc"><i>257</i>&nbsp;            }</b>
<b class="fc"><i>258</i>&nbsp;            this.lastRefreshedCheckpointListener = new LastRefreshedCheckpointListener(localCheckpointTracker.getProcessedCheckpoint());</b>
<b class="fc"><i>259</i>&nbsp;            this.internalReaderManager.addListener(lastRefreshedCheckpointListener);</b>
<b class="fc"><i>260</i>&nbsp;            maxSeqNoOfUpdatesOrDeletes = new AtomicLong(SequenceNumbers.max(localCheckpointTracker.getMaxSeqNo(), translog.getMaxSeqNo()));</b>
<b class="fc"><i>261</i>&nbsp;            if (softDeleteEnabled &amp;&amp; localCheckpointTracker.getPersistedCheckpoint() &lt; localCheckpointTracker.getMaxSeqNo()) {</b>
<b class="nc"><i>262</i>&nbsp;                try (Searcher searcher =</b>
<b class="nc"><i>263</i>&nbsp;                         acquireSearcher(&quot;restore_version_map_and_checkpoint_tracker&quot;, SearcherScope.INTERNAL)) {</b>
<b class="nc"><i>264</i>&nbsp;                    restoreVersionMapAndCheckpointTracker(Lucene.wrapAllDocsLive(searcher.getDirectoryReader()));</b>
<b class="nc"><i>265</i>&nbsp;                } catch (IOException e) {</b>
<b class="nc"><i>266</i>&nbsp;                    throw new EngineCreationFailureException(config().getShardId(),</b>
<i>267</i>&nbsp;                        &quot;failed to restore version map and local checkpoint tracker&quot;, e);
<b class="nc"><i>268</i>&nbsp;                }</b>
<i>269</i>&nbsp;            }
<b class="fc"><i>270</i>&nbsp;            success = true;</b>
<i>271</i>&nbsp;        } finally {
<b class="fc"><i>272</i>&nbsp;            if (success == false) {</b>
<b class="nc"><i>273</i>&nbsp;                IOUtils.closeWhileHandlingException(writer, translog, internalReaderManager, externalReaderManager, scheduler);</b>
<b class="nc"><i>274</i>&nbsp;                if (isClosed.get() == false) {</b>
<i>275</i>&nbsp;                    // failure we need to dec the store reference
<b class="nc"><i>276</i>&nbsp;                    store.decRef();</b>
<i>277</i>&nbsp;                }
<i>278</i>&nbsp;            }
<b class="nc"><i>279</i>&nbsp;        }</b>
<b class="fc"><i>280</i>&nbsp;        logger.trace(&quot;created new InternalEngine&quot;);</b>
<b class="fc"><i>281</i>&nbsp;    }</b>
<i>282</i>&nbsp;
<i>283</i>&nbsp;    private LocalCheckpointTracker createLocalCheckpointTracker(
<i>284</i>&nbsp;        BiFunction&lt;Long, Long, LocalCheckpointTracker&gt; localCheckpointTrackerSupplier) throws IOException {
<i>285</i>&nbsp;        final long maxSeqNo;
<i>286</i>&nbsp;        final long localCheckpoint;
<b class="fc"><i>287</i>&nbsp;        final SequenceNumbers.CommitInfo seqNoStats =</b>
<b class="fc"><i>288</i>&nbsp;            SequenceNumbers.loadSeqNoInfoFromLuceneCommit(store.readLastCommittedSegmentsInfo().userData.entrySet());</b>
<b class="fc"><i>289</i>&nbsp;        maxSeqNo = seqNoStats.maxSeqNo;</b>
<b class="fc"><i>290</i>&nbsp;        localCheckpoint = seqNoStats.localCheckpoint;</b>
<b class="fc"><i>291</i>&nbsp;        logger.trace(&quot;recovered maximum sequence number [{}] and local checkpoint [{}]&quot;, maxSeqNo, localCheckpoint);</b>
<b class="fc"><i>292</i>&nbsp;        return localCheckpointTrackerSupplier.apply(maxSeqNo, localCheckpoint);</b>
<i>293</i>&nbsp;    }
<i>294</i>&nbsp;
<i>295</i>&nbsp;    private SoftDeletesPolicy newSoftDeletesPolicy() throws IOException {
<b class="fc"><i>296</i>&nbsp;        final Map&lt;String, String&gt; commitUserData = store.readLastCommittedSegmentsInfo().userData;</b>
<i>297</i>&nbsp;        final long lastMinRetainedSeqNo;
<b class="fc"><i>298</i>&nbsp;        if (commitUserData.containsKey(Engine.MIN_RETAINED_SEQNO)) {</b>
<b class="nc"><i>299</i>&nbsp;            lastMinRetainedSeqNo = Long.parseLong(commitUserData.get(Engine.MIN_RETAINED_SEQNO));</b>
<i>300</i>&nbsp;        } else {
<b class="fc"><i>301</i>&nbsp;            lastMinRetainedSeqNo = Long.parseLong(commitUserData.get(SequenceNumbers.MAX_SEQ_NO)) + 1;</b>
<i>302</i>&nbsp;        }
<b class="fc"><i>303</i>&nbsp;        return new SoftDeletesPolicy(</b>
<b class="fc"><i>304</i>&nbsp;                translog::getLastSyncedGlobalCheckpoint,</b>
<i>305</i>&nbsp;                lastMinRetainedSeqNo,
<b class="fc"><i>306</i>&nbsp;                engineConfig.getIndexSettings().getSoftDeleteRetentionOperations(),</b>
<b class="fc"><i>307</i>&nbsp;                engineConfig.retentionLeasesSupplier());</b>
<i>308</i>&nbsp;    }
<i>309</i>&nbsp;
<i>310</i>&nbsp;    /**
<i>311</i>&nbsp;     * This reference manager delegates all it&#39;s refresh calls to another (internal) ReaderManager
<i>312</i>&nbsp;     * The main purpose for this is that if we have external refreshes happening we don&#39;t issue extra
<i>313</i>&nbsp;     * refreshes to clear version map memory etc. this can cause excessive segment creation if heavy indexing
<i>314</i>&nbsp;     * is happening and the refresh interval is low (ie. 1 sec)
<i>315</i>&nbsp;     *
<i>316</i>&nbsp;     * This also prevents segment starvation where an internal reader holds on to old segments literally forever
<i>317</i>&nbsp;     * since no indexing is happening and refreshes are only happening to the external reader manager, while with
<i>318</i>&nbsp;     * this specialized implementation an external refresh will immediately be reflected on the internal reader
<i>319</i>&nbsp;     * and old segments can be released in the same way previous version did this (as a side-effect of _refresh)
<i>320</i>&nbsp;     */
<i>321</i>&nbsp;    @SuppressForbidden(reason = &quot;reference counting is required here&quot;)
<b class="fc"><i>322</i>&nbsp;    private static final class ExternalReaderManager extends ReferenceManager&lt;ElasticsearchDirectoryReader&gt; {</b>
<i>323</i>&nbsp;        private final BiConsumer&lt;ElasticsearchDirectoryReader, ElasticsearchDirectoryReader&gt; refreshListener;
<i>324</i>&nbsp;        private final ElasticsearchReaderManager internalReaderManager;
<i>325</i>&nbsp;        private boolean isWarmedUp; //guarded by refreshLock
<i>326</i>&nbsp;
<i>327</i>&nbsp;        ExternalReaderManager(ElasticsearchReaderManager internalReaderManager,
<b class="fc"><i>328</i>&nbsp;                              BiConsumer&lt;ElasticsearchDirectoryReader, ElasticsearchDirectoryReader&gt; refreshListener) throws IOException {</b>
<b class="fc"><i>329</i>&nbsp;            this.refreshListener = refreshListener;</b>
<b class="fc"><i>330</i>&nbsp;            this.internalReaderManager = internalReaderManager;</b>
<b class="fc"><i>331</i>&nbsp;            this.current = internalReaderManager.acquire(); // steal the reference without warming up</b>
<b class="fc"><i>332</i>&nbsp;        }</b>
<i>333</i>&nbsp;
<i>334</i>&nbsp;        @Override
<i>335</i>&nbsp;        protected ElasticsearchDirectoryReader refreshIfNeeded(ElasticsearchDirectoryReader referenceToRefresh) throws IOException {
<i>336</i>&nbsp;            // we simply run a blocking refresh on the internal reference manager and then steal it&#39;s reader
<i>337</i>&nbsp;            // it&#39;s a save operation since we acquire the reader which incs it&#39;s reference but then down the road
<i>338</i>&nbsp;            // steal it by calling incRef on the &quot;stolen&quot; reader
<b class="fc"><i>339</i>&nbsp;            internalReaderManager.maybeRefreshBlocking();</b>
<b class="fc"><i>340</i>&nbsp;            final ElasticsearchDirectoryReader newReader = internalReaderManager.acquire();</b>
<b class="fc"><i>341</i>&nbsp;            if (isWarmedUp == false || newReader != referenceToRefresh) {</b>
<b class="fc"><i>342</i>&nbsp;                boolean success = false;</b>
<i>343</i>&nbsp;                try {
<b class="fc"><i>344</i>&nbsp;                    refreshListener.accept(newReader, isWarmedUp ? referenceToRefresh : null);</b>
<b class="fc"><i>345</i>&nbsp;                    isWarmedUp = true;</b>
<b class="fc"><i>346</i>&nbsp;                    success = true;</b>
<i>347</i>&nbsp;                } finally {
<b class="fc"><i>348</i>&nbsp;                    if (success == false) {</b>
<b class="nc"><i>349</i>&nbsp;                        internalReaderManager.release(newReader);</b>
<i>350</i>&nbsp;                    }
<b class="nc"><i>351</i>&nbsp;                }</b>
<i>352</i>&nbsp;            }
<i>353</i>&nbsp;            // nothing has changed - both ref managers share the same instance so we can use reference equality
<b class="fc"><i>354</i>&nbsp;            if (referenceToRefresh == newReader) {</b>
<b class="fc"><i>355</i>&nbsp;                internalReaderManager.release(newReader);</b>
<b class="fc"><i>356</i>&nbsp;                return null;</b>
<i>357</i>&nbsp;            } else {
<b class="nc"><i>358</i>&nbsp;                return newReader; // steal the reference</b>
<i>359</i>&nbsp;            }
<i>360</i>&nbsp;        }
<i>361</i>&nbsp;
<i>362</i>&nbsp;        @Override
<i>363</i>&nbsp;        protected boolean tryIncRef(ElasticsearchDirectoryReader reference) {
<b class="fc"><i>364</i>&nbsp;            return reference.tryIncRef();</b>
<i>365</i>&nbsp;        }
<i>366</i>&nbsp;
<i>367</i>&nbsp;        @Override
<i>368</i>&nbsp;        protected int getRefCount(ElasticsearchDirectoryReader reference) {
<b class="nc"><i>369</i>&nbsp;            return reference.getRefCount();</b>
<i>370</i>&nbsp;        }
<i>371</i>&nbsp;
<i>372</i>&nbsp;        @Override
<i>373</i>&nbsp;        protected void decRef(ElasticsearchDirectoryReader reference) throws IOException {
<b class="fc"><i>374</i>&nbsp;            reference.decRef();</b>
<b class="fc"><i>375</i>&nbsp;        }</b>
<i>376</i>&nbsp;    }
<i>377</i>&nbsp;
<i>378</i>&nbsp;    @Override
<i>379</i>&nbsp;    final boolean assertSearcherIsWarmedUp(String source, SearcherScope scope) {
<b class="fc"><i>380</i>&nbsp;        if (scope == SearcherScope.EXTERNAL) {</b>
<b class="fc"><i>381</i>&nbsp;            switch (source) {</b>
<i>382</i>&nbsp;                // we can access segment_stats while a shard is still in the recovering state.
<i>383</i>&nbsp;                case &quot;segments&quot;:
<i>384</i>&nbsp;                case &quot;segments_stats&quot;:
<b class="nc"><i>385</i>&nbsp;                    break;</b>
<i>386</i>&nbsp;                default:
<b class="fc"><i>387</i>&nbsp;                    assert externalReaderManager.isWarmedUp : &quot;searcher was not warmed up yet for source[&quot; + source + &quot;]&quot;;</b>
<i>388</i>&nbsp;            }
<i>389</i>&nbsp;        }
<b class="fc"><i>390</i>&nbsp;        return true;</b>
<i>391</i>&nbsp;    }
<i>392</i>&nbsp;
<i>393</i>&nbsp;    @Override
<i>394</i>&nbsp;    public int restoreLocalHistoryFromTranslog(TranslogRecoveryRunner translogRecoveryRunner) throws IOException {
<b class="nc"><i>395</i>&nbsp;        try (ReleasableLock ignored = readLock.acquire()) {</b>
<b class="nc"><i>396</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>397</i>&nbsp;            final long localCheckpoint = localCheckpointTracker.getProcessedCheckpoint();</b>
<b class="nc"><i>398</i>&nbsp;            try (Translog.Snapshot snapshot = getTranslog().newSnapshotFromMinSeqNo(localCheckpoint + 1)) {</b>
<b class="nc"><i>399</i>&nbsp;                return translogRecoveryRunner.run(this, snapshot);</b>
<b class="nc"><i>400</i>&nbsp;            }</b>
<b class="nc"><i>401</i>&nbsp;        }</b>
<i>402</i>&nbsp;    }
<i>403</i>&nbsp;
<i>404</i>&nbsp;    @Override
<i>405</i>&nbsp;    public int fillSeqNoGaps(long primaryTerm) throws IOException {
<b class="fc"><i>406</i>&nbsp;        try (ReleasableLock ignored = writeLock.acquire()) {</b>
<b class="fc"><i>407</i>&nbsp;            ensureOpen();</b>
<b class="fc"><i>408</i>&nbsp;            final long localCheckpoint = localCheckpointTracker.getProcessedCheckpoint();</b>
<b class="fc"><i>409</i>&nbsp;            final long maxSeqNo = localCheckpointTracker.getMaxSeqNo();</b>
<b class="fc"><i>410</i>&nbsp;            int numNoOpsAdded = 0;</b>
<i>411</i>&nbsp;            for (
<b class="fc"><i>412</i>&nbsp;                    long seqNo = localCheckpoint + 1;</b>
<b class="fc"><i>413</i>&nbsp;                    seqNo &lt;= maxSeqNo;</b>
<b class="nc"><i>414</i>&nbsp;                    seqNo = localCheckpointTracker.getProcessedCheckpoint() + 1 /* leap-frog the local checkpoint */) {</b>
<b class="nc"><i>415</i>&nbsp;                innerNoOp(new NoOp(seqNo, primaryTerm, Operation.Origin.PRIMARY, System.nanoTime(), &quot;filling gaps&quot;));</b>
<b class="nc"><i>416</i>&nbsp;                numNoOpsAdded++;</b>
<b class="nc"><i>417</i>&nbsp;                assert seqNo &lt;= localCheckpointTracker.getProcessedCheckpoint() :</b>
<b class="nc"><i>418</i>&nbsp;                    &quot;local checkpoint did not advance; was [&quot; + seqNo + &quot;], now [&quot; + localCheckpointTracker.getProcessedCheckpoint() + &quot;]&quot;;</b>
<i>419</i>&nbsp;
<i>420</i>&nbsp;            }
<b class="fc"><i>421</i>&nbsp;            syncTranslog(); // to persist noops associated with the advancement of the local checkpoint</b>
<b class="fc"><i>422</i>&nbsp;            assert localCheckpointTracker.getPersistedCheckpoint() == maxSeqNo</b>
<b class="nc"><i>423</i>&nbsp;                : &quot;persisted local checkpoint did not advance to max seq no; is [&quot; + localCheckpointTracker.getPersistedCheckpoint() +</b>
<i>424</i>&nbsp;                &quot;], max seq no [&quot; + maxSeqNo + &quot;]&quot;;
<b class="fc"><i>425</i>&nbsp;            return numNoOpsAdded;</b>
<b class="fc"><i>426</i>&nbsp;        }</b>
<i>427</i>&nbsp;    }
<i>428</i>&nbsp;
<i>429</i>&nbsp;    private void bootstrapAppendOnlyInfoFromWriter(IndexWriter writer) {
<b class="fc"><i>430</i>&nbsp;        for (Map.Entry&lt;String, String&gt; entry : writer.getLiveCommitData()) {</b>
<b class="fc"><i>431</i>&nbsp;            if (MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID.equals(entry.getKey())) {</b>
<b class="fc"><i>432</i>&nbsp;                assert maxUnsafeAutoIdTimestamp.get() == -1 :</b>
<b class="nc"><i>433</i>&nbsp;                    &quot;max unsafe timestamp was assigned already [&quot; + maxUnsafeAutoIdTimestamp.get() + &quot;]&quot;;</b>
<b class="fc"><i>434</i>&nbsp;                updateAutoIdTimestamp(Long.parseLong(entry.getValue()), true);</b>
<i>435</i>&nbsp;            }
<b class="fc"><i>436</i>&nbsp;        }</b>
<b class="fc"><i>437</i>&nbsp;    }</b>
<i>438</i>&nbsp;
<i>439</i>&nbsp;    @Override
<i>440</i>&nbsp;    public InternalEngine recoverFromTranslog(TranslogRecoveryRunner translogRecoveryRunner, long recoverUpToSeqNo) throws IOException {
<b class="fc"><i>441</i>&nbsp;        flushLock.lock();</b>
<b class="fc"><i>442</i>&nbsp;        try (ReleasableLock lock = readLock.acquire()) {</b>
<b class="fc"><i>443</i>&nbsp;            ensureOpen();</b>
<b class="fc"><i>444</i>&nbsp;            if (pendingTranslogRecovery.get() == false) {</b>
<b class="nc"><i>445</i>&nbsp;                throw new IllegalStateException(&quot;Engine has already been recovered&quot;);</b>
<i>446</i>&nbsp;            }
<i>447</i>&nbsp;            try {
<b class="fc"><i>448</i>&nbsp;                recoverFromTranslogInternal(translogRecoveryRunner, recoverUpToSeqNo);</b>
<b class="nc"><i>449</i>&nbsp;            } catch (Exception e) {</b>
<i>450</i>&nbsp;                try {
<b class="nc"><i>451</i>&nbsp;                    pendingTranslogRecovery.set(true); // just play safe and never allow commits on this see #ensureCanFlush</b>
<b class="nc"><i>452</i>&nbsp;                    failEngine(&quot;failed to recover from translog&quot;, e);</b>
<b class="nc"><i>453</i>&nbsp;                } catch (Exception inner) {</b>
<b class="nc"><i>454</i>&nbsp;                    e.addSuppressed(inner);</b>
<b class="nc"><i>455</i>&nbsp;                }</b>
<b class="nc"><i>456</i>&nbsp;                throw e;</b>
<b class="fc"><i>457</i>&nbsp;            }</b>
<b class="fc"><i>458</i>&nbsp;        } finally {</b>
<b class="fc"><i>459</i>&nbsp;            flushLock.unlock();</b>
<b class="fc"><i>460</i>&nbsp;        }</b>
<b class="fc"><i>461</i>&nbsp;        return this;</b>
<i>462</i>&nbsp;    }
<i>463</i>&nbsp;
<i>464</i>&nbsp;    @Override
<i>465</i>&nbsp;    public void skipTranslogRecovery() {
<b class="nc"><i>466</i>&nbsp;        assert pendingTranslogRecovery.get() : &quot;translogRecovery is not pending but should be&quot;;</b>
<b class="nc"><i>467</i>&nbsp;        pendingTranslogRecovery.set(false); // we are good - now we can commit</b>
<b class="nc"><i>468</i>&nbsp;    }</b>
<i>469</i>&nbsp;
<i>470</i>&nbsp;    private void recoverFromTranslogInternal(TranslogRecoveryRunner translogRecoveryRunner, long recoverUpToSeqNo) throws IOException {
<b class="fc"><i>471</i>&nbsp;        Translog.TranslogGeneration translogGeneration = translog.getGeneration();</b>
<i>472</i>&nbsp;        final int opsRecovered;
<b class="fc"><i>473</i>&nbsp;        final long translogFileGen = Long.parseLong(lastCommittedSegmentInfos.getUserData().get(Translog.TRANSLOG_GENERATION_KEY));</b>
<b class="fc"><i>474</i>&nbsp;        try (Translog.Snapshot snapshot = translog.newSnapshotFromGen(</b>
<b class="fc"><i>475</i>&nbsp;            new Translog.TranslogGeneration(translog.getTranslogUUID(), translogFileGen), recoverUpToSeqNo)) {</b>
<b class="fc"><i>476</i>&nbsp;            opsRecovered = translogRecoveryRunner.run(this, snapshot);</b>
<b class="fc"><i>477</i>&nbsp;        } catch (Exception e) {</b>
<b class="nc"><i>478</i>&nbsp;            throw new EngineException(shardId, &quot;failed to recover from translog&quot;, e);</b>
<b class="fc"><i>479</i>&nbsp;        }</b>
<i>480</i>&nbsp;        // flush if we recovered something or if we have references to older translogs
<i>481</i>&nbsp;        // note: if opsRecovered == 0 and we have older translogs it means they are corrupted or 0 length.
<b class="fc"><i>482</i>&nbsp;        assert pendingTranslogRecovery.get() : &quot;translogRecovery is not pending but should be&quot;;</b>
<b class="fc"><i>483</i>&nbsp;        pendingTranslogRecovery.set(false); // we are good - now we can commit</b>
<b class="fc"><i>484</i>&nbsp;        if (opsRecovered &gt; 0) {</b>
<b class="nc"><i>485</i>&nbsp;            logger.trace(&quot;flushing post recovery from translog. ops recovered [{}]. committed translog id [{}]. current id [{}]&quot;,</b>
<b class="nc"><i>486</i>&nbsp;                opsRecovered, translogGeneration == null ? null :</b>
<b class="nc"><i>487</i>&nbsp;                    translogGeneration.translogFileGeneration, translog.currentFileGeneration());</b>
<b class="nc"><i>488</i>&nbsp;            commitIndexWriter(indexWriter, translog, null);</b>
<b class="nc"><i>489</i>&nbsp;            refreshLastCommittedSegmentInfos();</b>
<b class="nc"><i>490</i>&nbsp;            refresh(&quot;translog_recovery&quot;);</b>
<i>491</i>&nbsp;        }
<b class="fc"><i>492</i>&nbsp;        translog.trimUnreferencedReaders();</b>
<b class="fc"><i>493</i>&nbsp;    }</b>
<i>494</i>&nbsp;
<i>495</i>&nbsp;    private Translog openTranslog(EngineConfig engineConfig, TranslogDeletionPolicy translogDeletionPolicy,
<i>496</i>&nbsp;                                  LongSupplier globalCheckpointSupplier, LongConsumer persistedSequenceNumberConsumer) throws IOException {
<i>497</i>&nbsp;
<b class="fc"><i>498</i>&nbsp;        final TranslogConfig translogConfig = engineConfig.getTranslogConfig();</b>
<b class="fc"><i>499</i>&nbsp;        final String translogUUID = loadTranslogUUIDFromLastCommit();</b>
<i>500</i>&nbsp;        // We expect that this shard already exists, so it must already have an existing translog else something is badly wrong!
<b class="fc"><i>501</i>&nbsp;        return new Translog(translogConfig, translogUUID, translogDeletionPolicy, globalCheckpointSupplier,</b>
<b class="fc"><i>502</i>&nbsp;            engineConfig.getPrimaryTermSupplier(), persistedSequenceNumberConsumer);</b>
<i>503</i>&nbsp;    }
<i>504</i>&nbsp;
<i>505</i>&nbsp;    // Package private for testing purposes only
<i>506</i>&nbsp;    Translog getTranslog() {
<b class="fc"><i>507</i>&nbsp;        ensureOpen();</b>
<b class="fc"><i>508</i>&nbsp;        return translog;</b>
<i>509</i>&nbsp;    }
<i>510</i>&nbsp;
<i>511</i>&nbsp;    // Package private for testing purposes only
<i>512</i>&nbsp;    boolean hasSnapshottedCommits() {
<b class="nc"><i>513</i>&nbsp;        return combinedDeletionPolicy.hasSnapshottedCommits();</b>
<i>514</i>&nbsp;    }
<i>515</i>&nbsp;
<i>516</i>&nbsp;    @Override
<i>517</i>&nbsp;    public boolean isTranslogSyncNeeded() {
<b class="nc"><i>518</i>&nbsp;        return getTranslog().syncNeeded();</b>
<i>519</i>&nbsp;    }
<i>520</i>&nbsp;
<i>521</i>&nbsp;    @Override
<i>522</i>&nbsp;    public boolean ensureTranslogSynced(Stream&lt;Translog.Location&gt; locations) throws IOException {
<b class="nc"><i>523</i>&nbsp;        final boolean synced = translog.ensureSynced(locations);</b>
<b class="nc"><i>524</i>&nbsp;        if (synced) {</b>
<b class="nc"><i>525</i>&nbsp;            revisitIndexDeletionPolicyOnTranslogSynced();</b>
<i>526</i>&nbsp;        }
<b class="nc"><i>527</i>&nbsp;        return synced;</b>
<i>528</i>&nbsp;    }
<i>529</i>&nbsp;
<i>530</i>&nbsp;    @Override
<i>531</i>&nbsp;    public void syncTranslog() throws IOException {
<b class="fc"><i>532</i>&nbsp;        translog.sync();</b>
<b class="fc"><i>533</i>&nbsp;        revisitIndexDeletionPolicyOnTranslogSynced();</b>
<b class="fc"><i>534</i>&nbsp;    }</b>
<i>535</i>&nbsp;
<i>536</i>&nbsp;    /**
<i>537</i>&nbsp;     * Creates a new history snapshot for reading operations since the provided seqno.
<i>538</i>&nbsp;     * The returned snapshot can be retrieved from either Lucene index or translog files.
<i>539</i>&nbsp;     */
<i>540</i>&nbsp;    @Override
<i>541</i>&nbsp;    public Translog.Snapshot readHistoryOperations(String reason, HistorySource historySource,
<i>542</i>&nbsp;                                                   MapperService mapperService, long startingSeqNo) throws IOException {
<b class="nc"><i>543</i>&nbsp;        if (historySource == HistorySource.INDEX) {</b>
<b class="nc"><i>544</i>&nbsp;            ensureSoftDeletesEnabled();</b>
<b class="nc"><i>545</i>&nbsp;            return newChangesSnapshot(reason, mapperService, Math.max(0, startingSeqNo), Long.MAX_VALUE, false);</b>
<i>546</i>&nbsp;        } else {
<b class="nc"><i>547</i>&nbsp;            return getTranslog().newSnapshotFromMinSeqNo(startingSeqNo);</b>
<i>548</i>&nbsp;        }
<i>549</i>&nbsp;    }
<i>550</i>&nbsp;
<i>551</i>&nbsp;    /**
<i>552</i>&nbsp;     * Returns the estimated number of history operations whose seq# at least the provided seq# in this engine.
<i>553</i>&nbsp;     */
<i>554</i>&nbsp;    @Override
<i>555</i>&nbsp;    public int estimateNumberOfHistoryOperations(String reason, HistorySource historySource,
<i>556</i>&nbsp;                                                 MapperService mapperService, long startingSeqNo) throws IOException {
<b class="nc"><i>557</i>&nbsp;        if (historySource == HistorySource.INDEX) {</b>
<b class="nc"><i>558</i>&nbsp;            ensureSoftDeletesEnabled();</b>
<b class="nc"><i>559</i>&nbsp;            try (Translog.Snapshot snapshot = newChangesSnapshot(reason, mapperService, Math.max(0, startingSeqNo),</b>
<i>560</i>&nbsp;                Long.MAX_VALUE, false)) {
<b class="nc"><i>561</i>&nbsp;                return snapshot.totalOperations();</b>
<b class="nc"><i>562</i>&nbsp;            }</b>
<i>563</i>&nbsp;        } else {
<b class="nc"><i>564</i>&nbsp;            return getTranslog().estimateTotalOperationsFromMinSeq(startingSeqNo);</b>
<i>565</i>&nbsp;        }
<i>566</i>&nbsp;    }
<i>567</i>&nbsp;
<i>568</i>&nbsp;    @Override
<i>569</i>&nbsp;    public TranslogStats getTranslogStats() {
<b class="nc"><i>570</i>&nbsp;        return getTranslog().stats();</b>
<i>571</i>&nbsp;    }
<i>572</i>&nbsp;
<i>573</i>&nbsp;    @Override
<i>574</i>&nbsp;    public Translog.Location getTranslogLastWriteLocation() {
<b class="fc"><i>575</i>&nbsp;        return getTranslog().getLastWriteLocation();</b>
<i>576</i>&nbsp;    }
<i>577</i>&nbsp;
<i>578</i>&nbsp;    private void revisitIndexDeletionPolicyOnTranslogSynced() throws IOException {
<b class="fc"><i>579</i>&nbsp;        if (combinedDeletionPolicy.hasUnreferencedCommits()) {</b>
<b class="nc"><i>580</i>&nbsp;            indexWriter.deleteUnusedFiles();</b>
<b class="nc"><i>581</i>&nbsp;            translog.trimUnreferencedReaders();</b>
<i>582</i>&nbsp;        }
<b class="fc"><i>583</i>&nbsp;    }</b>
<i>584</i>&nbsp;
<i>585</i>&nbsp;    @Override
<i>586</i>&nbsp;    public String getHistoryUUID() {
<b class="fc"><i>587</i>&nbsp;        return historyUUID;</b>
<i>588</i>&nbsp;    }
<i>589</i>&nbsp;
<i>590</i>&nbsp;    /** Returns how many bytes we are currently moving from indexing buffer to segments on disk */
<i>591</i>&nbsp;    @Override
<i>592</i>&nbsp;    public long getWritingBytes() {
<b class="fc"><i>593</i>&nbsp;        return indexWriter.getFlushingBytes() + versionMap.getRefreshingBytes();</b>
<i>594</i>&nbsp;    }
<i>595</i>&nbsp;
<i>596</i>&nbsp;    /**
<i>597</i>&nbsp;     * Reads the current stored translog ID from the last commit data.
<i>598</i>&nbsp;     */
<i>599</i>&nbsp;    @Nullable
<i>600</i>&nbsp;    private String loadTranslogUUIDFromLastCommit() throws IOException {
<b class="fc"><i>601</i>&nbsp;        final Map&lt;String, String&gt; commitUserData = store.readLastCommittedSegmentsInfo().getUserData();</b>
<b class="fc"><i>602</i>&nbsp;        if (commitUserData.containsKey(Translog.TRANSLOG_GENERATION_KEY) == false) {</b>
<b class="nc"><i>603</i>&nbsp;            throw new IllegalStateException(&quot;commit doesn&#39;t contain translog generation id&quot;);</b>
<i>604</i>&nbsp;        }
<b class="fc"><i>605</i>&nbsp;        return commitUserData.get(Translog.TRANSLOG_UUID_KEY);</b>
<i>606</i>&nbsp;    }
<i>607</i>&nbsp;
<i>608</i>&nbsp;    /**
<i>609</i>&nbsp;     * Reads the current stored history ID from the IW commit data.
<i>610</i>&nbsp;     */
<i>611</i>&nbsp;    private String loadHistoryUUID(final IndexWriter writer) {
<b class="fc"><i>612</i>&nbsp;        final String uuid = commitDataAsMap(writer).get(HISTORY_UUID_KEY);</b>
<b class="fc"><i>613</i>&nbsp;        if (uuid == null) {</b>
<b class="nc"><i>614</i>&nbsp;            throw new IllegalStateException(&quot;commit doesn&#39;t contain history uuid&quot;);</b>
<i>615</i>&nbsp;        }
<b class="fc"><i>616</i>&nbsp;        return uuid;</b>
<i>617</i>&nbsp;    }
<i>618</i>&nbsp;
<i>619</i>&nbsp;    private ExternalReaderManager createReaderManager(RefreshWarmerListener externalRefreshListener) throws EngineException {
<b class="fc"><i>620</i>&nbsp;        boolean success = false;</b>
<b class="fc"><i>621</i>&nbsp;        ElasticsearchReaderManager internalReaderManager = null;</b>
<i>622</i>&nbsp;        try {
<i>623</i>&nbsp;            try {
<b class="fc"><i>624</i>&nbsp;                final ElasticsearchDirectoryReader directoryReader =</b>
<b class="fc"><i>625</i>&nbsp;                    ElasticsearchDirectoryReader.wrap(DirectoryReader.open(indexWriter), shardId);</b>
<b class="fc"><i>626</i>&nbsp;                internalReaderManager = new ElasticsearchReaderManager(directoryReader,</b>
<b class="fc"><i>627</i>&nbsp;                       new RamAccountingRefreshListener(engineConfig.getCircuitBreakerService()));</b>
<b class="fc"><i>628</i>&nbsp;                lastCommittedSegmentInfos = store.readLastCommittedSegmentsInfo();</b>
<b class="fc"><i>629</i>&nbsp;                ExternalReaderManager externalReaderManager = new ExternalReaderManager(internalReaderManager, externalRefreshListener);</b>
<b class="fc"><i>630</i>&nbsp;                success = true;</b>
<b class="fc"><i>631</i>&nbsp;                return externalReaderManager;</b>
<b class="nc"><i>632</i>&nbsp;            } catch (IOException e) {</b>
<b class="nc"><i>633</i>&nbsp;                maybeFailEngine(&quot;start&quot;, e);</b>
<i>634</i>&nbsp;                try {
<b class="nc"><i>635</i>&nbsp;                    indexWriter.rollback();</b>
<b class="nc"><i>636</i>&nbsp;                } catch (IOException inner) { // iw is closed below</b>
<b class="nc"><i>637</i>&nbsp;                    e.addSuppressed(inner);</b>
<b class="nc"><i>638</i>&nbsp;                }</b>
<b class="nc"><i>639</i>&nbsp;                throw new EngineCreationFailureException(shardId, &quot;failed to open reader on writer&quot;, e);</b>
<i>640</i>&nbsp;            }
<i>641</i>&nbsp;        } finally {
<b class="fc"><i>642</i>&nbsp;            if (success == false) { // release everything we created on a failure</b>
<b class="nc"><i>643</i>&nbsp;                IOUtils.closeWhileHandlingException(internalReaderManager, indexWriter);</b>
<i>644</i>&nbsp;            }
<b class="nc"><i>645</i>&nbsp;        }</b>
<i>646</i>&nbsp;    }
<i>647</i>&nbsp;
<i>648</i>&nbsp;    @Override
<i>649</i>&nbsp;    public GetResult get(Get get, BiFunction&lt;String, SearcherScope, Engine.Searcher&gt; searcherFactory) throws EngineException {
<b class="nc"><i>650</i>&nbsp;        assert Objects.equals(get.uid().field(), IdFieldMapper.NAME) : get.uid().field();</b>
<b class="nc"><i>651</i>&nbsp;        try (ReleasableLock ignored = readLock.acquire()) {</b>
<b class="nc"><i>652</i>&nbsp;            ensureOpen();</b>
<i>653</i>&nbsp;            SearcherScope scope;
<b class="nc"><i>654</i>&nbsp;            if (get.realtime()) {</b>
<b class="nc"><i>655</i>&nbsp;                VersionValue versionValue = null;</b>
<b class="nc"><i>656</i>&nbsp;                try (Releasable ignore = versionMap.acquireLock(get.uid().bytes())) {</b>
<i>657</i>&nbsp;                    // we need to lock here to access the version map to do this truly in RT
<b class="nc"><i>658</i>&nbsp;                    versionValue = getVersionFromMap(get.uid().bytes());</b>
<b class="nc"><i>659</i>&nbsp;                }</b>
<b class="nc"><i>660</i>&nbsp;                if (versionValue != null) {</b>
<b class="nc"><i>661</i>&nbsp;                    if (versionValue.isDelete()) {</b>
<b class="nc"><i>662</i>&nbsp;                        return GetResult.NOT_EXISTS;</b>
<i>663</i>&nbsp;                    }
<b class="nc"><i>664</i>&nbsp;                    if (get.versionType().isVersionConflictForReads(versionValue.version, get.version())) {</b>
<b class="nc"><i>665</i>&nbsp;                        throw new VersionConflictEngineException(shardId, get.id(),</b>
<b class="nc"><i>666</i>&nbsp;                            get.versionType().explainConflictForReads(versionValue.version, get.version()));</b>
<i>667</i>&nbsp;                    }
<b class="nc"><i>668</i>&nbsp;                    if (get.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO &amp;&amp; (</b>
<b class="nc"><i>669</i>&nbsp;                        get.getIfSeqNo() != versionValue.seqNo || get.getIfPrimaryTerm() != versionValue.term</b>
<i>670</i>&nbsp;                        )) {
<b class="nc"><i>671</i>&nbsp;                        throw new VersionConflictEngineException(shardId, get.id(),</b>
<b class="nc"><i>672</i>&nbsp;                            get.getIfSeqNo(), get.getIfPrimaryTerm(), versionValue.seqNo, versionValue.term);</b>
<i>673</i>&nbsp;                    }
<b class="nc"><i>674</i>&nbsp;                    if (get.isReadFromTranslog()) {</b>
<i>675</i>&nbsp;                        // this is only used for updates - API _GET calls will always read form a reader for consistency
<i>676</i>&nbsp;                        // the update call doesn&#39;t need the consistency since it&#39;s source only + _parent but parent can go away in 7.0
<b class="nc"><i>677</i>&nbsp;                        if (versionValue.getLocation() != null) {</b>
<i>678</i>&nbsp;                            try {
<b class="nc"><i>679</i>&nbsp;                                Translog.Operation operation = translog.readOperation(versionValue.getLocation());</b>
<b class="nc"><i>680</i>&nbsp;                                if (operation != null) {</b>
<i>681</i>&nbsp;                                    // in the case of a already pruned translog generation we might get null here - yet very unlikely
<b class="nc"><i>682</i>&nbsp;                                    final Translog.Index index = (Translog.Index) operation;</b>
<b class="nc"><i>683</i>&nbsp;                                    TranslogLeafReader reader = new TranslogLeafReader(index);</b>
<b class="nc"><i>684</i>&nbsp;                                    return new GetResult(new Engine.Searcher(&quot;realtime_get&quot;, reader,</b>
<b class="nc"><i>685</i>&nbsp;                                        IndexSearcher.getDefaultSimilarity(), null, IndexSearcher.getDefaultQueryCachingPolicy(), reader),</b>
<b class="nc"><i>686</i>&nbsp;                                        new VersionsAndSeqNoResolver.DocIdAndVersion(0, index.version(), index.seqNo(), index.primaryTerm(),</b>
<i>687</i>&nbsp;                                            reader, 0));
<i>688</i>&nbsp;                                }
<b class="nc"><i>689</i>&nbsp;                            } catch (IOException e) {</b>
<b class="nc"><i>690</i>&nbsp;                                maybeFailEngine(&quot;realtime_get&quot;, e); // lets check if the translog has failed with a tragic event</b>
<b class="nc"><i>691</i>&nbsp;                                throw new EngineException(shardId, &quot;failed to read operation from translog&quot;, e);</b>
<b class="nc"><i>692</i>&nbsp;                            }</b>
<i>693</i>&nbsp;                        } else {
<b class="nc"><i>694</i>&nbsp;                            trackTranslogLocation.set(true);</b>
<i>695</i>&nbsp;                        }
<i>696</i>&nbsp;                    }
<b class="nc"><i>697</i>&nbsp;                    assert versionValue.seqNo &gt;= 0 : versionValue;</b>
<b class="nc"><i>698</i>&nbsp;                    refreshIfNeeded(&quot;realtime_get&quot;, versionValue.seqNo);</b>
<i>699</i>&nbsp;                }
<b class="nc"><i>700</i>&nbsp;                scope = SearcherScope.INTERNAL;</b>
<b class="nc"><i>701</i>&nbsp;            } else {</b>
<i>702</i>&nbsp;                // we expose what has been externally expose in a point in time snapshot via an explicit refresh
<b class="nc"><i>703</i>&nbsp;                scope = SearcherScope.EXTERNAL;</b>
<i>704</i>&nbsp;            }
<i>705</i>&nbsp;
<i>706</i>&nbsp;            // no version, get the version from the index, we know that we refresh on flush
<b class="nc"><i>707</i>&nbsp;            return getFromSearcher(get, searcherFactory, scope);</b>
<b class="nc"><i>708</i>&nbsp;        }</b>
<i>709</i>&nbsp;    }
<i>710</i>&nbsp;
<i>711</i>&nbsp;    /**
<i>712</i>&nbsp;     * the status of the current doc version in lucene, compared to the version in an incoming
<i>713</i>&nbsp;     * operation
<i>714</i>&nbsp;     */
<i>715</i>&nbsp;    enum OpVsLuceneDocStatus {
<i>716</i>&nbsp;        /** the op is more recent than the one that last modified the doc found in lucene*/
<i>717</i>&nbsp;        OP_NEWER,
<i>718</i>&nbsp;        /** the op is older or the same as the one that last modified the doc found in lucene*/
<i>719</i>&nbsp;        OP_STALE_OR_EQUAL,
<i>720</i>&nbsp;        /** no doc was found in lucene */
<i>721</i>&nbsp;        LUCENE_DOC_NOT_FOUND
<i>722</i>&nbsp;    }
<i>723</i>&nbsp;
<i>724</i>&nbsp;    private static OpVsLuceneDocStatus compareOpToVersionMapOnSeqNo(String id, long seqNo, long primaryTerm, VersionValue versionValue) {
<b class="nc"><i>725</i>&nbsp;        Objects.requireNonNull(versionValue);</b>
<b class="nc"><i>726</i>&nbsp;        if (seqNo &gt; versionValue.seqNo) {</b>
<b class="nc"><i>727</i>&nbsp;            return OpVsLuceneDocStatus.OP_NEWER;</b>
<b class="nc"><i>728</i>&nbsp;        } else if (seqNo == versionValue.seqNo) {</b>
<b class="nc"><i>729</i>&nbsp;            assert versionValue.term == primaryTerm : &quot;primary term not matched; id=&quot; + id + &quot; seq_no=&quot; + seqNo</b>
<i>730</i>&nbsp;                + &quot; op_term=&quot; + primaryTerm + &quot; existing_term=&quot; + versionValue.term;
<b class="nc"><i>731</i>&nbsp;            return OpVsLuceneDocStatus.OP_STALE_OR_EQUAL;</b>
<i>732</i>&nbsp;        } else {
<b class="nc"><i>733</i>&nbsp;            return OpVsLuceneDocStatus.OP_STALE_OR_EQUAL;</b>
<i>734</i>&nbsp;        }
<i>735</i>&nbsp;    }
<i>736</i>&nbsp;
<i>737</i>&nbsp;    private OpVsLuceneDocStatus compareOpToLuceneDocBasedOnSeqNo(final Operation op) throws IOException {
<b class="nc"><i>738</i>&nbsp;        assert op.seqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO : &quot;resolving ops based on seq# but no seqNo is found&quot;;</b>
<i>739</i>&nbsp;        final OpVsLuceneDocStatus status;
<b class="nc"><i>740</i>&nbsp;        VersionValue versionValue = getVersionFromMap(op.uid().bytes());</b>
<b class="nc"><i>741</i>&nbsp;        assert incrementVersionLookup();</b>
<b class="nc"><i>742</i>&nbsp;        if (versionValue != null) {</b>
<b class="nc"><i>743</i>&nbsp;            status = compareOpToVersionMapOnSeqNo(op.id(), op.seqNo(), op.primaryTerm(), versionValue);</b>
<i>744</i>&nbsp;        } else {
<i>745</i>&nbsp;            // load from index
<b class="nc"><i>746</i>&nbsp;            assert incrementIndexVersionLookup();</b>
<b class="nc"><i>747</i>&nbsp;            try (Searcher searcher = acquireSearcher(&quot;load_seq_no&quot;, SearcherScope.INTERNAL)) {</b>
<b class="nc"><i>748</i>&nbsp;                final DocIdAndSeqNo docAndSeqNo = VersionsAndSeqNoResolver.loadDocIdAndSeqNo(searcher.getIndexReader(), op.uid());</b>
<b class="nc"><i>749</i>&nbsp;                if (docAndSeqNo == null) {</b>
<b class="nc"><i>750</i>&nbsp;                    status = OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND;</b>
<b class="nc"><i>751</i>&nbsp;                } else if (op.seqNo() &gt; docAndSeqNo.seqNo) {</b>
<b class="nc"><i>752</i>&nbsp;                    status = OpVsLuceneDocStatus.OP_NEWER;</b>
<b class="nc"><i>753</i>&nbsp;                } else if (op.seqNo() == docAndSeqNo.seqNo) {</b>
<b class="nc"><i>754</i>&nbsp;                    assert localCheckpointTracker.hasProcessed(op.seqNo()) || softDeleteEnabled == false :</b>
<b class="nc"><i>755</i>&nbsp;                        &quot;local checkpoint tracker is not updated seq_no=&quot; + op.seqNo() + &quot; id=&quot; + op.id();</b>
<b class="nc"><i>756</i>&nbsp;                    status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL;</b>
<i>757</i>&nbsp;                } else {
<b class="nc"><i>758</i>&nbsp;                    status = OpVsLuceneDocStatus.OP_STALE_OR_EQUAL;</b>
<i>759</i>&nbsp;                }
<b class="nc"><i>760</i>&nbsp;            }</b>
<i>761</i>&nbsp;        }
<b class="nc"><i>762</i>&nbsp;        return status;</b>
<i>763</i>&nbsp;    }
<i>764</i>&nbsp;
<i>765</i>&nbsp;    /** resolves the current version of the document, returning null if not found */
<i>766</i>&nbsp;    private VersionValue resolveDocVersion(final Operation op, boolean loadSeqNo) throws IOException {
<b class="nc"><i>767</i>&nbsp;        assert incrementVersionLookup(); // used for asserting in tests</b>
<b class="nc"><i>768</i>&nbsp;        VersionValue versionValue = getVersionFromMap(op.uid().bytes());</b>
<b class="nc"><i>769</i>&nbsp;        if (versionValue == null) {</b>
<b class="nc"><i>770</i>&nbsp;            assert incrementIndexVersionLookup(); // used for asserting in tests</b>
<i>771</i>&nbsp;            final VersionsAndSeqNoResolver.DocIdAndVersion docIdAndVersion;
<b class="nc"><i>772</i>&nbsp;            try (Searcher searcher = acquireSearcher(&quot;load_version&quot;, SearcherScope.INTERNAL)) {</b>
<b class="nc"><i>773</i>&nbsp;                 docIdAndVersion = VersionsAndSeqNoResolver.loadDocIdAndVersion(searcher.getIndexReader(), op.uid(), loadSeqNo);</b>
<b class="nc"><i>774</i>&nbsp;            }</b>
<b class="nc"><i>775</i>&nbsp;            if (docIdAndVersion != null) {</b>
<b class="nc"><i>776</i>&nbsp;                versionValue = new IndexVersionValue(null, docIdAndVersion.version, docIdAndVersion.seqNo, docIdAndVersion.primaryTerm);</b>
<i>777</i>&nbsp;            }
<b class="nc"><i>778</i>&nbsp;        } else if (engineConfig.isEnableGcDeletes() &amp;&amp; versionValue.isDelete() &amp;&amp;</b>
<b class="nc"><i>779</i>&nbsp;            (engineConfig.getThreadPool().relativeTimeInMillis() - ((DeleteVersionValue)versionValue).time) &gt; getGcDeletesInMillis()) {</b>
<b class="nc"><i>780</i>&nbsp;            versionValue = null;</b>
<i>781</i>&nbsp;        }
<b class="nc"><i>782</i>&nbsp;        return versionValue;</b>
<i>783</i>&nbsp;    }
<i>784</i>&nbsp;
<i>785</i>&nbsp;    private VersionValue getVersionFromMap(BytesRef id) {
<b class="nc"><i>786</i>&nbsp;        if (versionMap.isUnsafe()) {</b>
<b class="nc"><i>787</i>&nbsp;            synchronized (versionMap) {</b>
<i>788</i>&nbsp;                // we are switching from an unsafe map to a safe map. This might happen concurrently
<i>789</i>&nbsp;                // but we only need to do this once since the last operation per ID is to add to the version
<i>790</i>&nbsp;                // map so once we pass this point we can safely lookup from the version map.
<b class="nc"><i>791</i>&nbsp;                if (versionMap.isUnsafe()) {</b>
<b class="nc"><i>792</i>&nbsp;                    refresh(&quot;unsafe_version_map&quot;, SearcherScope.INTERNAL, true);</b>
<i>793</i>&nbsp;                }
<b class="nc"><i>794</i>&nbsp;                versionMap.enforceSafeAccess();</b>
<b class="nc"><i>795</i>&nbsp;            }</b>
<i>796</i>&nbsp;        }
<b class="nc"><i>797</i>&nbsp;        return versionMap.getUnderLock(id);</b>
<i>798</i>&nbsp;    }
<i>799</i>&nbsp;
<i>800</i>&nbsp;    private boolean canOptimizeAddDocument(Index index) {
<b class="nc"><i>801</i>&nbsp;        if (index.getAutoGeneratedIdTimestamp() != IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP) {</b>
<b class="nc"><i>802</i>&nbsp;            assert index.getAutoGeneratedIdTimestamp() &gt;= 0 : &quot;autoGeneratedIdTimestamp must be positive but was: &quot;</b>
<b class="nc"><i>803</i>&nbsp;                + index.getAutoGeneratedIdTimestamp();</b>
<b class="pc"><i>804</i>&nbsp;            switch (index.origin()) {</b>
<i>805</i>&nbsp;                case PRIMARY:
<b class="nc"><i>806</i>&nbsp;                    assert assertPrimaryCanOptimizeAddDocument(index);</b>
<b class="nc"><i>807</i>&nbsp;                    return true;</b>
<i>808</i>&nbsp;                case PEER_RECOVERY:
<i>809</i>&nbsp;                case REPLICA:
<b class="nc"><i>810</i>&nbsp;                    assert index.version() == 1 &amp;&amp; index.versionType() == null</b>
<b class="nc"><i>811</i>&nbsp;                        : &quot;version: &quot; + index.version() + &quot; type: &quot; + index.versionType();</b>
<b class="nc"><i>812</i>&nbsp;                    return true;</b>
<i>813</i>&nbsp;                case LOCAL_TRANSLOG_RECOVERY:
<i>814</i>&nbsp;                case LOCAL_RESET:
<b class="nc"><i>815</i>&nbsp;                    assert index.isRetry();</b>
<b class="nc"><i>816</i>&nbsp;                    return true; // allow to optimize in order to update the max safe time stamp</b>
<i>817</i>&nbsp;                default:
<b class="nc"><i>818</i>&nbsp;                    throw new IllegalArgumentException(&quot;unknown origin &quot; + index.origin());</b>
<i>819</i>&nbsp;            }
<i>820</i>&nbsp;        }
<b class="nc"><i>821</i>&nbsp;        return false;</b>
<i>822</i>&nbsp;    }
<i>823</i>&nbsp;
<i>824</i>&nbsp;    protected boolean assertPrimaryCanOptimizeAddDocument(final Index index) {
<b class="nc"><i>825</i>&nbsp;        assert (index.version() == Versions.MATCH_DELETED || index.version() == Versions.MATCH_ANY) &amp;&amp;</b>
<b class="nc"><i>826</i>&nbsp;            index.versionType() == VersionType.INTERNAL</b>
<b class="nc"><i>827</i>&nbsp;            : &quot;version: &quot; + index.version() + &quot; type: &quot; + index.versionType();</b>
<b class="nc"><i>828</i>&nbsp;        return true;</b>
<i>829</i>&nbsp;    }
<i>830</i>&nbsp;
<i>831</i>&nbsp;    private boolean assertIncomingSequenceNumber(final Engine.Operation.Origin origin, final long seqNo) {
<b class="nc"><i>832</i>&nbsp;        if (origin == Operation.Origin.PRIMARY) {</b>
<b class="nc"><i>833</i>&nbsp;            assert assertPrimaryIncomingSequenceNumber(origin, seqNo);</b>
<i>834</i>&nbsp;        } else {
<i>835</i>&nbsp;            // sequence number should be set when operation origin is not primary
<b class="nc"><i>836</i>&nbsp;            assert seqNo &gt;= 0 : &quot;recovery or replica ops should have an assigned seq no.; origin: &quot; + origin;</b>
<i>837</i>&nbsp;        }
<b class="nc"><i>838</i>&nbsp;        return true;</b>
<i>839</i>&nbsp;    }
<i>840</i>&nbsp;
<i>841</i>&nbsp;    protected boolean assertPrimaryIncomingSequenceNumber(final Engine.Operation.Origin origin, final long seqNo) {
<i>842</i>&nbsp;        // sequence number should not be set when operation origin is primary
<b class="nc"><i>843</i>&nbsp;        assert seqNo == SequenceNumbers.UNASSIGNED_SEQ_NO</b>
<i>844</i>&nbsp;                : &quot;primary operations must never have an assigned sequence number but was [&quot; + seqNo + &quot;]&quot;;
<b class="nc"><i>845</i>&nbsp;        return true;</b>
<i>846</i>&nbsp;    }
<i>847</i>&nbsp;
<i>848</i>&nbsp;    protected long generateSeqNoForOperationOnPrimary(final Operation operation) {
<b class="nc"><i>849</i>&nbsp;        assert operation.origin() == Operation.Origin.PRIMARY;</b>
<b class="nc"><i>850</i>&nbsp;        assert operation.seqNo() == SequenceNumbers.UNASSIGNED_SEQ_NO :</b>
<b class="nc"><i>851</i>&nbsp;            &quot;ops should not have an assigned seq no. but was: &quot; + operation.seqNo();</b>
<b class="nc"><i>852</i>&nbsp;        return doGenerateSeqNoForOperation(operation);</b>
<i>853</i>&nbsp;    }
<i>854</i>&nbsp;
<i>855</i>&nbsp;    protected void advanceMaxSeqNoOfUpdatesOrDeletesOnPrimary(long seqNo) {
<b class="nc"><i>856</i>&nbsp;        advanceMaxSeqNoOfUpdatesOrDeletes(seqNo);</b>
<b class="nc"><i>857</i>&nbsp;    }</b>
<i>858</i>&nbsp;
<i>859</i>&nbsp;    /**
<i>860</i>&nbsp;     * Generate the sequence number for the specified operation.
<i>861</i>&nbsp;     *
<i>862</i>&nbsp;     * @param operation the operation
<i>863</i>&nbsp;     * @return the sequence number
<i>864</i>&nbsp;     */
<i>865</i>&nbsp;    long doGenerateSeqNoForOperation(final Operation operation) {
<b class="nc"><i>866</i>&nbsp;        return localCheckpointTracker.generateSeqNo();</b>
<i>867</i>&nbsp;    }
<i>868</i>&nbsp;
<i>869</i>&nbsp;    @Override
<i>870</i>&nbsp;    public IndexResult index(Index index) throws IOException {
<b class="nc"><i>871</i>&nbsp;        assert Objects.equals(index.uid().field(), IdFieldMapper.NAME) : index.uid().field();</b>
<b class="nc"><i>872</i>&nbsp;        final boolean doThrottle = index.origin().isRecovery() == false;</b>
<b class="nc"><i>873</i>&nbsp;        try (ReleasableLock releasableLock = readLock.acquire()) {</b>
<b class="nc"><i>874</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>875</i>&nbsp;            assert assertIncomingSequenceNumber(index.origin(), index.seqNo());</b>
<b class="nc"><i>876</i>&nbsp;            try (Releasable ignored = versionMap.acquireLock(index.uid().bytes());</b>
<b class="nc"><i>877</i>&nbsp;                Releasable indexThrottle = doThrottle ? () -&gt; {} : throttle.acquireThrottle()) {</b>
<b class="nc"><i>878</i>&nbsp;                lastWriteNanos = index.startTime();</b>
<i>879</i>&nbsp;                /* A NOTE ABOUT APPEND ONLY OPTIMIZATIONS:
<i>880</i>&nbsp;                 * if we have an autoGeneratedID that comes into the engine we can potentially optimize
<i>881</i>&nbsp;                 * and just use addDocument instead of updateDocument and skip the entire version and index lookupVersion across the board.
<i>882</i>&nbsp;                 * Yet, we have to deal with multiple document delivery, for this we use a property of the document that is added
<i>883</i>&nbsp;                 * to detect if it has potentially been added before. We use the documents timestamp for this since it&#39;s something
<i>884</i>&nbsp;                 * that:
<i>885</i>&nbsp;                 *  - doesn&#39;t change per document
<i>886</i>&nbsp;                 *  - is preserved in the transaction log
<i>887</i>&nbsp;                 *  - and is assigned before we start to index / replicate
<i>888</i>&nbsp;                 * NOTE: it&#39;s not important for this timestamp to be consistent across nodes etc. it&#39;s just a number that is in the common
<i>889</i>&nbsp;                 * case increasing and can be used in the failure case when we retry and resent documents to establish a happens before
<i>890</i>&nbsp;                 * relationship. For instance:
<i>891</i>&nbsp;                 *  - doc A has autoGeneratedIdTimestamp = 10, isRetry = false
<i>892</i>&nbsp;                 *  - doc B has autoGeneratedIdTimestamp = 9, isRetry = false
<i>893</i>&nbsp;                 *
<i>894</i>&nbsp;                 *  while both docs are in in flight, we disconnect on one node, reconnect and send doc A again
<i>895</i>&nbsp;                 *  - now doc A&#39; has autoGeneratedIdTimestamp = 10, isRetry = true
<i>896</i>&nbsp;                 *
<i>897</i>&nbsp;                 *  if A&#39; arrives on the shard first we update maxUnsafeAutoIdTimestamp to 10 and use update document. All subsequent
<i>898</i>&nbsp;                 *  documents that arrive (A and B) will also use updateDocument since their timestamps are less than
<i>899</i>&nbsp;                 *  maxUnsafeAutoIdTimestamp. While this is not strictly needed for doc B it is just much simpler to implement since it
<i>900</i>&nbsp;                 *  will just de-optimize some doc in the worst case.
<i>901</i>&nbsp;                 *
<i>902</i>&nbsp;                 *  if A arrives on the shard first we use addDocument since maxUnsafeAutoIdTimestamp is &lt; 10. A` will then just be skipped
<i>903</i>&nbsp;                 *  or calls updateDocument.
<i>904</i>&nbsp;                 */
<b class="nc"><i>905</i>&nbsp;                final IndexingStrategy plan = indexingStrategyForOperation(index);</b>
<i>906</i>&nbsp;
<i>907</i>&nbsp;                final IndexResult indexResult;
<b class="nc"><i>908</i>&nbsp;                if (plan.earlyResultOnPreFlightError.isPresent()) {</b>
<b class="nc"><i>909</i>&nbsp;                    indexResult = plan.earlyResultOnPreFlightError.get();</b>
<b class="nc"><i>910</i>&nbsp;                    assert indexResult.getResultType() == Result.Type.FAILURE : indexResult.getResultType();</b>
<i>911</i>&nbsp;                } else {
<i>912</i>&nbsp;                    // generate or register sequence number
<b class="nc"><i>913</i>&nbsp;                    if (index.origin() == Operation.Origin.PRIMARY) {</b>
<b class="nc"><i>914</i>&nbsp;                        index = new Index(index.uid(), index.parsedDoc(), generateSeqNoForOperationOnPrimary(index), index.primaryTerm(),</b>
<b class="nc"><i>915</i>&nbsp;                            index.version(), index.versionType(), index.origin(), index.startTime(), index.getAutoGeneratedIdTimestamp(),</b>
<b class="nc"><i>916</i>&nbsp;                            index.isRetry(), index.getIfSeqNo(), index.getIfPrimaryTerm());</b>
<i>917</i>&nbsp;
<b class="nc"><i>918</i>&nbsp;                        final boolean toAppend = plan.indexIntoLucene &amp;&amp; plan.useLuceneUpdateDocument == false;</b>
<b class="nc"><i>919</i>&nbsp;                        if (toAppend == false) {</b>
<b class="nc"><i>920</i>&nbsp;                            advanceMaxSeqNoOfUpdatesOrDeletesOnPrimary(index.seqNo());</b>
<i>921</i>&nbsp;                        }
<b class="nc"><i>922</i>&nbsp;                    } else {</b>
<b class="nc"><i>923</i>&nbsp;                        markSeqNoAsSeen(index.seqNo());</b>
<i>924</i>&nbsp;                    }
<i>925</i>&nbsp;
<b class="nc"><i>926</i>&nbsp;                    assert index.seqNo() &gt;= 0 : &quot;ops should have an assigned seq no.; origin: &quot; + index.origin();</b>
<i>927</i>&nbsp;
<b class="nc"><i>928</i>&nbsp;                    if (plan.indexIntoLucene || plan.addStaleOpToLucene) {</b>
<b class="nc"><i>929</i>&nbsp;                        indexResult = indexIntoLucene(index, plan);</b>
<i>930</i>&nbsp;                    } else {
<b class="nc"><i>931</i>&nbsp;                        indexResult = new IndexResult(</b>
<b class="nc"><i>932</i>&nbsp;                            plan.versionForIndexing, index.primaryTerm(), index.seqNo(), plan.currentNotFoundOrDeleted);</b>
<i>933</i>&nbsp;                    }
<i>934</i>&nbsp;                }
<b class="nc"><i>935</i>&nbsp;                if (index.origin().isFromTranslog() == false) {</b>
<i>936</i>&nbsp;                    final Translog.Location location;
<b class="nc"><i>937</i>&nbsp;                    if (indexResult.getResultType() == Result.Type.SUCCESS) {</b>
<b class="nc"><i>938</i>&nbsp;                        location = translog.add(new Translog.Index(index, indexResult));</b>
<b class="nc"><i>939</i>&nbsp;                    } else if (indexResult.getSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO) {</b>
<i>940</i>&nbsp;                        // if we have document failure, record it as a no-op in the translog and Lucene with the generated seq_no
<b class="nc"><i>941</i>&nbsp;                        final NoOp noOp = new NoOp(indexResult.getSeqNo(), index.primaryTerm(), index.origin(),</b>
<b class="nc"><i>942</i>&nbsp;                            index.startTime(), indexResult.getFailure().toString());</b>
<b class="nc"><i>943</i>&nbsp;                        location = innerNoOp(noOp).getTranslogLocation();</b>
<b class="nc"><i>944</i>&nbsp;                    } else {</b>
<b class="nc"><i>945</i>&nbsp;                        location = null;</b>
<i>946</i>&nbsp;                    }
<b class="nc"><i>947</i>&nbsp;                    indexResult.setTranslogLocation(location);</b>
<i>948</i>&nbsp;                }
<b class="nc"><i>949</i>&nbsp;                if (plan.indexIntoLucene &amp;&amp; indexResult.getResultType() == Result.Type.SUCCESS) {</b>
<b class="nc"><i>950</i>&nbsp;                    final Translog.Location translogLocation = trackTranslogLocation.get() ? indexResult.getTranslogLocation() : null;</b>
<b class="nc"><i>951</i>&nbsp;                    versionMap.maybePutIndexUnderLock(index.uid().bytes(),</b>
<b class="nc"><i>952</i>&nbsp;                        new IndexVersionValue(translogLocation, plan.versionForIndexing, index.seqNo(), index.primaryTerm()));</b>
<i>953</i>&nbsp;                }
<b class="nc"><i>954</i>&nbsp;                localCheckpointTracker.markSeqNoAsProcessed(indexResult.getSeqNo());</b>
<b class="nc"><i>955</i>&nbsp;                if (indexResult.getTranslogLocation() == null) {</b>
<i>956</i>&nbsp;                    // the op is coming from the translog (and is hence persisted already) or it does not have a sequence number
<b class="nc"><i>957</i>&nbsp;                    assert index.origin().isFromTranslog() || indexResult.getSeqNo() == SequenceNumbers.UNASSIGNED_SEQ_NO;</b>
<b class="nc"><i>958</i>&nbsp;                    localCheckpointTracker.markSeqNoAsPersisted(indexResult.getSeqNo());</b>
<i>959</i>&nbsp;                }
<b class="nc"><i>960</i>&nbsp;                indexResult.setTook(System.nanoTime() - index.startTime());</b>
<b class="nc"><i>961</i>&nbsp;                indexResult.freeze();</b>
<b class="nc"><i>962</i>&nbsp;                return indexResult;</b>
<b class="nc"><i>963</i>&nbsp;            }</b>
<b class="nc"><i>964</i>&nbsp;        } catch (RuntimeException | IOException e) {</b>
<i>965</i>&nbsp;            try {
<b class="nc"><i>966</i>&nbsp;                if (e instanceof AlreadyClosedException == false &amp;&amp; treatDocumentFailureAsTragicError(index)) {</b>
<b class="nc"><i>967</i>&nbsp;                    failEngine(&quot;index id[&quot; + index.id() + &quot;] origin[&quot; + index.origin() + &quot;] seq#[&quot; + index.seqNo() + &quot;]&quot;, e);</b>
<i>968</i>&nbsp;                } else {
<b class="nc"><i>969</i>&nbsp;                    maybeFailEngine(&quot;index id[&quot; + index.id() + &quot;] origin[&quot; + index.origin() + &quot;] seq#[&quot; + index.seqNo() + &quot;]&quot;, e);</b>
<i>970</i>&nbsp;                }
<b class="nc"><i>971</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>972</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>973</i>&nbsp;            }</b>
<b class="nc"><i>974</i>&nbsp;            throw e;</b>
<i>975</i>&nbsp;        }
<i>976</i>&nbsp;    }
<i>977</i>&nbsp;
<i>978</i>&nbsp;    protected final IndexingStrategy planIndexingAsNonPrimary(Index index) throws IOException {
<b class="nc"><i>979</i>&nbsp;        assert assertNonPrimaryOrigin(index);</b>
<i>980</i>&nbsp;        // needs to maintain the auto_id timestamp in case this replica becomes primary
<b class="nc"><i>981</i>&nbsp;        if (canOptimizeAddDocument(index)) {</b>
<b class="nc"><i>982</i>&nbsp;            mayHaveBeenIndexedBefore(index);</b>
<i>983</i>&nbsp;        }
<i>984</i>&nbsp;        final IndexingStrategy plan;
<i>985</i>&nbsp;        // unlike the primary, replicas don&#39;t really care to about creation status of documents
<i>986</i>&nbsp;        // this allows to ignore the case where a document was found in the live version maps in
<i>987</i>&nbsp;        // a delete state and return false for the created flag in favor of code simplicity
<b class="nc"><i>988</i>&nbsp;        final long maxSeqNoOfUpdatesOrDeletes = getMaxSeqNoOfUpdatesOrDeletes();</b>
<b class="nc"><i>989</i>&nbsp;        if (hasBeenProcessedBefore(index)) {</b>
<i>990</i>&nbsp;            // the operation seq# was processed and thus the same operation was already put into lucene
<i>991</i>&nbsp;            // this can happen during recovery where older operations are sent from the translog that are already
<i>992</i>&nbsp;            // part of the lucene commit (either from a peer recovery or a local translog)
<i>993</i>&nbsp;            // or due to concurrent indexing &amp; recovery. For the former it is important to skip lucene as the operation in
<i>994</i>&nbsp;            // question may have been deleted in an out of order op that is not replayed.
<i>995</i>&nbsp;            // See testRecoverFromStoreWithOutOfOrderDelete for an example of local recovery
<i>996</i>&nbsp;            // See testRecoveryWithOutOfOrderDelete for an example of peer recovery
<b class="nc"><i>997</i>&nbsp;            plan = IndexingStrategy.processButSkipLucene(false, index.version());</b>
<b class="nc"><i>998</i>&nbsp;        } else if (maxSeqNoOfUpdatesOrDeletes &lt;= localCheckpointTracker.getProcessedCheckpoint()) {</b>
<i>999</i>&nbsp;            // see Engine#getMaxSeqNoOfUpdatesOrDeletes for the explanation of the optimization using sequence numbers
<b class="nc"><i>1000</i>&nbsp;            assert maxSeqNoOfUpdatesOrDeletes &lt; index.seqNo() : index.seqNo() + &quot;&gt;=&quot; + maxSeqNoOfUpdatesOrDeletes;</b>
<b class="nc"><i>1001</i>&nbsp;            plan = IndexingStrategy.optimizedAppendOnly(index.version());</b>
<i>1002</i>&nbsp;        } else {
<b class="nc"><i>1003</i>&nbsp;            versionMap.enforceSafeAccess();</b>
<b class="nc"><i>1004</i>&nbsp;            final OpVsLuceneDocStatus opVsLucene = compareOpToLuceneDocBasedOnSeqNo(index);</b>
<b class="nc"><i>1005</i>&nbsp;            if (opVsLucene == OpVsLuceneDocStatus.OP_STALE_OR_EQUAL) {</b>
<b class="nc"><i>1006</i>&nbsp;                plan = IndexingStrategy.processAsStaleOp(softDeleteEnabled, index.version());</b>
<i>1007</i>&nbsp;            } else {
<b class="nc"><i>1008</i>&nbsp;                plan = IndexingStrategy.processNormally(opVsLucene == OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND, index.version());</b>
<i>1009</i>&nbsp;            }
<i>1010</i>&nbsp;        }
<b class="nc"><i>1011</i>&nbsp;        return plan;</b>
<i>1012</i>&nbsp;    }
<i>1013</i>&nbsp;
<i>1014</i>&nbsp;    protected IndexingStrategy indexingStrategyForOperation(final Index index) throws IOException {
<b class="nc"><i>1015</i>&nbsp;        if (index.origin() == Operation.Origin.PRIMARY) {</b>
<b class="nc"><i>1016</i>&nbsp;            return planIndexingAsPrimary(index);</b>
<i>1017</i>&nbsp;        } else {
<i>1018</i>&nbsp;            // non-primary mode (i.e., replica or recovery)
<b class="nc"><i>1019</i>&nbsp;            return planIndexingAsNonPrimary(index);</b>
<i>1020</i>&nbsp;        }
<i>1021</i>&nbsp;    }
<i>1022</i>&nbsp;
<i>1023</i>&nbsp;    private IndexingStrategy planIndexingAsPrimary(Index index) throws IOException {
<b class="nc"><i>1024</i>&nbsp;        assert index.origin() == Operation.Origin.PRIMARY : &quot;planing as primary but origin isn&#39;t. got &quot; + index.origin();</b>
<i>1025</i>&nbsp;        final IndexingStrategy plan;
<i>1026</i>&nbsp;        // resolve an external operation into an internal one which is safe to replay
<b class="nc"><i>1027</i>&nbsp;        final boolean canOptimizeAddDocument = canOptimizeAddDocument(index);</b>
<b class="nc"><i>1028</i>&nbsp;        if (canOptimizeAddDocument &amp;&amp; mayHaveBeenIndexedBefore(index) == false) {</b>
<b class="nc"><i>1029</i>&nbsp;            plan = IndexingStrategy.optimizedAppendOnly(1L);</b>
<i>1030</i>&nbsp;        } else {
<b class="nc"><i>1031</i>&nbsp;            versionMap.enforceSafeAccess();</b>
<i>1032</i>&nbsp;            // resolves incoming version
<b class="nc"><i>1033</i>&nbsp;            final VersionValue versionValue =</b>
<b class="nc"><i>1034</i>&nbsp;                resolveDocVersion(index, index.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO);</b>
<i>1035</i>&nbsp;            final long currentVersion;
<i>1036</i>&nbsp;            final boolean currentNotFoundOrDeleted;
<b class="nc"><i>1037</i>&nbsp;            if (versionValue == null) {</b>
<b class="nc"><i>1038</i>&nbsp;                currentVersion = Versions.NOT_FOUND;</b>
<b class="nc"><i>1039</i>&nbsp;                currentNotFoundOrDeleted = true;</b>
<i>1040</i>&nbsp;            } else {
<b class="nc"><i>1041</i>&nbsp;                currentVersion = versionValue.version;</b>
<b class="nc"><i>1042</i>&nbsp;                currentNotFoundOrDeleted = versionValue.isDelete();</b>
<i>1043</i>&nbsp;            }
<b class="nc"><i>1044</i>&nbsp;            if (index.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO &amp;&amp; versionValue == null) {</b>
<b class="nc"><i>1045</i>&nbsp;                final VersionConflictEngineException e = new VersionConflictEngineException(shardId, index.id(),</b>
<b class="nc"><i>1046</i>&nbsp;                    index.getIfSeqNo(), index.getIfPrimaryTerm(), SequenceNumbers.UNASSIGNED_SEQ_NO,</b>
<i>1047</i>&nbsp;                    SequenceNumbers.UNASSIGNED_PRIMARY_TERM);
<b class="nc"><i>1048</i>&nbsp;                plan = IndexingStrategy.skipDueToVersionConflict(e, true, currentVersion);</b>
<b class="nc"><i>1049</i>&nbsp;            } else if (index.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO &amp;&amp; (</b>
<b class="nc"><i>1050</i>&nbsp;                versionValue.seqNo != index.getIfSeqNo() || versionValue.term != index.getIfPrimaryTerm()</b>
<i>1051</i>&nbsp;            )) {
<b class="nc"><i>1052</i>&nbsp;                final VersionConflictEngineException e = new VersionConflictEngineException(shardId, index.id(),</b>
<b class="nc"><i>1053</i>&nbsp;                    index.getIfSeqNo(), index.getIfPrimaryTerm(), versionValue.seqNo, versionValue.term);</b>
<b class="nc"><i>1054</i>&nbsp;                plan = IndexingStrategy.skipDueToVersionConflict(e, currentNotFoundOrDeleted, currentVersion);</b>
<b class="nc"><i>1055</i>&nbsp;            } else if (index.versionType().isVersionConflictForWrites(</b>
<b class="nc"><i>1056</i>&nbsp;                currentVersion, index.version(), currentNotFoundOrDeleted)) {</b>
<b class="nc"><i>1057</i>&nbsp;                final VersionConflictEngineException e =</b>
<i>1058</i>&nbsp;                        new VersionConflictEngineException(shardId, index, currentVersion, currentNotFoundOrDeleted);
<b class="nc"><i>1059</i>&nbsp;                plan = IndexingStrategy.skipDueToVersionConflict(e, currentNotFoundOrDeleted, currentVersion);</b>
<b class="nc"><i>1060</i>&nbsp;            } else {</b>
<b class="nc"><i>1061</i>&nbsp;                plan = IndexingStrategy.processNormally(currentNotFoundOrDeleted,</b>
<b class="nc"><i>1062</i>&nbsp;                    canOptimizeAddDocument ? 1L : index.versionType().updateVersion(currentVersion, index.version())</b>
<i>1063</i>&nbsp;                );
<i>1064</i>&nbsp;            }
<i>1065</i>&nbsp;        }
<b class="nc"><i>1066</i>&nbsp;        return plan;</b>
<i>1067</i>&nbsp;    }
<i>1068</i>&nbsp;
<i>1069</i>&nbsp;    private IndexResult indexIntoLucene(Index index, IndexingStrategy plan)
<i>1070</i>&nbsp;        throws IOException {
<b class="nc"><i>1071</i>&nbsp;        assert index.seqNo() &gt;= 0 : &quot;ops should have an assigned seq no.; origin: &quot; + index.origin();</b>
<b class="nc"><i>1072</i>&nbsp;        assert plan.versionForIndexing &gt;= 0 : &quot;version must be set. got &quot; + plan.versionForIndexing;</b>
<b class="nc"><i>1073</i>&nbsp;        assert plan.indexIntoLucene || plan.addStaleOpToLucene;</b>
<i>1074</i>&nbsp;        /* Update the document&#39;s sequence number and primary term; the sequence number here is derived here from either the sequence
<i>1075</i>&nbsp;         * number service if this is on the primary, or the existing document&#39;s sequence number if this is on the replica. The
<i>1076</i>&nbsp;         * primary term here has already been set, see IndexShard#prepareIndex where the Engine$Index operation is created.
<i>1077</i>&nbsp;         */
<b class="nc"><i>1078</i>&nbsp;        index.parsedDoc().updateSeqID(index.seqNo(), index.primaryTerm());</b>
<b class="nc"><i>1079</i>&nbsp;        index.parsedDoc().version().setLongValue(plan.versionForIndexing);</b>
<i>1080</i>&nbsp;        try {
<b class="nc"><i>1081</i>&nbsp;            if (plan.addStaleOpToLucene) {</b>
<b class="nc"><i>1082</i>&nbsp;                addStaleDocs(index.docs(), indexWriter);</b>
<b class="nc"><i>1083</i>&nbsp;            } else if (plan.useLuceneUpdateDocument) {</b>
<b class="nc"><i>1084</i>&nbsp;                assert assertMaxSeqNoOfUpdatesIsAdvanced(index.uid(), index.seqNo(), true, true);</b>
<b class="nc"><i>1085</i>&nbsp;                updateDocs(index.uid(), index.docs(), indexWriter);</b>
<i>1086</i>&nbsp;            } else {
<i>1087</i>&nbsp;                // document does not exists, we can optimize for create, but double check if assertions are running
<b class="nc"><i>1088</i>&nbsp;                assert assertDocDoesNotExist(index, canOptimizeAddDocument(index) == false);</b>
<b class="nc"><i>1089</i>&nbsp;                addDocs(index.docs(), indexWriter);</b>
<i>1090</i>&nbsp;            }
<b class="nc"><i>1091</i>&nbsp;            return new IndexResult(plan.versionForIndexing, index.primaryTerm(), index.seqNo(), plan.currentNotFoundOrDeleted);</b>
<b class="nc"><i>1092</i>&nbsp;        } catch (Exception ex) {</b>
<b class="nc"><i>1093</i>&nbsp;            if (ex instanceof AlreadyClosedException == false &amp;&amp;</b>
<b class="nc"><i>1094</i>&nbsp;                indexWriter.getTragicException() == null &amp;&amp; treatDocumentFailureAsTragicError(index) == false) {</b>
<i>1095</i>&nbsp;                /* There is no tragic event recorded so this must be a document failure.
<i>1096</i>&nbsp;                 *
<i>1097</i>&nbsp;                 * The handling inside IW doesn&#39;t guarantee that an tragic / aborting exception
<i>1098</i>&nbsp;                 * will be used as THE tragicEventException since if there are multiple exceptions causing an abort in IW
<i>1099</i>&nbsp;                 * only one wins. Yet, only the one that wins will also close the IW and in turn fail the engine such that
<i>1100</i>&nbsp;                 * we can potentially handle the exception before the engine is failed.
<i>1101</i>&nbsp;                 * Bottom line is that we can only rely on the fact that if it&#39;s a document failure then
<i>1102</i>&nbsp;                 * `indexWriter.getTragicException()` will be null otherwise we have to rethrow and treat it as fatal or rather
<i>1103</i>&nbsp;                 * non-document failure
<i>1104</i>&nbsp;                 *
<i>1105</i>&nbsp;                 * we return a `MATCH_ANY` version to indicate no document was index. The value is
<i>1106</i>&nbsp;                 * not used anyway
<i>1107</i>&nbsp;                 */
<b class="nc"><i>1108</i>&nbsp;                return new IndexResult(ex, Versions.MATCH_ANY, index.primaryTerm(), index.seqNo());</b>
<i>1109</i>&nbsp;            } else {
<b class="nc"><i>1110</i>&nbsp;                throw ex;</b>
<i>1111</i>&nbsp;            }
<i>1112</i>&nbsp;        }
<i>1113</i>&nbsp;    }
<i>1114</i>&nbsp;
<i>1115</i>&nbsp;    /**
<i>1116</i>&nbsp;     * Whether we should treat any document failure as tragic error.
<i>1117</i>&nbsp;     * If we hit any failure while processing an indexing on a replica, we should treat that error as tragic and fail the engine.
<i>1118</i>&nbsp;     * However, we prefer to fail a request individually (instead of a shard) if we hit a document failure on the primary.
<i>1119</i>&nbsp;     */
<i>1120</i>&nbsp;    private boolean treatDocumentFailureAsTragicError(Index index) {
<i>1121</i>&nbsp;        // TODO: can we enable this check for all origins except primary on the leader?
<b class="nc"><i>1122</i>&nbsp;        return index.origin() == Operation.Origin.REPLICA</b>
<b class="nc"><i>1123</i>&nbsp;            || index.origin() == Operation.Origin.PEER_RECOVERY</b>
<b class="nc"><i>1124</i>&nbsp;            || index.origin() == Operation.Origin.LOCAL_RESET;</b>
<i>1125</i>&nbsp;    }
<i>1126</i>&nbsp;
<i>1127</i>&nbsp;    /**
<i>1128</i>&nbsp;     * returns true if the indexing operation may have already be processed by this engine.
<i>1129</i>&nbsp;     * Note that it is OK to rarely return true even if this is not the case. However a `false`
<i>1130</i>&nbsp;     * return value must always be correct.
<i>1131</i>&nbsp;     *
<i>1132</i>&nbsp;     */
<i>1133</i>&nbsp;    private boolean mayHaveBeenIndexedBefore(Index index) {
<b class="nc"><i>1134</i>&nbsp;        assert canOptimizeAddDocument(index);</b>
<i>1135</i>&nbsp;        final boolean mayHaveBeenIndexBefore;
<b class="nc"><i>1136</i>&nbsp;        if (index.isRetry()) {</b>
<b class="nc"><i>1137</i>&nbsp;            mayHaveBeenIndexBefore = true;</b>
<b class="nc"><i>1138</i>&nbsp;            updateAutoIdTimestamp(index.getAutoGeneratedIdTimestamp(), true);</b>
<b class="nc"><i>1139</i>&nbsp;            assert maxUnsafeAutoIdTimestamp.get() &gt;= index.getAutoGeneratedIdTimestamp();</b>
<i>1140</i>&nbsp;        } else {
<i>1141</i>&nbsp;            // in this case we force
<b class="nc"><i>1142</i>&nbsp;            mayHaveBeenIndexBefore = maxUnsafeAutoIdTimestamp.get() &gt;= index.getAutoGeneratedIdTimestamp();</b>
<b class="nc"><i>1143</i>&nbsp;            updateAutoIdTimestamp(index.getAutoGeneratedIdTimestamp(), false);</b>
<i>1144</i>&nbsp;        }
<b class="nc"><i>1145</i>&nbsp;        return mayHaveBeenIndexBefore;</b>
<i>1146</i>&nbsp;    }
<i>1147</i>&nbsp;
<i>1148</i>&nbsp;    private void addDocs(final List&lt;ParseContext.Document&gt; docs, final IndexWriter indexWriter) throws IOException {
<b class="nc"><i>1149</i>&nbsp;        if (docs.size() &gt; 1) {</b>
<b class="nc"><i>1150</i>&nbsp;            indexWriter.addDocuments(docs);</b>
<i>1151</i>&nbsp;        } else {
<b class="nc"><i>1152</i>&nbsp;            indexWriter.addDocument(docs.get(0));</b>
<i>1153</i>&nbsp;        }
<b class="nc"><i>1154</i>&nbsp;        numDocAppends.inc(docs.size());</b>
<b class="nc"><i>1155</i>&nbsp;    }</b>
<i>1156</i>&nbsp;
<i>1157</i>&nbsp;    private void addStaleDocs(final List&lt;ParseContext.Document&gt; docs, final IndexWriter indexWriter) throws IOException {
<b class="nc"><i>1158</i>&nbsp;        assert softDeleteEnabled : &quot;Add history documents but soft-deletes is disabled&quot;;</b>
<b class="nc"><i>1159</i>&nbsp;        for (ParseContext.Document doc : docs) {</b>
<b class="nc"><i>1160</i>&nbsp;            doc.add(softDeletesField); // soft-deleted every document before adding to Lucene</b>
<b class="nc"><i>1161</i>&nbsp;        }</b>
<b class="nc"><i>1162</i>&nbsp;        if (docs.size() &gt; 1) {</b>
<b class="nc"><i>1163</i>&nbsp;            indexWriter.addDocuments(docs);</b>
<i>1164</i>&nbsp;        } else {
<b class="nc"><i>1165</i>&nbsp;            indexWriter.addDocument(docs.get(0));</b>
<i>1166</i>&nbsp;        }
<b class="nc"><i>1167</i>&nbsp;    }</b>
<i>1168</i>&nbsp;
<i>1169</i>&nbsp;    protected static final class IndexingStrategy {
<i>1170</i>&nbsp;        final boolean currentNotFoundOrDeleted;
<i>1171</i>&nbsp;        final boolean useLuceneUpdateDocument;
<i>1172</i>&nbsp;        final long versionForIndexing;
<i>1173</i>&nbsp;        final boolean indexIntoLucene;
<i>1174</i>&nbsp;        final boolean addStaleOpToLucene;
<i>1175</i>&nbsp;        final Optional&lt;IndexResult&gt; earlyResultOnPreFlightError;
<i>1176</i>&nbsp;
<i>1177</i>&nbsp;        private IndexingStrategy(boolean currentNotFoundOrDeleted, boolean useLuceneUpdateDocument,
<i>1178</i>&nbsp;                                 boolean indexIntoLucene, boolean addStaleOpToLucene,
<i>1179</i>&nbsp;                                 long versionForIndexing, IndexResult earlyResultOnPreFlightError) {
<i>1180</i>&nbsp;            assert useLuceneUpdateDocument == false || indexIntoLucene :
<i>1181</i>&nbsp;                &quot;use lucene update is set to true, but we&#39;re not indexing into lucene&quot;;
<i>1182</i>&nbsp;            assert (indexIntoLucene &amp;&amp; earlyResultOnPreFlightError != null) == false :
<i>1183</i>&nbsp;                &quot;can only index into lucene or have a preflight result but not both.&quot; +
<i>1184</i>&nbsp;                    &quot;indexIntoLucene: &quot; + indexIntoLucene
<i>1185</i>&nbsp;                    + &quot;  earlyResultOnPreFlightError:&quot; + earlyResultOnPreFlightError;
<i>1186</i>&nbsp;            this.currentNotFoundOrDeleted = currentNotFoundOrDeleted;
<i>1187</i>&nbsp;            this.useLuceneUpdateDocument = useLuceneUpdateDocument;
<i>1188</i>&nbsp;            this.versionForIndexing = versionForIndexing;
<i>1189</i>&nbsp;            this.indexIntoLucene = indexIntoLucene;
<i>1190</i>&nbsp;            this.addStaleOpToLucene = addStaleOpToLucene;
<i>1191</i>&nbsp;            this.earlyResultOnPreFlightError =
<i>1192</i>&nbsp;                earlyResultOnPreFlightError == null ? Optional.empty() :
<i>1193</i>&nbsp;                    Optional.of(earlyResultOnPreFlightError);
<i>1194</i>&nbsp;        }
<i>1195</i>&nbsp;
<i>1196</i>&nbsp;        static IndexingStrategy optimizedAppendOnly(long versionForIndexing) {
<i>1197</i>&nbsp;            return new IndexingStrategy(true, false, true, false, versionForIndexing, null);
<i>1198</i>&nbsp;        }
<i>1199</i>&nbsp;
<i>1200</i>&nbsp;        public static IndexingStrategy skipDueToVersionConflict(
<i>1201</i>&nbsp;                VersionConflictEngineException e, boolean currentNotFoundOrDeleted, long currentVersion) {
<i>1202</i>&nbsp;            final IndexResult result = new IndexResult(e, currentVersion);
<i>1203</i>&nbsp;            return new IndexingStrategy(
<i>1204</i>&nbsp;                    currentNotFoundOrDeleted, false, false, false,
<i>1205</i>&nbsp;                Versions.NOT_FOUND, result);
<i>1206</i>&nbsp;        }
<i>1207</i>&nbsp;
<i>1208</i>&nbsp;        static IndexingStrategy processNormally(boolean currentNotFoundOrDeleted,
<i>1209</i>&nbsp;                                                long versionForIndexing) {
<i>1210</i>&nbsp;            return new IndexingStrategy(currentNotFoundOrDeleted, currentNotFoundOrDeleted == false,
<i>1211</i>&nbsp;                true, false, versionForIndexing, null);
<i>1212</i>&nbsp;        }
<i>1213</i>&nbsp;
<i>1214</i>&nbsp;        public static IndexingStrategy processButSkipLucene(boolean currentNotFoundOrDeleted, long versionForIndexing) {
<i>1215</i>&nbsp;            return new IndexingStrategy(currentNotFoundOrDeleted, false, false,
<i>1216</i>&nbsp;                false, versionForIndexing, null);
<i>1217</i>&nbsp;        }
<i>1218</i>&nbsp;
<i>1219</i>&nbsp;        static IndexingStrategy processAsStaleOp(boolean addStaleOpToLucene, long versionForIndexing) {
<i>1220</i>&nbsp;            return new IndexingStrategy(false, false, false,
<i>1221</i>&nbsp;                addStaleOpToLucene, versionForIndexing, null);
<i>1222</i>&nbsp;        }
<i>1223</i>&nbsp;    }
<i>1224</i>&nbsp;
<i>1225</i>&nbsp;    /**
<i>1226</i>&nbsp;     * Asserts that the doc in the index operation really doesn&#39;t exist
<i>1227</i>&nbsp;     */
<i>1228</i>&nbsp;    private boolean assertDocDoesNotExist(final Index index, final boolean allowDeleted) throws IOException {
<i>1229</i>&nbsp;        // NOTE this uses direct access to the version map since we are in the assertion code where we maintain a secondary
<i>1230</i>&nbsp;        // map in the version map such that we don&#39;t need to refresh if we are unsafe;
<b class="nc"><i>1231</i>&nbsp;        final VersionValue versionValue = versionMap.getVersionForAssert(index.uid().bytes());</b>
<b class="nc"><i>1232</i>&nbsp;        if (versionValue != null) {</b>
<b class="nc"><i>1233</i>&nbsp;            if (versionValue.isDelete() == false || allowDeleted == false) {</b>
<b class="nc"><i>1234</i>&nbsp;                throw new AssertionError(&quot;doc [&quot; + index.type() + &quot;][&quot; + index.id() + &quot;] exists in version map (version &quot; +</b>
<i>1235</i>&nbsp;                    versionValue + &quot;)&quot;);
<i>1236</i>&nbsp;            }
<i>1237</i>&nbsp;        } else {
<b class="nc"><i>1238</i>&nbsp;            try (Searcher searcher = acquireSearcher(&quot;assert doc doesn&#39;t exist&quot;, SearcherScope.INTERNAL)) {</b>
<b class="nc"><i>1239</i>&nbsp;                final long docsWithId = searcher.count(new TermQuery(index.uid()));</b>
<b class="nc"><i>1240</i>&nbsp;                if (docsWithId &gt; 0) {</b>
<b class="nc"><i>1241</i>&nbsp;                    throw new AssertionError(&quot;doc [&quot; + index.type() + &quot;][&quot; + index.id() + &quot;] exists [&quot; + docsWithId +</b>
<i>1242</i>&nbsp;                        &quot;] times in index&quot;);
<i>1243</i>&nbsp;                }
<b class="nc"><i>1244</i>&nbsp;            }</b>
<i>1245</i>&nbsp;        }
<b class="nc"><i>1246</i>&nbsp;        return true;</b>
<i>1247</i>&nbsp;    }
<i>1248</i>&nbsp;
<i>1249</i>&nbsp;    private void updateDocs(final Term uid, final List&lt;ParseContext.Document&gt; docs, final IndexWriter indexWriter) throws IOException {
<b class="nc"><i>1250</i>&nbsp;        if (softDeleteEnabled) {</b>
<b class="nc"><i>1251</i>&nbsp;            if (docs.size() &gt; 1) {</b>
<b class="nc"><i>1252</i>&nbsp;                indexWriter.softUpdateDocuments(uid, docs, softDeletesField);</b>
<i>1253</i>&nbsp;            } else {
<b class="nc"><i>1254</i>&nbsp;                indexWriter.softUpdateDocument(uid, docs.get(0), softDeletesField);</b>
<i>1255</i>&nbsp;            }
<i>1256</i>&nbsp;        } else {
<b class="nc"><i>1257</i>&nbsp;            if (docs.size() &gt; 1) {</b>
<b class="nc"><i>1258</i>&nbsp;                indexWriter.updateDocuments(uid, docs);</b>
<i>1259</i>&nbsp;            } else {
<b class="nc"><i>1260</i>&nbsp;                indexWriter.updateDocument(uid, docs.get(0));</b>
<i>1261</i>&nbsp;            }
<i>1262</i>&nbsp;        }
<b class="nc"><i>1263</i>&nbsp;        numDocUpdates.inc(docs.size());</b>
<b class="nc"><i>1264</i>&nbsp;    }</b>
<i>1265</i>&nbsp;
<i>1266</i>&nbsp;    @Override
<i>1267</i>&nbsp;    public DeleteResult delete(Delete delete) throws IOException {
<b class="nc"><i>1268</i>&nbsp;        versionMap.enforceSafeAccess();</b>
<b class="nc"><i>1269</i>&nbsp;        assert Objects.equals(delete.uid().field(), IdFieldMapper.NAME) : delete.uid().field();</b>
<b class="nc"><i>1270</i>&nbsp;        assert assertIncomingSequenceNumber(delete.origin(), delete.seqNo());</b>
<i>1271</i>&nbsp;        final DeleteResult deleteResult;
<i>1272</i>&nbsp;        // NOTE: we don&#39;t throttle this when merges fall behind because delete-by-id does not create new segments:
<b class="nc"><i>1273</i>&nbsp;        try (ReleasableLock ignored = readLock.acquire(); Releasable ignored2 = versionMap.acquireLock(delete.uid().bytes())) {</b>
<b class="nc"><i>1274</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1275</i>&nbsp;            lastWriteNanos = delete.startTime();</b>
<b class="nc"><i>1276</i>&nbsp;            final DeletionStrategy plan = deletionStrategyForOperation(delete);</b>
<i>1277</i>&nbsp;
<b class="nc"><i>1278</i>&nbsp;            if (plan.earlyResultOnPreflightError.isPresent()) {</b>
<b class="nc"><i>1279</i>&nbsp;                deleteResult = plan.earlyResultOnPreflightError.get();</b>
<i>1280</i>&nbsp;            } else {
<i>1281</i>&nbsp;                // generate or register sequence number
<b class="nc"><i>1282</i>&nbsp;                if (delete.origin() == Operation.Origin.PRIMARY) {</b>
<b class="nc"><i>1283</i>&nbsp;                    delete = new Delete(delete.type(), delete.id(), delete.uid(), generateSeqNoForOperationOnPrimary(delete),</b>
<b class="nc"><i>1284</i>&nbsp;                        delete.primaryTerm(), delete.version(), delete.versionType(), delete.origin(), delete.startTime(),</b>
<b class="nc"><i>1285</i>&nbsp;                        delete.getIfSeqNo(), delete.getIfPrimaryTerm());</b>
<i>1286</i>&nbsp;
<b class="nc"><i>1287</i>&nbsp;                    advanceMaxSeqNoOfUpdatesOrDeletesOnPrimary(delete.seqNo());</b>
<i>1288</i>&nbsp;                } else {
<b class="nc"><i>1289</i>&nbsp;                    markSeqNoAsSeen(delete.seqNo());</b>
<i>1290</i>&nbsp;                }
<i>1291</i>&nbsp;
<b class="nc"><i>1292</i>&nbsp;                assert delete.seqNo() &gt;= 0 : &quot;ops should have an assigned seq no.; origin: &quot; + delete.origin();</b>
<i>1293</i>&nbsp;
<b class="nc"><i>1294</i>&nbsp;                if (plan.deleteFromLucene || plan.addStaleOpToLucene) {</b>
<b class="nc"><i>1295</i>&nbsp;                    deleteResult = deleteInLucene(delete, plan);</b>
<i>1296</i>&nbsp;                } else {
<b class="nc"><i>1297</i>&nbsp;                    deleteResult = new DeleteResult(</b>
<b class="nc"><i>1298</i>&nbsp;                        plan.versionOfDeletion, delete.primaryTerm(), delete.seqNo(), plan.currentlyDeleted == false);</b>
<i>1299</i>&nbsp;                }
<i>1300</i>&nbsp;            }
<b class="nc"><i>1301</i>&nbsp;            if (delete.origin().isFromTranslog() == false &amp;&amp; deleteResult.getResultType() == Result.Type.SUCCESS) {</b>
<b class="nc"><i>1302</i>&nbsp;                final Translog.Location location = translog.add(new Translog.Delete(delete, deleteResult));</b>
<b class="nc"><i>1303</i>&nbsp;                deleteResult.setTranslogLocation(location);</b>
<i>1304</i>&nbsp;            }
<b class="nc"><i>1305</i>&nbsp;            localCheckpointTracker.markSeqNoAsProcessed(deleteResult.getSeqNo());</b>
<b class="nc"><i>1306</i>&nbsp;            if (deleteResult.getTranslogLocation() == null) {</b>
<i>1307</i>&nbsp;                // the op is coming from the translog (and is hence persisted already) or does not have a sequence number (version conflict)
<b class="nc"><i>1308</i>&nbsp;                assert delete.origin().isFromTranslog() || deleteResult.getSeqNo() == SequenceNumbers.UNASSIGNED_SEQ_NO;</b>
<b class="nc"><i>1309</i>&nbsp;                localCheckpointTracker.markSeqNoAsPersisted(deleteResult.getSeqNo());</b>
<i>1310</i>&nbsp;            }
<b class="nc"><i>1311</i>&nbsp;            deleteResult.setTook(System.nanoTime() - delete.startTime());</b>
<b class="nc"><i>1312</i>&nbsp;            deleteResult.freeze();</b>
<b class="nc"><i>1313</i>&nbsp;        } catch (RuntimeException | IOException e) {</b>
<i>1314</i>&nbsp;            try {
<b class="nc"><i>1315</i>&nbsp;                maybeFailEngine(&quot;delete&quot;, e);</b>
<b class="nc"><i>1316</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1317</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1318</i>&nbsp;            }</b>
<b class="nc"><i>1319</i>&nbsp;            throw e;</b>
<b class="nc"><i>1320</i>&nbsp;        }</b>
<b class="nc"><i>1321</i>&nbsp;        maybePruneDeletes();</b>
<b class="nc"><i>1322</i>&nbsp;        return deleteResult;</b>
<i>1323</i>&nbsp;    }
<i>1324</i>&nbsp;
<i>1325</i>&nbsp;    protected DeletionStrategy deletionStrategyForOperation(final Delete delete) throws IOException {
<b class="nc"><i>1326</i>&nbsp;        if (delete.origin() == Operation.Origin.PRIMARY) {</b>
<b class="nc"><i>1327</i>&nbsp;            return planDeletionAsPrimary(delete);</b>
<i>1328</i>&nbsp;        } else {
<i>1329</i>&nbsp;            // non-primary mode (i.e., replica or recovery)
<b class="nc"><i>1330</i>&nbsp;            return planDeletionAsNonPrimary(delete);</b>
<i>1331</i>&nbsp;        }
<i>1332</i>&nbsp;    }
<i>1333</i>&nbsp;
<i>1334</i>&nbsp;    protected final DeletionStrategy planDeletionAsNonPrimary(Delete delete) throws IOException {
<b class="nc"><i>1335</i>&nbsp;        assert assertNonPrimaryOrigin(delete);</b>
<i>1336</i>&nbsp;        final DeletionStrategy plan;
<b class="nc"><i>1337</i>&nbsp;        if (hasBeenProcessedBefore(delete)) {</b>
<i>1338</i>&nbsp;            // the operation seq# was processed thus this operation was already put into lucene
<i>1339</i>&nbsp;            // this can happen during recovery where older operations are sent from the translog that are already
<i>1340</i>&nbsp;            // part of the lucene commit (either from a peer recovery or a local translog)
<i>1341</i>&nbsp;            // or due to concurrent indexing &amp; recovery. For the former it is important to skip lucene as the operation in
<i>1342</i>&nbsp;            // question may have been deleted in an out of order op that is not replayed.
<i>1343</i>&nbsp;            // See testRecoverFromStoreWithOutOfOrderDelete for an example of local recovery
<i>1344</i>&nbsp;            // See testRecoveryWithOutOfOrderDelete for an example of peer recovery
<b class="nc"><i>1345</i>&nbsp;            plan = DeletionStrategy.processButSkipLucene(false, delete.version());</b>
<i>1346</i>&nbsp;        } else {
<b class="nc"><i>1347</i>&nbsp;            final OpVsLuceneDocStatus opVsLucene = compareOpToLuceneDocBasedOnSeqNo(delete);</b>
<b class="nc"><i>1348</i>&nbsp;            if (opVsLucene == OpVsLuceneDocStatus.OP_STALE_OR_EQUAL) {</b>
<b class="nc"><i>1349</i>&nbsp;                plan = DeletionStrategy.processAsStaleOp(softDeleteEnabled, delete.version());</b>
<i>1350</i>&nbsp;            } else {
<b class="nc"><i>1351</i>&nbsp;                plan = DeletionStrategy.processNormally(opVsLucene == OpVsLuceneDocStatus.LUCENE_DOC_NOT_FOUND, delete.version());</b>
<i>1352</i>&nbsp;            }
<i>1353</i>&nbsp;        }
<b class="nc"><i>1354</i>&nbsp;        return plan;</b>
<i>1355</i>&nbsp;    }
<i>1356</i>&nbsp;
<i>1357</i>&nbsp;    protected boolean assertNonPrimaryOrigin(final Operation operation) {
<b class="nc"><i>1358</i>&nbsp;        assert operation.origin() != Operation.Origin.PRIMARY : &quot;planing as primary but got &quot; + operation.origin();</b>
<b class="nc"><i>1359</i>&nbsp;        return true;</b>
<i>1360</i>&nbsp;    }
<i>1361</i>&nbsp;
<i>1362</i>&nbsp;    private DeletionStrategy planDeletionAsPrimary(Delete delete) throws IOException {
<b class="nc"><i>1363</i>&nbsp;        assert delete.origin() == Operation.Origin.PRIMARY : &quot;planing as primary but got &quot; + delete.origin();</b>
<i>1364</i>&nbsp;        // resolve operation from external to internal
<b class="nc"><i>1365</i>&nbsp;        final VersionValue versionValue = resolveDocVersion(delete, delete.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO);</b>
<b class="nc"><i>1366</i>&nbsp;        assert incrementVersionLookup();</b>
<i>1367</i>&nbsp;        final long currentVersion;
<i>1368</i>&nbsp;        final boolean currentlyDeleted;
<b class="nc"><i>1369</i>&nbsp;        if (versionValue == null) {</b>
<b class="nc"><i>1370</i>&nbsp;            currentVersion = Versions.NOT_FOUND;</b>
<b class="nc"><i>1371</i>&nbsp;            currentlyDeleted = true;</b>
<i>1372</i>&nbsp;        } else {
<b class="nc"><i>1373</i>&nbsp;            currentVersion = versionValue.version;</b>
<b class="nc"><i>1374</i>&nbsp;            currentlyDeleted = versionValue.isDelete();</b>
<i>1375</i>&nbsp;        }
<i>1376</i>&nbsp;        final DeletionStrategy plan;
<b class="nc"><i>1377</i>&nbsp;        if (delete.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO &amp;&amp; versionValue == null) {</b>
<b class="nc"><i>1378</i>&nbsp;            final VersionConflictEngineException e = new VersionConflictEngineException(shardId, delete.id(),</b>
<b class="nc"><i>1379</i>&nbsp;                delete.getIfSeqNo(), delete.getIfPrimaryTerm(), SequenceNumbers.UNASSIGNED_SEQ_NO, SequenceNumbers.UNASSIGNED_PRIMARY_TERM);</b>
<b class="nc"><i>1380</i>&nbsp;            plan = DeletionStrategy.skipDueToVersionConflict(e, currentVersion, true);</b>
<b class="nc"><i>1381</i>&nbsp;        } else if (delete.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO &amp;&amp; (</b>
<b class="nc"><i>1382</i>&nbsp;            versionValue.seqNo != delete.getIfSeqNo() || versionValue.term != delete.getIfPrimaryTerm()</b>
<i>1383</i>&nbsp;        )) {
<b class="nc"><i>1384</i>&nbsp;            final VersionConflictEngineException e = new VersionConflictEngineException(shardId, delete.id(),</b>
<b class="nc"><i>1385</i>&nbsp;                delete.getIfSeqNo(), delete.getIfPrimaryTerm(), versionValue.seqNo, versionValue.term);</b>
<b class="nc"><i>1386</i>&nbsp;            plan = DeletionStrategy.skipDueToVersionConflict(e, currentVersion, currentlyDeleted);</b>
<b class="nc"><i>1387</i>&nbsp;        } else if (delete.versionType().isVersionConflictForWrites(currentVersion, delete.version(), currentlyDeleted)) {</b>
<b class="nc"><i>1388</i>&nbsp;            final VersionConflictEngineException e = new VersionConflictEngineException(shardId, delete, currentVersion, currentlyDeleted);</b>
<b class="nc"><i>1389</i>&nbsp;            plan = DeletionStrategy.skipDueToVersionConflict(e, currentVersion, currentlyDeleted);</b>
<b class="nc"><i>1390</i>&nbsp;        } else {</b>
<b class="nc"><i>1391</i>&nbsp;            plan = DeletionStrategy.processNormally(currentlyDeleted, delete.versionType().updateVersion(currentVersion, delete.version()));</b>
<i>1392</i>&nbsp;        }
<b class="nc"><i>1393</i>&nbsp;        return plan;</b>
<i>1394</i>&nbsp;    }
<i>1395</i>&nbsp;
<i>1396</i>&nbsp;    private DeleteResult deleteInLucene(Delete delete, DeletionStrategy plan) throws IOException {
<b class="nc"><i>1397</i>&nbsp;        assert assertMaxSeqNoOfUpdatesIsAdvanced(delete.uid(), delete.seqNo(), false, false);</b>
<i>1398</i>&nbsp;        try {
<b class="nc"><i>1399</i>&nbsp;            if (softDeleteEnabled) {</b>
<b class="nc"><i>1400</i>&nbsp;                final ParsedDocument tombstone = engineConfig.getTombstoneDocSupplier().newDeleteTombstoneDoc(delete.type(), delete.id());</b>
<b class="nc"><i>1401</i>&nbsp;                assert tombstone.docs().size() == 1 : &quot;Tombstone doc should have single doc [&quot; + tombstone + &quot;]&quot;;</b>
<b class="nc"><i>1402</i>&nbsp;                tombstone.updateSeqID(delete.seqNo(), delete.primaryTerm());</b>
<b class="nc"><i>1403</i>&nbsp;                tombstone.version().setLongValue(plan.versionOfDeletion);</b>
<b class="nc"><i>1404</i>&nbsp;                final ParseContext.Document doc = tombstone.docs().get(0);</b>
<b class="nc"><i>1405</i>&nbsp;                assert doc.getField(SeqNoFieldMapper.TOMBSTONE_NAME) != null :</b>
<i>1406</i>&nbsp;                    &quot;Delete tombstone document but _tombstone field is not set [&quot; + doc + &quot; ]&quot;;
<b class="nc"><i>1407</i>&nbsp;                doc.add(softDeletesField);</b>
<b class="nc"><i>1408</i>&nbsp;                if (plan.addStaleOpToLucene || plan.currentlyDeleted) {</b>
<b class="nc"><i>1409</i>&nbsp;                    indexWriter.addDocument(doc);</b>
<i>1410</i>&nbsp;                } else {
<b class="nc"><i>1411</i>&nbsp;                    indexWriter.softUpdateDocument(delete.uid(), doc, softDeletesField);</b>
<i>1412</i>&nbsp;                }
<b class="nc"><i>1413</i>&nbsp;            } else if (plan.currentlyDeleted == false) {</b>
<i>1414</i>&nbsp;                // any exception that comes from this is a either an ACE or a fatal exception there
<i>1415</i>&nbsp;                // can&#39;t be any document failures  coming from this
<b class="nc"><i>1416</i>&nbsp;                indexWriter.deleteDocuments(delete.uid());</b>
<i>1417</i>&nbsp;            }
<b class="nc"><i>1418</i>&nbsp;            if (plan.deleteFromLucene) {</b>
<b class="nc"><i>1419</i>&nbsp;                numDocDeletes.inc();</b>
<b class="nc"><i>1420</i>&nbsp;                versionMap.putDeleteUnderLock(delete.uid().bytes(),</b>
<b class="nc"><i>1421</i>&nbsp;                    new DeleteVersionValue(plan.versionOfDeletion, delete.seqNo(), delete.primaryTerm(),</b>
<b class="nc"><i>1422</i>&nbsp;                        engineConfig.getThreadPool().relativeTimeInMillis()));</b>
<i>1423</i>&nbsp;            }
<b class="nc"><i>1424</i>&nbsp;            return new DeleteResult(</b>
<b class="nc"><i>1425</i>&nbsp;                plan.versionOfDeletion, delete.primaryTerm(), delete.seqNo(), plan.currentlyDeleted == false);</b>
<b class="nc"><i>1426</i>&nbsp;        } catch (final Exception ex) {</b>
<i>1427</i>&nbsp;            /*
<i>1428</i>&nbsp;             * Document level failures when deleting are unexpected, we likely hit something fatal such as the Lucene index being corrupt,
<i>1429</i>&nbsp;             * or the Lucene document limit. We have already issued a sequence number here so this is fatal, fail the engine.
<i>1430</i>&nbsp;             */
<b class="nc"><i>1431</i>&nbsp;            if (ex instanceof AlreadyClosedException == false &amp;&amp; indexWriter.getTragicException() == null) {</b>
<b class="nc"><i>1432</i>&nbsp;                final String reason = String.format(</b>
<i>1433</i>&nbsp;                    Locale.ROOT,
<i>1434</i>&nbsp;                    &quot;delete id[%s] origin [%s] seq#[%d] failed at the document level&quot;,
<b class="nc"><i>1435</i>&nbsp;                    delete.id(),</b>
<b class="nc"><i>1436</i>&nbsp;                    delete.origin(),</b>
<b class="nc"><i>1437</i>&nbsp;                    delete.seqNo());</b>
<b class="nc"><i>1438</i>&nbsp;                failEngine(reason, ex);</b>
<i>1439</i>&nbsp;            }
<b class="nc"><i>1440</i>&nbsp;            throw ex;</b>
<i>1441</i>&nbsp;        }
<i>1442</i>&nbsp;    }
<i>1443</i>&nbsp;
<i>1444</i>&nbsp;    protected static final class DeletionStrategy {
<i>1445</i>&nbsp;        // of a rare double delete
<i>1446</i>&nbsp;        final boolean deleteFromLucene;
<i>1447</i>&nbsp;        final boolean addStaleOpToLucene;
<i>1448</i>&nbsp;        final boolean currentlyDeleted;
<i>1449</i>&nbsp;        final long versionOfDeletion;
<i>1450</i>&nbsp;        final Optional&lt;DeleteResult&gt; earlyResultOnPreflightError;
<i>1451</i>&nbsp;
<i>1452</i>&nbsp;        private DeletionStrategy(boolean deleteFromLucene, boolean addStaleOpToLucene, boolean currentlyDeleted,
<i>1453</i>&nbsp;                                 long versionOfDeletion, DeleteResult earlyResultOnPreflightError) {
<i>1454</i>&nbsp;            assert (deleteFromLucene &amp;&amp; earlyResultOnPreflightError != null) == false :
<i>1455</i>&nbsp;                &quot;can only delete from lucene or have a preflight result but not both.&quot; +
<i>1456</i>&nbsp;                    &quot;deleteFromLucene: &quot; + deleteFromLucene
<i>1457</i>&nbsp;                    + &quot;  earlyResultOnPreFlightError:&quot; + earlyResultOnPreflightError;
<i>1458</i>&nbsp;            this.deleteFromLucene = deleteFromLucene;
<i>1459</i>&nbsp;            this.addStaleOpToLucene = addStaleOpToLucene;
<i>1460</i>&nbsp;            this.currentlyDeleted = currentlyDeleted;
<i>1461</i>&nbsp;            this.versionOfDeletion = versionOfDeletion;
<i>1462</i>&nbsp;            this.earlyResultOnPreflightError = earlyResultOnPreflightError == null ?
<i>1463</i>&nbsp;                Optional.empty() : Optional.of(earlyResultOnPreflightError);
<i>1464</i>&nbsp;        }
<i>1465</i>&nbsp;
<i>1466</i>&nbsp;        public static DeletionStrategy skipDueToVersionConflict(
<i>1467</i>&nbsp;                VersionConflictEngineException e, long currentVersion, boolean currentlyDeleted) {
<i>1468</i>&nbsp;            final DeleteResult deleteResult = new DeleteResult(e, currentVersion, SequenceNumbers.UNASSIGNED_PRIMARY_TERM,
<i>1469</i>&nbsp;                SequenceNumbers.UNASSIGNED_SEQ_NO, currentlyDeleted == false);
<i>1470</i>&nbsp;            return new DeletionStrategy(false, false, currentlyDeleted, Versions.NOT_FOUND, deleteResult);
<i>1471</i>&nbsp;        }
<i>1472</i>&nbsp;
<i>1473</i>&nbsp;        static DeletionStrategy processNormally(boolean currentlyDeleted, long versionOfDeletion) {
<i>1474</i>&nbsp;            return new DeletionStrategy(true, false, currentlyDeleted, versionOfDeletion, null);
<i>1475</i>&nbsp;
<i>1476</i>&nbsp;        }
<i>1477</i>&nbsp;
<i>1478</i>&nbsp;        public static DeletionStrategy processButSkipLucene(boolean currentlyDeleted, long versionOfDeletion) {
<i>1479</i>&nbsp;            return new DeletionStrategy(false, false, currentlyDeleted, versionOfDeletion, null);
<i>1480</i>&nbsp;        }
<i>1481</i>&nbsp;
<i>1482</i>&nbsp;        static DeletionStrategy processAsStaleOp(boolean addStaleOpToLucene, long versionOfDeletion) {
<i>1483</i>&nbsp;            return new DeletionStrategy(false, addStaleOpToLucene, false, versionOfDeletion, null);
<i>1484</i>&nbsp;        }
<i>1485</i>&nbsp;    }
<i>1486</i>&nbsp;
<i>1487</i>&nbsp;    @Override
<i>1488</i>&nbsp;    public void maybePruneDeletes() {
<i>1489</i>&nbsp;        // It&#39;s expensive to prune because we walk the deletes map acquiring dirtyLock for each uid so we only do it
<i>1490</i>&nbsp;        // every 1/4 of gcDeletesInMillis:
<b class="fc"><i>1491</i>&nbsp;        if (engineConfig.isEnableGcDeletes() &amp;&amp;</b>
<b class="fc"><i>1492</i>&nbsp;                engineConfig.getThreadPool().relativeTimeInMillis() - lastDeleteVersionPruneTimeMSec &gt; getGcDeletesInMillis() * 0.25) {</b>
<b class="nc"><i>1493</i>&nbsp;            pruneDeletedTombstones();</b>
<i>1494</i>&nbsp;        }
<b class="fc"><i>1495</i>&nbsp;    }</b>
<i>1496</i>&nbsp;
<i>1497</i>&nbsp;    @Override
<i>1498</i>&nbsp;    public NoOpResult noOp(final NoOp noOp) throws IOException {
<i>1499</i>&nbsp;        final NoOpResult noOpResult;
<b class="nc"><i>1500</i>&nbsp;        try (ReleasableLock ignored = readLock.acquire()) {</b>
<b class="nc"><i>1501</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1502</i>&nbsp;            noOpResult = innerNoOp(noOp);</b>
<b class="nc"><i>1503</i>&nbsp;        } catch (final Exception e) {</b>
<i>1504</i>&nbsp;            try {
<b class="nc"><i>1505</i>&nbsp;                maybeFailEngine(&quot;noop&quot;, e);</b>
<b class="nc"><i>1506</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1507</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1508</i>&nbsp;            }</b>
<b class="nc"><i>1509</i>&nbsp;            throw e;</b>
<b class="nc"><i>1510</i>&nbsp;        }</b>
<b class="nc"><i>1511</i>&nbsp;        return noOpResult;</b>
<i>1512</i>&nbsp;    }
<i>1513</i>&nbsp;
<i>1514</i>&nbsp;    private NoOpResult innerNoOp(final NoOp noOp) throws IOException {
<b class="nc"><i>1515</i>&nbsp;        assert readLock.isHeldByCurrentThread() || writeLock.isHeldByCurrentThread();</b>
<b class="nc"><i>1516</i>&nbsp;        assert noOp.seqNo() &gt; SequenceNumbers.NO_OPS_PERFORMED;</b>
<b class="nc"><i>1517</i>&nbsp;        final long seqNo = noOp.seqNo();</b>
<b class="nc"><i>1518</i>&nbsp;        try (Releasable ignored = noOpKeyedLock.acquire(seqNo)) {</b>
<i>1519</i>&nbsp;            final NoOpResult noOpResult;
<b class="nc"><i>1520</i>&nbsp;            final Optional&lt;Exception&gt; preFlightError = preFlightCheckForNoOp(noOp);</b>
<b class="nc"><i>1521</i>&nbsp;            if (preFlightError.isPresent()) {</b>
<b class="nc"><i>1522</i>&nbsp;                noOpResult = new NoOpResult(SequenceNumbers.UNASSIGNED_PRIMARY_TERM,</b>
<b class="nc"><i>1523</i>&nbsp;                    SequenceNumbers.UNASSIGNED_SEQ_NO, preFlightError.get());</b>
<i>1524</i>&nbsp;            } else {
<b class="nc"><i>1525</i>&nbsp;                markSeqNoAsSeen(noOp.seqNo());</b>
<b class="nc"><i>1526</i>&nbsp;                if (softDeleteEnabled &amp;&amp; hasBeenProcessedBefore(noOp) == false) {</b>
<i>1527</i>&nbsp;                    try {
<b class="nc"><i>1528</i>&nbsp;                        final ParsedDocument tombstone = engineConfig.getTombstoneDocSupplier().newNoopTombstoneDoc(noOp.reason());</b>
<b class="nc"><i>1529</i>&nbsp;                        tombstone.updateSeqID(noOp.seqNo(), noOp.primaryTerm());</b>
<i>1530</i>&nbsp;                        // A noop tombstone does not require a _version but it&#39;s added to have a fully dense docvalues for the version
<i>1531</i>&nbsp;                        // field. 1L is selected to optimize the compression because it might probably be the most common value in
<i>1532</i>&nbsp;                        // version field.
<b class="nc"><i>1533</i>&nbsp;                        tombstone.version().setLongValue(1L);</b>
<b class="nc"><i>1534</i>&nbsp;                        assert tombstone.docs().size() == 1 : &quot;Tombstone should have a single doc [&quot; + tombstone + &quot;]&quot;;</b>
<b class="nc"><i>1535</i>&nbsp;                        final ParseContext.Document doc = tombstone.docs().get(0);</b>
<b class="nc"><i>1536</i>&nbsp;                        assert doc.getField(SeqNoFieldMapper.TOMBSTONE_NAME) != null</b>
<i>1537</i>&nbsp;                            : &quot;Noop tombstone document but _tombstone field is not set [&quot; + doc + &quot; ]&quot;;
<b class="nc"><i>1538</i>&nbsp;                        doc.add(softDeletesField);</b>
<b class="nc"><i>1539</i>&nbsp;                        indexWriter.addDocument(doc);</b>
<b class="nc"><i>1540</i>&nbsp;                    } catch (final Exception ex) {</b>
<i>1541</i>&nbsp;                        /*
<i>1542</i>&nbsp;                         * Document level failures when adding a no-op are unexpected, we likely hit something fatal such as the Lucene
<i>1543</i>&nbsp;                         * index being corrupt, or the Lucene document limit. We have already issued a sequence number here so this is
<i>1544</i>&nbsp;                         * fatal, fail the engine.
<i>1545</i>&nbsp;                         */
<b class="nc"><i>1546</i>&nbsp;                        if (ex instanceof AlreadyClosedException == false &amp;&amp; indexWriter.getTragicException() == null) {</b>
<b class="nc"><i>1547</i>&nbsp;                            failEngine(&quot;no-op origin[&quot; + noOp.origin() + &quot;] seq#[&quot; + noOp.seqNo() + &quot;] failed at document level&quot;, ex);</b>
<i>1548</i>&nbsp;                        }
<b class="nc"><i>1549</i>&nbsp;                        throw ex;</b>
<b class="nc"><i>1550</i>&nbsp;                    }</b>
<i>1551</i>&nbsp;                }
<b class="nc"><i>1552</i>&nbsp;                noOpResult = new NoOpResult(noOp.primaryTerm(), noOp.seqNo());</b>
<b class="nc"><i>1553</i>&nbsp;                if (noOp.origin().isFromTranslog() == false &amp;&amp; noOpResult.getResultType() == Result.Type.SUCCESS) {</b>
<b class="nc"><i>1554</i>&nbsp;                    final Translog.Location location = translog.add(new Translog.NoOp(noOp.seqNo(), noOp.primaryTerm(), noOp.reason()));</b>
<b class="nc"><i>1555</i>&nbsp;                    noOpResult.setTranslogLocation(location);</b>
<i>1556</i>&nbsp;                }
<i>1557</i>&nbsp;            }
<b class="nc"><i>1558</i>&nbsp;            localCheckpointTracker.markSeqNoAsProcessed(noOpResult.getSeqNo());</b>
<b class="nc"><i>1559</i>&nbsp;            if (noOpResult.getTranslogLocation() == null) {</b>
<i>1560</i>&nbsp;                // the op is coming from the translog (and is hence persisted already) or it does not have a sequence number
<b class="nc"><i>1561</i>&nbsp;                assert noOp.origin().isFromTranslog() || noOpResult.getSeqNo() == SequenceNumbers.UNASSIGNED_SEQ_NO;</b>
<b class="nc"><i>1562</i>&nbsp;                localCheckpointTracker.markSeqNoAsPersisted(noOpResult.getSeqNo());</b>
<i>1563</i>&nbsp;            }
<b class="nc"><i>1564</i>&nbsp;            noOpResult.setTook(System.nanoTime() - noOp.startTime());</b>
<b class="nc"><i>1565</i>&nbsp;            noOpResult.freeze();</b>
<b class="nc"><i>1566</i>&nbsp;            return noOpResult;</b>
<b class="nc"><i>1567</i>&nbsp;        }</b>
<i>1568</i>&nbsp;    }
<i>1569</i>&nbsp;
<i>1570</i>&nbsp;    /**
<i>1571</i>&nbsp;     * Executes a pre-flight check for a given NoOp.
<i>1572</i>&nbsp;     * If this method returns a non-empty result, the engine won&#39;t process this NoOp and returns a failure.
<i>1573</i>&nbsp;     */
<i>1574</i>&nbsp;    protected Optional&lt;Exception&gt; preFlightCheckForNoOp(final NoOp noOp) throws IOException {
<b class="nc"><i>1575</i>&nbsp;        return Optional.empty();</b>
<i>1576</i>&nbsp;    }
<i>1577</i>&nbsp;
<i>1578</i>&nbsp;    @Override
<i>1579</i>&nbsp;    public void refresh(String source) throws EngineException {
<b class="fc"><i>1580</i>&nbsp;        refresh(source, SearcherScope.EXTERNAL, true);</b>
<b class="fc"><i>1581</i>&nbsp;    }</b>
<i>1582</i>&nbsp;
<i>1583</i>&nbsp;    @Override
<i>1584</i>&nbsp;    public boolean maybeRefresh(String source) throws EngineException {
<b class="nc"><i>1585</i>&nbsp;        return refresh(source, SearcherScope.EXTERNAL, false);</b>
<i>1586</i>&nbsp;    }
<i>1587</i>&nbsp;
<i>1588</i>&nbsp;    final boolean refresh(String source, SearcherScope scope, boolean block) throws EngineException {
<i>1589</i>&nbsp;        // both refresh types will result in an internal refresh but only the external will also
<i>1590</i>&nbsp;        // pass the new reader reference to the external reader manager.
<b class="fc"><i>1591</i>&nbsp;        final long localCheckpointBeforeRefresh = localCheckpointTracker.getProcessedCheckpoint();</b>
<i>1592</i>&nbsp;        boolean refreshed;
<i>1593</i>&nbsp;        try {
<i>1594</i>&nbsp;            // refresh does not need to hold readLock as ReferenceManager can handle correctly if the engine is closed in mid-way.
<b class="fc"><i>1595</i>&nbsp;            if (store.tryIncRef()) {</b>
<i>1596</i>&nbsp;                // increment the ref just to ensure nobody closes the store during a refresh
<i>1597</i>&nbsp;                try {
<i>1598</i>&nbsp;                    // even though we maintain 2 managers we really do the heavy-lifting only once.
<i>1599</i>&nbsp;                    // the second refresh will only do the extra work we have to do for warming caches etc.
<b class="fc"><i>1600</i>&nbsp;                    ReferenceManager&lt;ElasticsearchDirectoryReader&gt; referenceManager = getReferenceManager(scope);</b>
<i>1601</i>&nbsp;                    // it is intentional that we never refresh both internal / external together
<b class="fc"><i>1602</i>&nbsp;                    if (block) {</b>
<b class="fc"><i>1603</i>&nbsp;                        referenceManager.maybeRefreshBlocking();</b>
<b class="fc"><i>1604</i>&nbsp;                        refreshed = true;</b>
<i>1605</i>&nbsp;                    } else {
<b class="nc"><i>1606</i>&nbsp;                        refreshed = referenceManager.maybeRefresh();</b>
<i>1607</i>&nbsp;                    }
<i>1608</i>&nbsp;                } finally {
<b class="fc"><i>1609</i>&nbsp;                    store.decRef();</b>
<b class="fc"><i>1610</i>&nbsp;                }</b>
<b class="fc"><i>1611</i>&nbsp;                if (refreshed) {</b>
<b class="fc"><i>1612</i>&nbsp;                    lastRefreshedCheckpointListener.updateRefreshedCheckpoint(localCheckpointBeforeRefresh);</b>
<i>1613</i>&nbsp;                }
<i>1614</i>&nbsp;            } else {
<b class="nc"><i>1615</i>&nbsp;                refreshed = false;</b>
<i>1616</i>&nbsp;            }
<b class="nc"><i>1617</i>&nbsp;        } catch (AlreadyClosedException e) {</b>
<b class="nc"><i>1618</i>&nbsp;            failOnTragicEvent(e);</b>
<b class="nc"><i>1619</i>&nbsp;            throw e;</b>
<b class="nc"><i>1620</i>&nbsp;        } catch (Exception e) {</b>
<i>1621</i>&nbsp;            try {
<b class="nc"><i>1622</i>&nbsp;                failEngine(&quot;refresh failed source[&quot; + source + &quot;]&quot;, e);</b>
<b class="nc"><i>1623</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1624</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1625</i>&nbsp;            }</b>
<b class="nc"><i>1626</i>&nbsp;            throw new RefreshFailedEngineException(shardId, e);</b>
<b class="fc"><i>1627</i>&nbsp;        }</b>
<b class="fc"><i>1628</i>&nbsp;        assert refreshed == false || lastRefreshedCheckpoint() &gt;= localCheckpointBeforeRefresh : &quot;refresh checkpoint was not advanced; &quot; +</b>
<b class="nc"><i>1629</i>&nbsp;            &quot;local_checkpoint=&quot; + localCheckpointBeforeRefresh + &quot; refresh_checkpoint=&quot; + lastRefreshedCheckpoint();</b>
<i>1630</i>&nbsp;        // TODO: maybe we should just put a scheduled job in threadPool?
<i>1631</i>&nbsp;        // We check for pruning in each delete request, but we also prune here e.g. in case a delete burst comes in and then no more deletes
<i>1632</i>&nbsp;        // for a long time:
<b class="fc"><i>1633</i>&nbsp;        maybePruneDeletes();</b>
<b class="fc"><i>1634</i>&nbsp;        mergeScheduler.refreshConfig();</b>
<b class="fc"><i>1635</i>&nbsp;        return refreshed;</b>
<i>1636</i>&nbsp;    }
<i>1637</i>&nbsp;
<i>1638</i>&nbsp;    @Override
<i>1639</i>&nbsp;    public void writeIndexingBuffer() throws EngineException {
<i>1640</i>&nbsp;        // we obtain a read lock here, since we don&#39;t want a flush to happen while we are writing
<i>1641</i>&nbsp;        // since it flushes the index as well (though, in terms of concurrency, we are allowed to do it)
<b class="nc"><i>1642</i>&nbsp;        refresh(&quot;write indexing buffer&quot;, SearcherScope.INTERNAL, true);</b>
<b class="nc"><i>1643</i>&nbsp;    }</b>
<i>1644</i>&nbsp;
<i>1645</i>&nbsp;    @Override
<i>1646</i>&nbsp;    public SyncedFlushResult syncFlush(String syncId, CommitId expectedCommitId) throws EngineException {
<i>1647</i>&nbsp;        // best effort attempt before we acquire locks
<b class="nc"><i>1648</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>1649</i>&nbsp;        if (indexWriter.hasUncommittedChanges()) {</b>
<b class="nc"><i>1650</i>&nbsp;            logger.trace(&quot;can&#39;t sync commit [{}]. have pending changes&quot;, syncId);</b>
<b class="nc"><i>1651</i>&nbsp;            return SyncedFlushResult.PENDING_OPERATIONS;</b>
<i>1652</i>&nbsp;        }
<b class="nc"><i>1653</i>&nbsp;        if (expectedCommitId.idsEqual(lastCommittedSegmentInfos.getId()) == false) {</b>
<b class="nc"><i>1654</i>&nbsp;            logger.trace(&quot;can&#39;t sync commit [{}]. current commit id is not equal to expected.&quot;, syncId);</b>
<b class="nc"><i>1655</i>&nbsp;            return SyncedFlushResult.COMMIT_MISMATCH;</b>
<i>1656</i>&nbsp;        }
<b class="nc"><i>1657</i>&nbsp;        try (ReleasableLock lock = writeLock.acquire()) {</b>
<b class="nc"><i>1658</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1659</i>&nbsp;            ensureCanFlush();</b>
<i>1660</i>&nbsp;            // lets do a refresh to make sure we shrink the version map. This refresh will be either a no-op (just shrink the version map)
<i>1661</i>&nbsp;            // or we also have uncommitted changes and that causes this syncFlush to fail.
<b class="nc"><i>1662</i>&nbsp;            refresh(&quot;sync_flush&quot;, SearcherScope.INTERNAL, true);</b>
<b class="nc"><i>1663</i>&nbsp;            if (indexWriter.hasUncommittedChanges()) {</b>
<b class="nc"><i>1664</i>&nbsp;                logger.trace(&quot;can&#39;t sync commit [{}]. have pending changes&quot;, syncId);</b>
<b class="nc"><i>1665</i>&nbsp;                return SyncedFlushResult.PENDING_OPERATIONS;</b>
<i>1666</i>&nbsp;            }
<b class="nc"><i>1667</i>&nbsp;            if (expectedCommitId.idsEqual(lastCommittedSegmentInfos.getId()) == false) {</b>
<b class="nc"><i>1668</i>&nbsp;                logger.trace(&quot;can&#39;t sync commit [{}]. current commit id is not equal to expected.&quot;, syncId);</b>
<b class="nc"><i>1669</i>&nbsp;                return SyncedFlushResult.COMMIT_MISMATCH;</b>
<i>1670</i>&nbsp;            }
<b class="nc"><i>1671</i>&nbsp;            logger.trace(&quot;starting sync commit [{}]&quot;, syncId);</b>
<b class="nc"><i>1672</i>&nbsp;            commitIndexWriter(indexWriter, translog, syncId);</b>
<b class="nc"><i>1673</i>&nbsp;            logger.debug(&quot;successfully sync committed. sync id [{}].&quot;, syncId);</b>
<b class="nc"><i>1674</i>&nbsp;            lastCommittedSegmentInfos = store.readLastCommittedSegmentsInfo();</b>
<b class="nc"><i>1675</i>&nbsp;            return SyncedFlushResult.SUCCESS;</b>
<b class="nc"><i>1676</i>&nbsp;        } catch (IOException ex) {</b>
<b class="nc"><i>1677</i>&nbsp;            maybeFailEngine(&quot;sync commit&quot;, ex);</b>
<b class="nc"><i>1678</i>&nbsp;            throw new EngineException(shardId, &quot;failed to sync commit&quot;, ex);</b>
<i>1679</i>&nbsp;        }
<i>1680</i>&nbsp;    }
<i>1681</i>&nbsp;
<i>1682</i>&nbsp;    final boolean tryRenewSyncCommit() {
<b class="nc"><i>1683</i>&nbsp;        boolean renewed = false;</b>
<b class="nc"><i>1684</i>&nbsp;        try (ReleasableLock lock = writeLock.acquire()) {</b>
<b class="nc"><i>1685</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1686</i>&nbsp;            ensureCanFlush();</b>
<b class="nc"><i>1687</i>&nbsp;            String syncId = lastCommittedSegmentInfos.getUserData().get(SYNC_COMMIT_ID);</b>
<b class="nc"><i>1688</i>&nbsp;            long translogGenOfLastCommit = Long.parseLong(lastCommittedSegmentInfos.userData.get(Translog.TRANSLOG_GENERATION_KEY));</b>
<b class="nc"><i>1689</i>&nbsp;            if (syncId != null &amp;&amp; indexWriter.hasUncommittedChanges() &amp;&amp; translog.totalOperationsByMinGen(translogGenOfLastCommit) == 0) {</b>
<b class="nc"><i>1690</i>&nbsp;                logger.trace(&quot;start renewing sync commit [{}]&quot;, syncId);</b>
<b class="nc"><i>1691</i>&nbsp;                commitIndexWriter(indexWriter, translog, syncId);</b>
<b class="nc"><i>1692</i>&nbsp;                logger.debug(&quot;successfully sync committed. sync id [{}].&quot;, syncId);</b>
<b class="nc"><i>1693</i>&nbsp;                lastCommittedSegmentInfos = store.readLastCommittedSegmentsInfo();</b>
<b class="nc"><i>1694</i>&nbsp;                renewed = true;</b>
<i>1695</i>&nbsp;            }
<b class="nc"><i>1696</i>&nbsp;        } catch (IOException ex) {</b>
<b class="nc"><i>1697</i>&nbsp;            maybeFailEngine(&quot;renew sync commit&quot;, ex);</b>
<b class="nc"><i>1698</i>&nbsp;            throw new EngineException(shardId, &quot;failed to renew sync commit&quot;, ex);</b>
<b class="nc"><i>1699</i>&nbsp;        }</b>
<b class="nc"><i>1700</i>&nbsp;        if (renewed) {</b>
<i>1701</i>&nbsp;            // refresh outside of the write lock
<i>1702</i>&nbsp;            // we have to refresh internal reader here to ensure we release unreferenced segments.
<b class="nc"><i>1703</i>&nbsp;            refresh(&quot;renew sync commit&quot;, SearcherScope.INTERNAL, true);</b>
<i>1704</i>&nbsp;        }
<b class="nc"><i>1705</i>&nbsp;        return renewed;</b>
<i>1706</i>&nbsp;    }
<i>1707</i>&nbsp;
<i>1708</i>&nbsp;    @Override
<i>1709</i>&nbsp;    public boolean shouldPeriodicallyFlush() {
<b class="nc"><i>1710</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>1711</i>&nbsp;        if (shouldPeriodicallyFlushAfterBigMerge.get()) {</b>
<b class="nc"><i>1712</i>&nbsp;            return true;</b>
<i>1713</i>&nbsp;        }
<b class="nc"><i>1714</i>&nbsp;        final long translogGenerationOfLastCommit =</b>
<b class="nc"><i>1715</i>&nbsp;            Long.parseLong(lastCommittedSegmentInfos.userData.get(Translog.TRANSLOG_GENERATION_KEY));</b>
<b class="nc"><i>1716</i>&nbsp;        final long flushThreshold = config().getIndexSettings().getFlushThresholdSize().getBytes();</b>
<b class="nc"><i>1717</i>&nbsp;        if (translog.sizeInBytesByMinGen(translogGenerationOfLastCommit) &lt; flushThreshold) {</b>
<b class="nc"><i>1718</i>&nbsp;            return false;</b>
<i>1719</i>&nbsp;        }
<i>1720</i>&nbsp;        /*
<i>1721</i>&nbsp;         * We flush to reduce the size of uncommitted translog but strictly speaking the uncommitted size won&#39;t always be
<i>1722</i>&nbsp;         * below the flush-threshold after a flush. To avoid getting into an endless loop of flushing, we only enable the
<i>1723</i>&nbsp;         * periodically flush condition if this condition is disabled after a flush. The condition will change if the new
<i>1724</i>&nbsp;         * commit points to the later generation the last commit&#39;s(eg. gen-of-last-commit &lt; gen-of-new-commit)[1].
<i>1725</i>&nbsp;         *
<i>1726</i>&nbsp;         * When the local checkpoint equals to max_seqno, and translog-gen of the last commit equals to translog-gen of
<i>1727</i>&nbsp;         * the new commit, we know that the last generation must contain operations because its size is above the flush
<i>1728</i>&nbsp;         * threshold and the flush-threshold is guaranteed to be higher than an empty translog by the setting validation.
<i>1729</i>&nbsp;         * This guarantees that the new commit will point to the newly rolled generation. In fact, this scenario only
<i>1730</i>&nbsp;         * happens when the generation-threshold is close to or above the flush-threshold; otherwise we have rolled
<i>1731</i>&nbsp;         * generations as the generation-threshold was reached, then the first condition (eg. [1]) is already satisfied.
<i>1732</i>&nbsp;         *
<i>1733</i>&nbsp;         * This method is to maintain translog only, thus IndexWriter#hasUncommittedChanges condition is not considered.
<i>1734</i>&nbsp;         */
<b class="nc"><i>1735</i>&nbsp;        final long translogGenerationOfNewCommit =</b>
<b class="nc"><i>1736</i>&nbsp;            translog.getMinGenerationForSeqNo(localCheckpointTracker.getProcessedCheckpoint() + 1).translogFileGeneration;</b>
<b class="nc"><i>1737</i>&nbsp;        return translogGenerationOfLastCommit &lt; translogGenerationOfNewCommit</b>
<b class="nc"><i>1738</i>&nbsp;            || localCheckpointTracker.getProcessedCheckpoint() == localCheckpointTracker.getMaxSeqNo();</b>
<i>1739</i>&nbsp;    }
<i>1740</i>&nbsp;
<i>1741</i>&nbsp;    @Override
<i>1742</i>&nbsp;    public CommitId flush(boolean force, boolean waitIfOngoing) throws EngineException {
<b class="nc"><i>1743</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>1744</i>&nbsp;        if (force &amp;&amp; waitIfOngoing == false) {</b>
<b class="nc"><i>1745</i>&nbsp;            assert false : &quot;wait_if_ongoing must be true for a force flush: force=&quot; + force + &quot; wait_if_ongoing=&quot; + waitIfOngoing;</b>
<b class="nc"><i>1746</i>&nbsp;            throw new IllegalArgumentException(</b>
<i>1747</i>&nbsp;                &quot;wait_if_ongoing must be true for a force flush: force=&quot; + force + &quot; wait_if_ongoing=&quot; + waitIfOngoing);
<i>1748</i>&nbsp;        }
<i>1749</i>&nbsp;        final byte[] newCommitId;
<i>1750</i>&nbsp;        /*
<i>1751</i>&nbsp;         * Unfortunately the lock order is important here. We have to acquire the readlock first otherwise
<i>1752</i>&nbsp;         * if we are flushing at the end of the recovery while holding the write lock we can deadlock if:
<i>1753</i>&nbsp;         *  Thread 1: flushes via API and gets the flush lock but blocks on the readlock since Thread 2 has the writeLock
<i>1754</i>&nbsp;         *  Thread 2: flushes at the end of the recovery holding the writeLock and blocks on the flushLock owned by Thread 1
<i>1755</i>&nbsp;         */
<b class="nc"><i>1756</i>&nbsp;        try (ReleasableLock lock = readLock.acquire()) {</b>
<b class="nc"><i>1757</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1758</i>&nbsp;            if (flushLock.tryLock() == false) {</b>
<i>1759</i>&nbsp;                // if we can&#39;t get the lock right away we block if needed otherwise barf
<b class="nc"><i>1760</i>&nbsp;                if (waitIfOngoing) {</b>
<b class="nc"><i>1761</i>&nbsp;                    logger.trace(&quot;waiting for in-flight flush to finish&quot;);</b>
<b class="nc"><i>1762</i>&nbsp;                    flushLock.lock();</b>
<b class="nc"><i>1763</i>&nbsp;                    logger.trace(&quot;acquired flush lock after blocking&quot;);</b>
<i>1764</i>&nbsp;                } else {
<b class="nc"><i>1765</i>&nbsp;                    return new CommitId(lastCommittedSegmentInfos.getId());</b>
<i>1766</i>&nbsp;                }
<i>1767</i>&nbsp;            } else {
<b class="nc"><i>1768</i>&nbsp;                logger.trace(&quot;acquired flush lock immediately&quot;);</b>
<i>1769</i>&nbsp;            }
<i>1770</i>&nbsp;            try {
<i>1771</i>&nbsp;                // Only flush if (1) Lucene has uncommitted docs, or (2) forced by caller, or (3) the
<i>1772</i>&nbsp;                // newly created commit points to a different translog generation (can free translog)
<b class="nc"><i>1773</i>&nbsp;                boolean hasUncommittedChanges = indexWriter.hasUncommittedChanges();</b>
<b class="nc"><i>1774</i>&nbsp;                boolean shouldPeriodicallyFlush = shouldPeriodicallyFlush();</b>
<b class="nc"><i>1775</i>&nbsp;                if (hasUncommittedChanges || force || shouldPeriodicallyFlush) {</b>
<b class="nc"><i>1776</i>&nbsp;                    ensureCanFlush();</b>
<i>1777</i>&nbsp;                    try {
<b class="nc"><i>1778</i>&nbsp;                        translog.rollGeneration();</b>
<b class="nc"><i>1779</i>&nbsp;                        logger.trace(&quot;starting commit for flush; commitTranslog=true&quot;);</b>
<b class="nc"><i>1780</i>&nbsp;                        commitIndexWriter(indexWriter, translog, null);</b>
<b class="nc"><i>1781</i>&nbsp;                        logger.trace(&quot;finished commit for flush&quot;);</b>
<i>1782</i>&nbsp;
<i>1783</i>&nbsp;                        // a temporary debugging to investigate test failure - issue#32827. Remove when the issue is resolved
<b class="nc"><i>1784</i>&nbsp;                        logger.debug(&quot;new commit on flush, hasUncommittedChanges:{}, force:{}, shouldPeriodicallyFlush:{}&quot;,</b>
<b class="nc"><i>1785</i>&nbsp;                            hasUncommittedChanges, force, shouldPeriodicallyFlush);</b>
<i>1786</i>&nbsp;
<i>1787</i>&nbsp;                        // we need to refresh in order to clear older version values
<b class="nc"><i>1788</i>&nbsp;                        refresh(&quot;version_table_flush&quot;, SearcherScope.INTERNAL, true);</b>
<b class="nc"><i>1789</i>&nbsp;                        translog.trimUnreferencedReaders();</b>
<b class="nc"><i>1790</i>&nbsp;                    } catch (AlreadyClosedException e) {</b>
<b class="nc"><i>1791</i>&nbsp;                        throw e;</b>
<b class="nc"><i>1792</i>&nbsp;                    } catch (Exception e) {</b>
<b class="nc"><i>1793</i>&nbsp;                        throw new FlushFailedEngineException(shardId, e);</b>
<b class="nc"><i>1794</i>&nbsp;                    }</b>
<b class="nc"><i>1795</i>&nbsp;                    refreshLastCommittedSegmentInfos();</b>
<i>1796</i>&nbsp;
<i>1797</i>&nbsp;                }
<b class="nc"><i>1798</i>&nbsp;                newCommitId = lastCommittedSegmentInfos.getId();</b>
<b class="nc"><i>1799</i>&nbsp;            } catch (FlushFailedEngineException ex) {</b>
<b class="nc"><i>1800</i>&nbsp;                maybeFailEngine(&quot;flush&quot;, ex);</b>
<b class="nc"><i>1801</i>&nbsp;                throw ex;</b>
<i>1802</i>&nbsp;            } finally {
<b class="nc"><i>1803</i>&nbsp;                flushLock.unlock();</b>
<b class="nc"><i>1804</i>&nbsp;            }</b>
<b class="nc"><i>1805</i>&nbsp;        }</b>
<i>1806</i>&nbsp;        // We don&#39;t have to do this here; we do it defensively to make sure that even if wall clock time is misbehaving
<i>1807</i>&nbsp;        // (e.g., moves backwards) we will at least still sometimes prune deleted tombstones:
<b class="nc"><i>1808</i>&nbsp;        if (engineConfig.isEnableGcDeletes()) {</b>
<b class="nc"><i>1809</i>&nbsp;            pruneDeletedTombstones();</b>
<i>1810</i>&nbsp;        }
<b class="nc"><i>1811</i>&nbsp;        return new CommitId(newCommitId);</b>
<i>1812</i>&nbsp;    }
<i>1813</i>&nbsp;
<i>1814</i>&nbsp;    private void refreshLastCommittedSegmentInfos() {
<i>1815</i>&nbsp;    /*
<i>1816</i>&nbsp;     * we have to inc-ref the store here since if the engine is closed by a tragic event
<i>1817</i>&nbsp;     * we don&#39;t acquire the write lock and wait until we have exclusive access. This might also
<i>1818</i>&nbsp;     * dec the store reference which can essentially close the store and unless we can inc the reference
<i>1819</i>&nbsp;     * we can&#39;t use it.
<i>1820</i>&nbsp;     */
<b class="nc"><i>1821</i>&nbsp;        store.incRef();</b>
<i>1822</i>&nbsp;        try {
<i>1823</i>&nbsp;            // reread the last committed segment infos
<b class="nc"><i>1824</i>&nbsp;            lastCommittedSegmentInfos = store.readLastCommittedSegmentsInfo();</b>
<b class="nc"><i>1825</i>&nbsp;        } catch (Exception e) {</b>
<b class="nc"><i>1826</i>&nbsp;            if (isClosed.get() == false) {</b>
<i>1827</i>&nbsp;                try {
<b class="nc"><i>1828</i>&nbsp;                    logger.warn(&quot;failed to read latest segment infos on flush&quot;, e);</b>
<b class="nc"><i>1829</i>&nbsp;                } catch (Exception inner) {</b>
<b class="nc"><i>1830</i>&nbsp;                    e.addSuppressed(inner);</b>
<b class="nc"><i>1831</i>&nbsp;                }</b>
<b class="nc"><i>1832</i>&nbsp;                if (Lucene.isCorruptionException(e)) {</b>
<b class="nc"><i>1833</i>&nbsp;                    throw new FlushFailedEngineException(shardId, e);</b>
<i>1834</i>&nbsp;                }
<i>1835</i>&nbsp;            }
<i>1836</i>&nbsp;        } finally {
<b class="nc"><i>1837</i>&nbsp;            store.decRef();</b>
<b class="nc"><i>1838</i>&nbsp;        }</b>
<b class="nc"><i>1839</i>&nbsp;    }</b>
<i>1840</i>&nbsp;
<i>1841</i>&nbsp;    @Override
<i>1842</i>&nbsp;    public void rollTranslogGeneration() throws EngineException {
<b class="nc"><i>1843</i>&nbsp;        try (ReleasableLock ignored = readLock.acquire()) {</b>
<b class="nc"><i>1844</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1845</i>&nbsp;            translog.rollGeneration();</b>
<b class="nc"><i>1846</i>&nbsp;            translog.trimUnreferencedReaders();</b>
<b class="nc"><i>1847</i>&nbsp;        } catch (AlreadyClosedException e) {</b>
<b class="nc"><i>1848</i>&nbsp;            failOnTragicEvent(e);</b>
<b class="nc"><i>1849</i>&nbsp;            throw e;</b>
<b class="nc"><i>1850</i>&nbsp;        } catch (Exception e) {</b>
<i>1851</i>&nbsp;            try {
<b class="nc"><i>1852</i>&nbsp;                failEngine(&quot;translog trimming failed&quot;, e);</b>
<b class="nc"><i>1853</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1854</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1855</i>&nbsp;            }</b>
<b class="nc"><i>1856</i>&nbsp;            throw new EngineException(shardId, &quot;failed to roll translog&quot;, e);</b>
<b class="nc"><i>1857</i>&nbsp;        }</b>
<b class="nc"><i>1858</i>&nbsp;    }</b>
<i>1859</i>&nbsp;
<i>1860</i>&nbsp;    @Override
<i>1861</i>&nbsp;    public void trimUnreferencedTranslogFiles() throws EngineException {
<b class="nc"><i>1862</i>&nbsp;        try (ReleasableLock lock = readLock.acquire()) {</b>
<b class="nc"><i>1863</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1864</i>&nbsp;            translog.trimUnreferencedReaders();</b>
<b class="nc"><i>1865</i>&nbsp;        } catch (AlreadyClosedException e) {</b>
<b class="nc"><i>1866</i>&nbsp;            failOnTragicEvent(e);</b>
<b class="nc"><i>1867</i>&nbsp;            throw e;</b>
<b class="nc"><i>1868</i>&nbsp;        } catch (Exception e) {</b>
<i>1869</i>&nbsp;            try {
<b class="nc"><i>1870</i>&nbsp;                failEngine(&quot;translog trimming failed&quot;, e);</b>
<b class="nc"><i>1871</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1872</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1873</i>&nbsp;            }</b>
<b class="nc"><i>1874</i>&nbsp;            throw new EngineException(shardId, &quot;failed to trim translog&quot;, e);</b>
<b class="nc"><i>1875</i>&nbsp;        }</b>
<b class="nc"><i>1876</i>&nbsp;    }</b>
<i>1877</i>&nbsp;
<i>1878</i>&nbsp;    @Override
<i>1879</i>&nbsp;    public boolean shouldRollTranslogGeneration() {
<b class="nc"><i>1880</i>&nbsp;        return getTranslog().shouldRollGeneration();</b>
<i>1881</i>&nbsp;    }
<i>1882</i>&nbsp;
<i>1883</i>&nbsp;    @Override
<i>1884</i>&nbsp;    public void trimOperationsFromTranslog(long belowTerm, long aboveSeqNo) throws EngineException {
<b class="nc"><i>1885</i>&nbsp;        try (ReleasableLock lock = readLock.acquire()) {</b>
<b class="nc"><i>1886</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1887</i>&nbsp;            translog.trimOperations(belowTerm, aboveSeqNo);</b>
<b class="nc"><i>1888</i>&nbsp;        } catch (AlreadyClosedException e) {</b>
<b class="nc"><i>1889</i>&nbsp;            failOnTragicEvent(e);</b>
<b class="nc"><i>1890</i>&nbsp;            throw e;</b>
<b class="nc"><i>1891</i>&nbsp;        } catch (Exception e) {</b>
<i>1892</i>&nbsp;            try {
<b class="nc"><i>1893</i>&nbsp;                failEngine(&quot;translog operations trimming failed&quot;, e);</b>
<b class="nc"><i>1894</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1895</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1896</i>&nbsp;            }</b>
<b class="nc"><i>1897</i>&nbsp;            throw new EngineException(shardId, &quot;failed to trim translog operations&quot;, e);</b>
<b class="nc"><i>1898</i>&nbsp;        }</b>
<b class="nc"><i>1899</i>&nbsp;    }</b>
<i>1900</i>&nbsp;
<i>1901</i>&nbsp;    private void pruneDeletedTombstones() {
<i>1902</i>&nbsp;        /*
<i>1903</i>&nbsp;         * We need to deploy two different trimming strategies for GC deletes on primary and replicas. Delete operations on primary
<i>1904</i>&nbsp;         * are remembered for at least one GC delete cycle and trimmed periodically. This is, at the moment, the best we can do on
<i>1905</i>&nbsp;         * primary for user facing APIs but this arbitrary time limit is problematic for replicas. On replicas however we should
<i>1906</i>&nbsp;         * trim only deletes whose seqno at most the local checkpoint. This requirement is explained as follows.
<i>1907</i>&nbsp;         *
<i>1908</i>&nbsp;         * Suppose o1 and o2 are two operations on the same document with seq#(o1) &lt; seq#(o2), and o2 arrives before o1 on the replica.
<i>1909</i>&nbsp;         * o2 is processed normally since it arrives first; when o1 arrives it should be discarded:
<i>1910</i>&nbsp;         * - If seq#(o1) &lt;= LCP, then it will be not be added to Lucene, as it was already previously added.
<i>1911</i>&nbsp;         * - If seq#(o1)  &gt; LCP, then it depends on the nature of o2:
<i>1912</i>&nbsp;         *   *) If o2 is a delete then its seq# is recorded in the VersionMap, since seq#(o2) &gt; seq#(o1) &gt; LCP,
<i>1913</i>&nbsp;         *      so a lookup can find it and determine that o1 is stale.
<i>1914</i>&nbsp;         *   *) If o2 is an indexing then its seq# is either in Lucene (if refreshed) or the VersionMap (if not refreshed yet),
<i>1915</i>&nbsp;         *      so a real-time lookup can find it and determine that o1 is stale.
<i>1916</i>&nbsp;         *
<i>1917</i>&nbsp;         * Here we prefer to deploy a single trimming strategy, which satisfies two constraints, on both primary and replicas because:
<i>1918</i>&nbsp;         * - It&#39;s simpler - no need to distinguish if an engine is running at primary mode or replica mode or being promoted.
<i>1919</i>&nbsp;         * - If a replica subsequently is promoted, user experience is maintained as that replica remembers deletes for the last GC cycle.
<i>1920</i>&nbsp;         *
<i>1921</i>&nbsp;         * However, the version map may consume less memory if we deploy two different trimming strategies for primary and replicas.
<i>1922</i>&nbsp;         */
<b class="nc"><i>1923</i>&nbsp;        final long timeMSec = engineConfig.getThreadPool().relativeTimeInMillis();</b>
<b class="nc"><i>1924</i>&nbsp;        final long maxTimestampToPrune = timeMSec - engineConfig.getIndexSettings().getGcDeletesInMillis();</b>
<b class="nc"><i>1925</i>&nbsp;        versionMap.pruneTombstones(maxTimestampToPrune, localCheckpointTracker.getProcessedCheckpoint());</b>
<b class="nc"><i>1926</i>&nbsp;        lastDeleteVersionPruneTimeMSec = timeMSec;</b>
<b class="nc"><i>1927</i>&nbsp;    }</b>
<i>1928</i>&nbsp;
<i>1929</i>&nbsp;    // testing
<i>1930</i>&nbsp;    void clearDeletedTombstones() {
<b class="nc"><i>1931</i>&nbsp;        versionMap.pruneTombstones(Long.MAX_VALUE, localCheckpointTracker.getMaxSeqNo());</b>
<b class="nc"><i>1932</i>&nbsp;    }</b>
<i>1933</i>&nbsp;
<i>1934</i>&nbsp;    // for testing
<i>1935</i>&nbsp;    final Map&lt;BytesRef, VersionValue&gt; getVersionMap() {
<b class="nc"><i>1936</i>&nbsp;        return Stream.concat(versionMap.getAllCurrent().entrySet().stream(), versionMap.getAllTombstones().entrySet().stream())</b>
<b class="nc"><i>1937</i>&nbsp;            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));</b>
<i>1938</i>&nbsp;    }
<i>1939</i>&nbsp;
<i>1940</i>&nbsp;    @Override
<i>1941</i>&nbsp;    public void forceMerge(final boolean flush, int maxNumSegments, boolean onlyExpungeDeletes,
<i>1942</i>&nbsp;                           final boolean upgrade, final boolean upgradeOnlyAncientSegments) throws EngineException, IOException {
<i>1943</i>&nbsp;        /*
<i>1944</i>&nbsp;         * We do NOT acquire the readlock here since we are waiting on the merges to finish
<i>1945</i>&nbsp;         * that&#39;s fine since the IW.rollback should stop all the threads and trigger an IOException
<i>1946</i>&nbsp;         * causing us to fail the forceMerge
<i>1947</i>&nbsp;         *
<i>1948</i>&nbsp;         * The way we implement upgrades is a bit hackish in the sense that we set an instance
<i>1949</i>&nbsp;         * variable and that this setting will thus apply to the next forced merge that will be run.
<i>1950</i>&nbsp;         * This is ok because (1) this is the only place we call forceMerge, (2) we have a single
<i>1951</i>&nbsp;         * thread for optimize, and the &#39;optimizeLock&#39; guarding this code, and (3) ConcurrentMergeScheduler
<i>1952</i>&nbsp;         * syncs calls to findForcedMerges.
<i>1953</i>&nbsp;         */
<b class="nc"><i>1954</i>&nbsp;        assert indexWriter.getConfig().getMergePolicy() instanceof ElasticsearchMergePolicy : &quot;MergePolicy is &quot; +</b>
<b class="nc"><i>1955</i>&nbsp;            indexWriter.getConfig().getMergePolicy().getClass().getName();</b>
<b class="nc"><i>1956</i>&nbsp;        ElasticsearchMergePolicy mp = (ElasticsearchMergePolicy) indexWriter.getConfig().getMergePolicy();</b>
<b class="nc"><i>1957</i>&nbsp;        optimizeLock.lock();</b>
<i>1958</i>&nbsp;        try {
<b class="nc"><i>1959</i>&nbsp;            ensureOpen();</b>
<b class="nc"><i>1960</i>&nbsp;            if (upgrade) {</b>
<b class="nc"><i>1961</i>&nbsp;                logger.info(&quot;starting segment upgrade upgradeOnlyAncientSegments={}&quot;, upgradeOnlyAncientSegments);</b>
<b class="nc"><i>1962</i>&nbsp;                mp.setUpgradeInProgress(true, upgradeOnlyAncientSegments);</b>
<i>1963</i>&nbsp;            }
<b class="nc"><i>1964</i>&nbsp;            store.incRef(); // increment the ref just to ensure nobody closes the store while we optimize</b>
<i>1965</i>&nbsp;            try {
<b class="nc"><i>1966</i>&nbsp;                if (onlyExpungeDeletes) {</b>
<b class="nc"><i>1967</i>&nbsp;                    assert upgrade == false;</b>
<b class="nc"><i>1968</i>&nbsp;                    indexWriter.forceMergeDeletes(true /* blocks and waits for merges*/);</b>
<b class="nc"><i>1969</i>&nbsp;                } else if (maxNumSegments &lt;= 0) {</b>
<b class="nc"><i>1970</i>&nbsp;                    assert upgrade == false;</b>
<b class="nc"><i>1971</i>&nbsp;                    indexWriter.maybeMerge();</b>
<i>1972</i>&nbsp;                } else {
<b class="nc"><i>1973</i>&nbsp;                    indexWriter.forceMerge(maxNumSegments, true /* blocks and waits for merges*/);</b>
<i>1974</i>&nbsp;                }
<b class="nc"><i>1975</i>&nbsp;                if (flush) {</b>
<b class="nc"><i>1976</i>&nbsp;                    if (tryRenewSyncCommit() == false) {</b>
<b class="nc"><i>1977</i>&nbsp;                        flush(false, true);</b>
<i>1978</i>&nbsp;                    }
<i>1979</i>&nbsp;                }
<b class="nc"><i>1980</i>&nbsp;                if (upgrade) {</b>
<b class="nc"><i>1981</i>&nbsp;                    logger.info(&quot;finished segment upgrade&quot;);</b>
<i>1982</i>&nbsp;                }
<i>1983</i>&nbsp;            } finally {
<b class="nc"><i>1984</i>&nbsp;                store.decRef();</b>
<b class="nc"><i>1985</i>&nbsp;            }</b>
<b class="nc"><i>1986</i>&nbsp;        } catch (AlreadyClosedException ex) {</b>
<i>1987</i>&nbsp;            /* in this case we first check if the engine is still open. If so this exception is just fine
<i>1988</i>&nbsp;             * and expected. We don&#39;t hold any locks while we block on forceMerge otherwise it would block
<i>1989</i>&nbsp;             * closing the engine as well. If we are not closed we pass it on to failOnTragicEvent which ensures
<i>1990</i>&nbsp;             * we are handling a tragic even exception here */
<b class="nc"><i>1991</i>&nbsp;            ensureOpen(ex);</b>
<b class="nc"><i>1992</i>&nbsp;            failOnTragicEvent(ex);</b>
<b class="nc"><i>1993</i>&nbsp;            throw ex;</b>
<b class="nc"><i>1994</i>&nbsp;        } catch (Exception e) {</b>
<i>1995</i>&nbsp;            try {
<b class="nc"><i>1996</i>&nbsp;                maybeFailEngine(&quot;force merge&quot;, e);</b>
<b class="nc"><i>1997</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>1998</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>1999</i>&nbsp;            }</b>
<b class="nc"><i>2000</i>&nbsp;            throw e;</b>
<i>2001</i>&nbsp;        } finally {
<b class="nc"><i>2002</i>&nbsp;            try {</b>
<i>2003</i>&nbsp;                // reset it just to make sure we reset it in a case of an error
<b class="nc"><i>2004</i>&nbsp;                mp.setUpgradeInProgress(false, false);</b>
<i>2005</i>&nbsp;            } finally {
<b class="nc"><i>2006</i>&nbsp;                optimizeLock.unlock();</b>
<b class="nc"><i>2007</i>&nbsp;            }</b>
<b class="nc"><i>2008</i>&nbsp;        }</b>
<b class="nc"><i>2009</i>&nbsp;    }</b>
<i>2010</i>&nbsp;
<i>2011</i>&nbsp;    @Override
<i>2012</i>&nbsp;    public IndexCommitRef acquireLastIndexCommit(final boolean flushFirst) throws EngineException {
<i>2013</i>&nbsp;        // we have to flush outside of the readlock otherwise we might have a problem upgrading
<i>2014</i>&nbsp;        // the to a write lock when we fail the engine in this operation
<b class="nc"><i>2015</i>&nbsp;        if (flushFirst) {</b>
<b class="nc"><i>2016</i>&nbsp;            logger.trace(&quot;start flush for snapshot&quot;);</b>
<b class="nc"><i>2017</i>&nbsp;            flush(false, true);</b>
<b class="nc"><i>2018</i>&nbsp;            logger.trace(&quot;finish flush for snapshot&quot;);</b>
<i>2019</i>&nbsp;        }
<b class="nc"><i>2020</i>&nbsp;        final IndexCommit lastCommit = combinedDeletionPolicy.acquireIndexCommit(false);</b>
<b class="nc"><i>2021</i>&nbsp;        return new Engine.IndexCommitRef(lastCommit, () -&gt; releaseIndexCommit(lastCommit));</b>
<i>2022</i>&nbsp;    }
<i>2023</i>&nbsp;
<i>2024</i>&nbsp;    @Override
<i>2025</i>&nbsp;    public IndexCommitRef acquireSafeIndexCommit() throws EngineException {
<b class="nc"><i>2026</i>&nbsp;        final IndexCommit safeCommit = combinedDeletionPolicy.acquireIndexCommit(true);</b>
<b class="nc"><i>2027</i>&nbsp;        return new Engine.IndexCommitRef(safeCommit, () -&gt; releaseIndexCommit(safeCommit));</b>
<i>2028</i>&nbsp;    }
<i>2029</i>&nbsp;
<i>2030</i>&nbsp;    private void releaseIndexCommit(IndexCommit snapshot) throws IOException {
<i>2031</i>&nbsp;        // Revisit the deletion policy if we can clean up the snapshotting commit.
<b class="nc"><i>2032</i>&nbsp;        if (combinedDeletionPolicy.releaseCommit(snapshot)) {</b>
<b class="nc"><i>2033</i>&nbsp;            ensureOpen();</b>
<i>2034</i>&nbsp;            // Here we don&#39;t have to trim translog because snapshotting an index commit
<i>2035</i>&nbsp;            // does not lock translog or prevents unreferenced files from trimming.
<b class="nc"><i>2036</i>&nbsp;            indexWriter.deleteUnusedFiles();</b>
<i>2037</i>&nbsp;        }
<b class="nc"><i>2038</i>&nbsp;    }</b>
<i>2039</i>&nbsp;
<i>2040</i>&nbsp;    @Override
<i>2041</i>&nbsp;    public SafeCommitInfo getSafeCommitInfo() {
<b class="nc"><i>2042</i>&nbsp;        return combinedDeletionPolicy.getSafeCommitInfo();</b>
<i>2043</i>&nbsp;    }
<i>2044</i>&nbsp;
<i>2045</i>&nbsp;    private boolean failOnTragicEvent(AlreadyClosedException ex) {
<i>2046</i>&nbsp;        final boolean engineFailed;
<i>2047</i>&nbsp;        // if we are already closed due to some tragic exception
<i>2048</i>&nbsp;        // we need to fail the engine. it might have already been failed before
<i>2049</i>&nbsp;        // but we are double-checking it&#39;s failed and closed
<b class="nc"><i>2050</i>&nbsp;        if (indexWriter.isOpen() == false &amp;&amp; indexWriter.getTragicException() != null) {</b>
<i>2051</i>&nbsp;            final Exception tragicException;
<b class="nc"><i>2052</i>&nbsp;            if (indexWriter.getTragicException() instanceof Exception) {</b>
<b class="nc"><i>2053</i>&nbsp;                tragicException = (Exception) indexWriter.getTragicException();</b>
<i>2054</i>&nbsp;            } else {
<b class="nc"><i>2055</i>&nbsp;                tragicException = new RuntimeException(indexWriter.getTragicException());</b>
<i>2056</i>&nbsp;            }
<b class="nc"><i>2057</i>&nbsp;            failEngine(&quot;already closed by tragic event on the index writer&quot;, tragicException);</b>
<b class="nc"><i>2058</i>&nbsp;            engineFailed = true;</b>
<b class="nc"><i>2059</i>&nbsp;        } else if (translog.isOpen() == false &amp;&amp; translog.getTragicException() != null) {</b>
<b class="nc"><i>2060</i>&nbsp;            failEngine(&quot;already closed by tragic event on the translog&quot;, translog.getTragicException());</b>
<b class="nc"><i>2061</i>&nbsp;            engineFailed = true;</b>
<b class="nc"><i>2062</i>&nbsp;        } else if (failedEngine.get() == null &amp;&amp; isClosed.get() == false) { // we are closed but the engine is not failed yet?</b>
<i>2063</i>&nbsp;            // this smells like a bug - we only expect ACE if we are in a fatal case ie. either translog or IW is closed by
<i>2064</i>&nbsp;            // a tragic event or has closed itself. if that is not the case we are in a buggy state and raise an assertion error
<b class="nc"><i>2065</i>&nbsp;            throw new AssertionError(&quot;Unexpected AlreadyClosedException&quot;, ex);</b>
<i>2066</i>&nbsp;        } else {
<b class="nc"><i>2067</i>&nbsp;            engineFailed = false;</b>
<i>2068</i>&nbsp;        }
<b class="nc"><i>2069</i>&nbsp;        return engineFailed;</b>
<i>2070</i>&nbsp;    }
<i>2071</i>&nbsp;
<i>2072</i>&nbsp;    @Override
<i>2073</i>&nbsp;    protected boolean maybeFailEngine(String source, Exception e) {
<b class="nc"><i>2074</i>&nbsp;        boolean shouldFail = super.maybeFailEngine(source, e);</b>
<b class="nc"><i>2075</i>&nbsp;        if (shouldFail) {</b>
<b class="nc"><i>2076</i>&nbsp;            return true;</b>
<i>2077</i>&nbsp;        }
<i>2078</i>&nbsp;        // Check for AlreadyClosedException -- ACE is a very special
<i>2079</i>&nbsp;        // exception that should only be thrown in a tragic event. we pass on the checks to failOnTragicEvent which will
<i>2080</i>&nbsp;        // throw and AssertionError if the tragic event condition is not met.
<b class="nc"><i>2081</i>&nbsp;        if (e instanceof AlreadyClosedException) {</b>
<b class="nc"><i>2082</i>&nbsp;            return failOnTragicEvent((AlreadyClosedException)e);</b>
<b class="nc"><i>2083</i>&nbsp;        } else if (e != null &amp;&amp;</b>
<b class="nc"><i>2084</i>&nbsp;                ((indexWriter.isOpen() == false &amp;&amp; indexWriter.getTragicException() == e)</b>
<b class="nc"><i>2085</i>&nbsp;                        || (translog.isOpen() == false &amp;&amp; translog.getTragicException() == e))) {</b>
<i>2086</i>&nbsp;            // this spot on - we are handling the tragic event exception here so we have to fail the engine
<i>2087</i>&nbsp;            // right away
<b class="nc"><i>2088</i>&nbsp;            failEngine(source, e);</b>
<b class="nc"><i>2089</i>&nbsp;            return true;</b>
<i>2090</i>&nbsp;        }
<b class="nc"><i>2091</i>&nbsp;        return false;</b>
<i>2092</i>&nbsp;    }
<i>2093</i>&nbsp;
<i>2094</i>&nbsp;    @Override
<i>2095</i>&nbsp;    protected SegmentInfos getLastCommittedSegmentInfos() {
<b class="nc"><i>2096</i>&nbsp;        return lastCommittedSegmentInfos;</b>
<i>2097</i>&nbsp;    }
<i>2098</i>&nbsp;
<i>2099</i>&nbsp;    @Override
<i>2100</i>&nbsp;    protected final void writerSegmentStats(SegmentsStats stats) {
<b class="nc"><i>2101</i>&nbsp;        stats.addVersionMapMemoryInBytes(versionMap.ramBytesUsed());</b>
<b class="nc"><i>2102</i>&nbsp;        stats.addIndexWriterMemoryInBytes(indexWriter.ramBytesUsed());</b>
<b class="nc"><i>2103</i>&nbsp;        stats.updateMaxUnsafeAutoIdTimestamp(maxUnsafeAutoIdTimestamp.get());</b>
<b class="nc"><i>2104</i>&nbsp;    }</b>
<i>2105</i>&nbsp;
<i>2106</i>&nbsp;    @Override
<i>2107</i>&nbsp;    public long getIndexBufferRAMBytesUsed() {
<i>2108</i>&nbsp;        // We don&#39;t guard w/ readLock here, so we could throw AlreadyClosedException
<b class="fc"><i>2109</i>&nbsp;        return indexWriter.ramBytesUsed() + versionMap.ramBytesUsedForRefresh();</b>
<i>2110</i>&nbsp;    }
<i>2111</i>&nbsp;
<i>2112</i>&nbsp;    @Override
<i>2113</i>&nbsp;    public List&lt;Segment&gt; segments(boolean verbose) {
<b class="nc"><i>2114</i>&nbsp;        try (ReleasableLock lock = readLock.acquire()) {</b>
<b class="nc"><i>2115</i>&nbsp;            Segment[] segmentsArr = getSegmentInfo(lastCommittedSegmentInfos, verbose);</b>
<i>2116</i>&nbsp;
<i>2117</i>&nbsp;            // fill in the merges flag
<b class="nc"><i>2118</i>&nbsp;            Set&lt;OnGoingMerge&gt; onGoingMerges = mergeScheduler.onGoingMerges();</b>
<b class="nc"><i>2119</i>&nbsp;            for (OnGoingMerge onGoingMerge : onGoingMerges) {</b>
<b class="nc"><i>2120</i>&nbsp;                for (SegmentCommitInfo segmentInfoPerCommit : onGoingMerge.getMergedSegments()) {</b>
<b class="nc"><i>2121</i>&nbsp;                    for (Segment segment : segmentsArr) {</b>
<b class="nc"><i>2122</i>&nbsp;                        if (segment.getName().equals(segmentInfoPerCommit.info.name)) {</b>
<b class="nc"><i>2123</i>&nbsp;                            segment.mergeId = onGoingMerge.getId();</b>
<b class="nc"><i>2124</i>&nbsp;                            break;</b>
<i>2125</i>&nbsp;                        }
<i>2126</i>&nbsp;                    }
<b class="nc"><i>2127</i>&nbsp;                }</b>
<b class="nc"><i>2128</i>&nbsp;            }</b>
<b class="nc"><i>2129</i>&nbsp;            return Arrays.asList(segmentsArr);</b>
<b class="nc"><i>2130</i>&nbsp;        }</b>
<i>2131</i>&nbsp;    }
<i>2132</i>&nbsp;
<i>2133</i>&nbsp;    /**
<i>2134</i>&nbsp;     * Closes the engine without acquiring the write lock. This should only be
<i>2135</i>&nbsp;     * called while the write lock is hold or in a disaster condition ie. if the engine
<i>2136</i>&nbsp;     * is failed.
<i>2137</i>&nbsp;     */
<i>2138</i>&nbsp;    @Override
<i>2139</i>&nbsp;    protected final void closeNoLock(String reason, CountDownLatch closedLatch) {
<b class="fc"><i>2140</i>&nbsp;        if (isClosed.compareAndSet(false, true)) {</b>
<b class="fc"><i>2141</i>&nbsp;            assert rwl.isWriteLockedByCurrentThread() || failEngineLock.isHeldByCurrentThread() :</b>
<i>2142</i>&nbsp;                &quot;Either the write lock must be held or the engine must be currently be failing itself&quot;;
<i>2143</i>&nbsp;            try {
<b class="fc"><i>2144</i>&nbsp;                this.versionMap.clear();</b>
<b class="fc"><i>2145</i>&nbsp;                if (internalReaderManager != null) {</b>
<b class="fc"><i>2146</i>&nbsp;                    internalReaderManager.removeListener(versionMap);</b>
<i>2147</i>&nbsp;                }
<i>2148</i>&nbsp;                try {
<b class="fc"><i>2149</i>&nbsp;                    IOUtils.close(externalReaderManager, internalReaderManager);</b>
<b class="nc"><i>2150</i>&nbsp;                } catch (Exception e) {</b>
<b class="nc"><i>2151</i>&nbsp;                    logger.warn(&quot;Failed to close ReaderManager&quot;, e);</b>
<b class="fc"><i>2152</i>&nbsp;                }</b>
<i>2153</i>&nbsp;                try {
<b class="fc"><i>2154</i>&nbsp;                    IOUtils.close(translog);</b>
<b class="nc"><i>2155</i>&nbsp;                } catch (Exception e) {</b>
<b class="nc"><i>2156</i>&nbsp;                    logger.warn(&quot;Failed to close translog&quot;, e);</b>
<b class="fc"><i>2157</i>&nbsp;                }</b>
<i>2158</i>&nbsp;                // no need to commit in this case!, we snapshot before we close the shard, so translog and all sync&#39;ed
<b class="fc"><i>2159</i>&nbsp;                logger.trace(&quot;rollback indexWriter&quot;);</b>
<i>2160</i>&nbsp;                try {
<b class="fc"><i>2161</i>&nbsp;                    indexWriter.rollback();</b>
<b class="nc"><i>2162</i>&nbsp;                } catch (AlreadyClosedException ex) {</b>
<b class="nc"><i>2163</i>&nbsp;                    failOnTragicEvent(ex);</b>
<b class="nc"><i>2164</i>&nbsp;                    throw ex;</b>
<b class="fc"><i>2165</i>&nbsp;                }</b>
<b class="fc"><i>2166</i>&nbsp;                logger.trace(&quot;rollback indexWriter done&quot;);</b>
<b class="nc"><i>2167</i>&nbsp;            } catch (Exception e) {</b>
<b class="nc"><i>2168</i>&nbsp;                logger.warn(&quot;failed to rollback writer on close&quot;, e);</b>
<i>2169</i>&nbsp;            } finally {
<b class="nc"><i>2170</i>&nbsp;                try {</b>
<b class="fc"><i>2171</i>&nbsp;                    store.decRef();</b>
<b class="fc"><i>2172</i>&nbsp;                    logger.debug(&quot;engine closed [{}]&quot;, reason);</b>
<i>2173</i>&nbsp;                } finally {
<b class="fc"><i>2174</i>&nbsp;                    closedLatch.countDown();</b>
<b class="fc"><i>2175</i>&nbsp;                }</b>
<b class="fc"><i>2176</i>&nbsp;            }</b>
<i>2177</i>&nbsp;        }
<b class="fc"><i>2178</i>&nbsp;    }</b>
<i>2179</i>&nbsp;
<i>2180</i>&nbsp;    @Override
<i>2181</i>&nbsp;    protected final ReferenceManager&lt;ElasticsearchDirectoryReader&gt; getReferenceManager(SearcherScope scope) {
<b class="fc"><i>2182</i>&nbsp;        switch (scope) {</b>
<i>2183</i>&nbsp;            case INTERNAL:
<b class="nc"><i>2184</i>&nbsp;                return internalReaderManager;</b>
<i>2185</i>&nbsp;            case EXTERNAL:
<b class="fc"><i>2186</i>&nbsp;                return externalReaderManager;</b>
<i>2187</i>&nbsp;            default:
<b class="nc"><i>2188</i>&nbsp;                throw new IllegalStateException(&quot;unknown scope: &quot; + scope);</b>
<i>2189</i>&nbsp;        }
<i>2190</i>&nbsp;    }
<i>2191</i>&nbsp;
<i>2192</i>&nbsp;    private IndexWriter createWriter() throws IOException {
<i>2193</i>&nbsp;        try {
<b class="fc"><i>2194</i>&nbsp;            final IndexWriterConfig iwc = getIndexWriterConfig();</b>
<b class="fc"><i>2195</i>&nbsp;            return createWriter(store.directory(), iwc);</b>
<b class="nc"><i>2196</i>&nbsp;        } catch (LockObtainFailedException ex) {</b>
<b class="nc"><i>2197</i>&nbsp;            logger.warn(&quot;could not lock IndexWriter&quot;, ex);</b>
<b class="nc"><i>2198</i>&nbsp;            throw ex;</b>
<i>2199</i>&nbsp;        }
<i>2200</i>&nbsp;    }
<i>2201</i>&nbsp;
<i>2202</i>&nbsp;    // pkg-private for testing
<i>2203</i>&nbsp;    IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException {
<b class="fc"><i>2204</i>&nbsp;        if (Assertions.ENABLED) {</b>
<b class="fc"><i>2205</i>&nbsp;            return new AssertingIndexWriter(directory, iwc);</b>
<i>2206</i>&nbsp;        } else {
<b class="nc"><i>2207</i>&nbsp;            return new IndexWriter(directory, iwc);</b>
<i>2208</i>&nbsp;        }
<i>2209</i>&nbsp;    }
<i>2210</i>&nbsp;
<i>2211</i>&nbsp;    static Map&lt;String, String&gt; getReaderAttributes(Directory directory) {
<b class="fc"><i>2212</i>&nbsp;        Directory unwrap = FilterDirectory.unwrap(directory);</b>
<b class="fc"><i>2213</i>&nbsp;        boolean defaultOffHeap = FsDirectoryFactory.isHybridFs(unwrap) || unwrap instanceof MMapDirectory;</b>
<b class="fc"><i>2214</i>&nbsp;        HashMap&lt;String, String&gt; map = new HashMap(2);</b>
<b class="fc"><i>2215</i>&nbsp;        map.put(BlockTreeTermsReader.FST_MODE_KEY, // if we are using MMAP for term dics we force all off heap unless it&#39;s the ID field</b>
<b class="fc"><i>2216</i>&nbsp;            defaultOffHeap ? FSTLoadMode.OFF_HEAP.name() : FSTLoadMode.ON_HEAP.name());</b>
<b class="fc"><i>2217</i>&nbsp;        map.put(BlockTreeTermsReader.FST_MODE_KEY + &quot;.&quot; + IdFieldMapper.NAME, // always force ID field on-heap for fast updates</b>
<b class="fc"><i>2218</i>&nbsp;            FSTLoadMode.ON_HEAP.name());</b>
<b class="fc"><i>2219</i>&nbsp;        return Collections.unmodifiableMap(map);</b>
<i>2220</i>&nbsp;    }
<i>2221</i>&nbsp;
<i>2222</i>&nbsp;    private IndexWriterConfig getIndexWriterConfig() {
<b class="fc"><i>2223</i>&nbsp;        final IndexWriterConfig iwc = new IndexWriterConfig(engineConfig.getAnalyzer());</b>
<b class="fc"><i>2224</i>&nbsp;        iwc.setCommitOnClose(false); // we by default don&#39;t commit on close</b>
<b class="fc"><i>2225</i>&nbsp;        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);</b>
<b class="fc"><i>2226</i>&nbsp;        iwc.setReaderAttributes(getReaderAttributes(store.directory()));</b>
<b class="fc"><i>2227</i>&nbsp;        iwc.setIndexDeletionPolicy(combinedDeletionPolicy);</b>
<i>2228</i>&nbsp;        // with tests.verbose, lucene sets this up: plumb to align with filesystem stream
<b class="fc"><i>2229</i>&nbsp;        boolean verbose = false;</b>
<i>2230</i>&nbsp;        try {
<b class="fc"><i>2231</i>&nbsp;            verbose = Boolean.parseBoolean(System.getProperty(&quot;tests.verbose&quot;));</b>
<b class="nc"><i>2232</i>&nbsp;        } catch (Exception ignore) {</b>
<b class="fc"><i>2233</i>&nbsp;        }</b>
<b class="fc"><i>2234</i>&nbsp;        iwc.setInfoStream(verbose ? InfoStream.getDefault() : new LoggerInfoStream(logger));</b>
<b class="fc"><i>2235</i>&nbsp;        iwc.setMergeScheduler(mergeScheduler);</b>
<i>2236</i>&nbsp;        // Give us the opportunity to upgrade old segments while performing
<i>2237</i>&nbsp;        // background merges
<b class="fc"><i>2238</i>&nbsp;        MergePolicy mergePolicy = config().getMergePolicy();</b>
<i>2239</i>&nbsp;        // always configure soft-deletes field so an engine with soft-deletes disabled can open a Lucene index with soft-deletes.
<b class="fc"><i>2240</i>&nbsp;        iwc.setSoftDeletesField(Lucene.SOFT_DELETES_FIELD);</b>
<b class="fc"><i>2241</i>&nbsp;        if (softDeleteEnabled) {</b>
<b class="nc"><i>2242</i>&nbsp;            mergePolicy = new RecoverySourcePruneMergePolicy(SourceFieldMapper.RECOVERY_SOURCE_NAME, softDeletesPolicy::getRetentionQuery,</b>
<b class="nc"><i>2243</i>&nbsp;                new SoftDeletesRetentionMergePolicy(Lucene.SOFT_DELETES_FIELD, softDeletesPolicy::getRetentionQuery,</b>
<i>2244</i>&nbsp;                    new PrunePostingsMergePolicy(mergePolicy, IdFieldMapper.NAME)));
<i>2245</i>&nbsp;        }
<b class="fc"><i>2246</i>&nbsp;        iwc.setMergePolicy(new ElasticsearchMergePolicy(mergePolicy));</b>
<b class="fc"><i>2247</i>&nbsp;        iwc.setSimilarity(engineConfig.getSimilarity());</b>
<b class="fc"><i>2248</i>&nbsp;        iwc.setRAMBufferSizeMB(engineConfig.getIndexingBufferSize().getMbFrac());</b>
<b class="fc"><i>2249</i>&nbsp;        iwc.setCodec(engineConfig.getCodec());</b>
<b class="fc"><i>2250</i>&nbsp;        iwc.setUseCompoundFile(true); // always use compound on flush - reduces # of file-handles on refresh</b>
<b class="fc"><i>2251</i>&nbsp;        if (config().getIndexSort() != null) {</b>
<b class="nc"><i>2252</i>&nbsp;            iwc.setIndexSort(config().getIndexSort());</b>
<i>2253</i>&nbsp;        }
<b class="fc"><i>2254</i>&nbsp;        return iwc;</b>
<i>2255</i>&nbsp;    }
<i>2256</i>&nbsp;
<i>2257</i>&nbsp;    /** A listener that warms the segments if needed when acquiring a new reader */
<i>2258</i>&nbsp;    static final class RefreshWarmerListener implements BiConsumer&lt;ElasticsearchDirectoryReader, ElasticsearchDirectoryReader&gt; {
<i>2259</i>&nbsp;        private final Engine.Warmer warmer;
<i>2260</i>&nbsp;        private final Logger logger;
<i>2261</i>&nbsp;        private final AtomicBoolean isEngineClosed;
<i>2262</i>&nbsp;
<b class="fc"><i>2263</i>&nbsp;        RefreshWarmerListener(Logger logger, AtomicBoolean isEngineClosed, EngineConfig engineConfig) {</b>
<b class="fc"><i>2264</i>&nbsp;            warmer = engineConfig.getWarmer();</b>
<b class="fc"><i>2265</i>&nbsp;            this.logger = logger;</b>
<b class="fc"><i>2266</i>&nbsp;            this.isEngineClosed = isEngineClosed;</b>
<b class="fc"><i>2267</i>&nbsp;        }</b>
<i>2268</i>&nbsp;
<i>2269</i>&nbsp;        @Override
<i>2270</i>&nbsp;        public void accept(ElasticsearchDirectoryReader reader, ElasticsearchDirectoryReader previousReader) {
<b class="fc"><i>2271</i>&nbsp;            if (warmer != null) {</b>
<i>2272</i>&nbsp;                try {
<b class="fc"><i>2273</i>&nbsp;                    warmer.warm(reader);</b>
<b class="nc"><i>2274</i>&nbsp;                } catch (Exception e) {</b>
<b class="nc"><i>2275</i>&nbsp;                    if (isEngineClosed.get() == false) {</b>
<b class="nc"><i>2276</i>&nbsp;                        logger.warn(&quot;failed to prepare/warm&quot;, e);</b>
<i>2277</i>&nbsp;                    }
<b class="fc"><i>2278</i>&nbsp;                }</b>
<i>2279</i>&nbsp;            }
<b class="fc"><i>2280</i>&nbsp;        }</b>
<i>2281</i>&nbsp;    }
<i>2282</i>&nbsp;
<i>2283</i>&nbsp;    @Override
<i>2284</i>&nbsp;    public void activateThrottling() {
<b class="nc"><i>2285</i>&nbsp;        int count = throttleRequestCount.incrementAndGet();</b>
<b class="nc"><i>2286</i>&nbsp;        assert count &gt;= 1 : &quot;invalid post-increment throttleRequestCount=&quot; + count;</b>
<b class="nc"><i>2287</i>&nbsp;        if (count == 1) {</b>
<b class="nc"><i>2288</i>&nbsp;            throttle.activate();</b>
<i>2289</i>&nbsp;        }
<b class="nc"><i>2290</i>&nbsp;    }</b>
<i>2291</i>&nbsp;
<i>2292</i>&nbsp;    @Override
<i>2293</i>&nbsp;    public void deactivateThrottling() {
<b class="nc"><i>2294</i>&nbsp;        int count = throttleRequestCount.decrementAndGet();</b>
<b class="nc"><i>2295</i>&nbsp;        assert count &gt;= 0 : &quot;invalid post-decrement throttleRequestCount=&quot; + count;</b>
<b class="nc"><i>2296</i>&nbsp;        if (count == 0) {</b>
<b class="nc"><i>2297</i>&nbsp;            throttle.deactivate();</b>
<i>2298</i>&nbsp;        }
<b class="nc"><i>2299</i>&nbsp;    }</b>
<i>2300</i>&nbsp;
<i>2301</i>&nbsp;    @Override
<i>2302</i>&nbsp;    public boolean isThrottled() {
<b class="fc"><i>2303</i>&nbsp;        return throttle.isThrottled();</b>
<i>2304</i>&nbsp;    }
<i>2305</i>&nbsp;
<i>2306</i>&nbsp;    @Override
<i>2307</i>&nbsp;    public long getIndexThrottleTimeInMillis() {
<b class="fc"><i>2308</i>&nbsp;        return throttle.getThrottleTimeInMillis();</b>
<i>2309</i>&nbsp;    }
<i>2310</i>&nbsp;
<i>2311</i>&nbsp;    long getGcDeletesInMillis() {
<b class="fc"><i>2312</i>&nbsp;        return engineConfig.getIndexSettings().getGcDeletesInMillis();</b>
<i>2313</i>&nbsp;    }
<i>2314</i>&nbsp;
<i>2315</i>&nbsp;    LiveIndexWriterConfig getCurrentIndexWriterConfig() {
<b class="nc"><i>2316</i>&nbsp;        return indexWriter.getConfig();</b>
<i>2317</i>&nbsp;    }
<i>2318</i>&nbsp;
<i>2319</i>&nbsp;    private final class EngineMergeScheduler extends ElasticsearchConcurrentMergeScheduler {
<b class="fc"><i>2320</i>&nbsp;        private final AtomicInteger numMergesInFlight = new AtomicInteger(0);</b>
<b class="fc"><i>2321</i>&nbsp;        private final AtomicBoolean isThrottling = new AtomicBoolean();</b>
<i>2322</i>&nbsp;
<b class="fc"><i>2323</i>&nbsp;        EngineMergeScheduler(ShardId shardId, IndexSettings indexSettings) {</b>
<b class="fc"><i>2324</i>&nbsp;            super(shardId, indexSettings);</b>
<b class="fc"><i>2325</i>&nbsp;        }</b>
<i>2326</i>&nbsp;
<i>2327</i>&nbsp;        @Override
<i>2328</i>&nbsp;        public synchronized void beforeMerge(OnGoingMerge merge) {
<b class="nc"><i>2329</i>&nbsp;            int maxNumMerges = mergeScheduler.getMaxMergeCount();</b>
<b class="nc"><i>2330</i>&nbsp;            if (numMergesInFlight.incrementAndGet() &gt; maxNumMerges) {</b>
<b class="nc"><i>2331</i>&nbsp;                if (isThrottling.getAndSet(true) == false) {</b>
<b class="nc"><i>2332</i>&nbsp;                    logger.info(&quot;now throttling indexing: numMergesInFlight={}, maxNumMerges={}&quot;, numMergesInFlight, maxNumMerges);</b>
<b class="nc"><i>2333</i>&nbsp;                    activateThrottling();</b>
<i>2334</i>&nbsp;                }
<i>2335</i>&nbsp;            }
<b class="nc"><i>2336</i>&nbsp;        }</b>
<i>2337</i>&nbsp;
<i>2338</i>&nbsp;        @Override
<i>2339</i>&nbsp;        public synchronized void afterMerge(OnGoingMerge merge) {
<b class="nc"><i>2340</i>&nbsp;            int maxNumMerges = mergeScheduler.getMaxMergeCount();</b>
<b class="nc"><i>2341</i>&nbsp;            if (numMergesInFlight.decrementAndGet() &lt; maxNumMerges) {</b>
<b class="nc"><i>2342</i>&nbsp;                if (isThrottling.getAndSet(false)) {</b>
<b class="nc"><i>2343</i>&nbsp;                    logger.info(&quot;stop throttling indexing: numMergesInFlight={}, maxNumMerges={}&quot;,</b>
<b class="nc"><i>2344</i>&nbsp;                        numMergesInFlight, maxNumMerges);</b>
<b class="nc"><i>2345</i>&nbsp;                    deactivateThrottling();</b>
<i>2346</i>&nbsp;                }
<i>2347</i>&nbsp;            }
<b class="nc"><i>2348</i>&nbsp;            if (indexWriter.hasPendingMerges() == false &amp;&amp;</b>
<b class="nc"><i>2349</i>&nbsp;                    System.nanoTime() - lastWriteNanos &gt;= engineConfig.getFlushMergesAfter().nanos()) {</b>
<i>2350</i>&nbsp;                // NEVER do this on a merge thread since we acquire some locks blocking here and if we concurrently rollback the writer
<i>2351</i>&nbsp;                // we deadlock on engine#close for instance.
<b class="nc"><i>2352</i>&nbsp;                engineConfig.getThreadPool().executor(ThreadPool.Names.FLUSH).execute(new AbstractRunnable() {</b>
<i>2353</i>&nbsp;                    @Override
<i>2354</i>&nbsp;                    public void onFailure(Exception e) {
<i>2355</i>&nbsp;                        if (isClosed.get() == false) {
<i>2356</i>&nbsp;                            logger.warn(&quot;failed to flush after merge has finished&quot;);
<i>2357</i>&nbsp;                        }
<i>2358</i>&nbsp;                    }
<i>2359</i>&nbsp;
<i>2360</i>&nbsp;                    @Override
<i>2361</i>&nbsp;                    protected void doRun() {
<i>2362</i>&nbsp;                        // if we have no pending merges and we are supposed to flush once merges have finished
<i>2363</i>&nbsp;                        // we try to renew a sync commit which is the case when we are having a big merge after we
<i>2364</i>&nbsp;                        // are inactive. If that didn&#39;t work we go and do a real flush which is ok since it only doesn&#39;t work
<i>2365</i>&nbsp;                        // if we either have records in the translog or if we don&#39;t have a sync ID at all...
<i>2366</i>&nbsp;                        // maybe even more important, we flush after all merges finish and we are inactive indexing-wise to
<i>2367</i>&nbsp;                        // free up transient disk usage of the (presumably biggish) segments that were just merged
<i>2368</i>&nbsp;                        if (tryRenewSyncCommit() == false) {
<i>2369</i>&nbsp;                            flush();
<i>2370</i>&nbsp;                        }
<i>2371</i>&nbsp;                    }
<i>2372</i>&nbsp;                });
<b class="nc"><i>2373</i>&nbsp;            } else if (merge.getTotalBytesSize() &gt;= engineConfig.getIndexSettings().getFlushAfterMergeThresholdSize().getBytes()) {</b>
<i>2374</i>&nbsp;                // we hit a significant merge which would allow us to free up memory if we&#39;d commit it hence on the next change
<i>2375</i>&nbsp;                // we should execute a flush on the next operation if that&#39;s a flush after inactive or indexing a document.
<i>2376</i>&nbsp;                // we could fork a thread and do it right away but we try to minimize forking and piggyback on outside events.
<b class="nc"><i>2377</i>&nbsp;                shouldPeriodicallyFlushAfterBigMerge.set(true);</b>
<i>2378</i>&nbsp;            }
<b class="nc"><i>2379</i>&nbsp;        }</b>
<i>2380</i>&nbsp;
<i>2381</i>&nbsp;        @Override
<i>2382</i>&nbsp;        protected void handleMergeException(final Directory dir, final Throwable exc) {
<b class="nc"><i>2383</i>&nbsp;            engineConfig.getThreadPool().generic().execute(new AbstractRunnable() {</b>
<i>2384</i>&nbsp;                @Override
<i>2385</i>&nbsp;                public void onFailure(Exception e) {
<i>2386</i>&nbsp;                    logger.debug(&quot;merge failure action rejected&quot;, e);
<i>2387</i>&nbsp;                }
<i>2388</i>&nbsp;
<i>2389</i>&nbsp;                @Override
<i>2390</i>&nbsp;                protected void doRun() throws Exception {
<i>2391</i>&nbsp;                    /*
<i>2392</i>&nbsp;                     * We do this on another thread rather than the merge thread that we are initially called on so that we have complete
<i>2393</i>&nbsp;                     * confidence that the call stack does not contain catch statements that would cause the error that might be thrown
<i>2394</i>&nbsp;                     * here from being caught and never reaching the uncaught exception handler.
<i>2395</i>&nbsp;                     */
<i>2396</i>&nbsp;                    failEngine(&quot;merge failed&quot;, new MergePolicy.MergeException(exc, dir));
<i>2397</i>&nbsp;                }
<i>2398</i>&nbsp;            });
<b class="nc"><i>2399</i>&nbsp;        }</b>
<i>2400</i>&nbsp;    }
<i>2401</i>&nbsp;
<i>2402</i>&nbsp;    /**
<i>2403</i>&nbsp;     * Commits the specified index writer.
<i>2404</i>&nbsp;     *
<i>2405</i>&nbsp;     * @param writer   the index writer to commit
<i>2406</i>&nbsp;     * @param translog the translog
<i>2407</i>&nbsp;     * @param syncId   the sync flush ID ({@code null} if not committing a synced flush)
<i>2408</i>&nbsp;     * @throws IOException if an I/O exception occurs committing the specfied writer
<i>2409</i>&nbsp;     */
<i>2410</i>&nbsp;    protected void commitIndexWriter(final IndexWriter writer, final Translog translog, @Nullable final String syncId) throws IOException {
<b class="nc"><i>2411</i>&nbsp;        ensureCanFlush();</b>
<i>2412</i>&nbsp;        try {
<b class="nc"><i>2413</i>&nbsp;            final long localCheckpoint = localCheckpointTracker.getProcessedCheckpoint();</b>
<b class="nc"><i>2414</i>&nbsp;            final Translog.TranslogGeneration translogGeneration = translog.getMinGenerationForSeqNo(localCheckpoint + 1);</b>
<b class="nc"><i>2415</i>&nbsp;            final String translogFileGeneration = Long.toString(translogGeneration.translogFileGeneration);</b>
<b class="nc"><i>2416</i>&nbsp;            final String translogUUID = translogGeneration.translogUUID;</b>
<b class="nc"><i>2417</i>&nbsp;            final String localCheckpointValue = Long.toString(localCheckpoint);</b>
<i>2418</i>&nbsp;
<b class="nc"><i>2419</i>&nbsp;            writer.setLiveCommitData(() -&gt; {</b>
<i>2420</i>&nbsp;                /*
<i>2421</i>&nbsp;                 * The user data captured above (e.g. local checkpoint) contains data that must be evaluated *before* Lucene flushes
<i>2422</i>&nbsp;                 * segments, including the local checkpoint amongst other values. The maximum sequence number is different, we never want
<i>2423</i>&nbsp;                 * the maximum sequence number to be less than the last sequence number to go into a Lucene commit, otherwise we run the
<i>2424</i>&nbsp;                 * risk of re-using a sequence number for two different documents when restoring from this commit point and subsequently
<i>2425</i>&nbsp;                 * writing new documents to the index. Since we only know which Lucene documents made it into the final commit after the
<i>2426</i>&nbsp;                 * {@link IndexWriter#commit()} call flushes all documents, we defer computation of the maximum sequence number to the time
<i>2427</i>&nbsp;                 * of invocation of the commit data iterator (which occurs after all documents have been flushed to Lucene).
<i>2428</i>&nbsp;                 */
<b class="nc"><i>2429</i>&nbsp;                final Map&lt;String, String&gt; commitData = new HashMap&lt;&gt;(8);</b>
<b class="nc"><i>2430</i>&nbsp;                commitData.put(Translog.TRANSLOG_GENERATION_KEY, translogFileGeneration);</b>
<b class="nc"><i>2431</i>&nbsp;                commitData.put(Translog.TRANSLOG_UUID_KEY, translogUUID);</b>
<b class="nc"><i>2432</i>&nbsp;                commitData.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, localCheckpointValue);</b>
<b class="nc"><i>2433</i>&nbsp;                if (syncId != null) {</b>
<b class="nc"><i>2434</i>&nbsp;                    commitData.put(Engine.SYNC_COMMIT_ID, syncId);</b>
<i>2435</i>&nbsp;                }
<b class="nc"><i>2436</i>&nbsp;                commitData.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(localCheckpointTracker.getMaxSeqNo()));</b>
<b class="nc"><i>2437</i>&nbsp;                commitData.put(MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, Long.toString(maxUnsafeAutoIdTimestamp.get()));</b>
<b class="nc"><i>2438</i>&nbsp;                commitData.put(HISTORY_UUID_KEY, historyUUID);</b>
<b class="nc"><i>2439</i>&nbsp;                if (softDeleteEnabled) {</b>
<b class="nc"><i>2440</i>&nbsp;                    commitData.put(Engine.MIN_RETAINED_SEQNO, Long.toString(softDeletesPolicy.getMinRetainedSeqNo()));</b>
<i>2441</i>&nbsp;                }
<b class="nc"><i>2442</i>&nbsp;                logger.trace(&quot;committing writer with commit data [{}]&quot;, commitData);</b>
<b class="nc"><i>2443</i>&nbsp;                return commitData.entrySet().iterator();</b>
<i>2444</i>&nbsp;            });
<b class="nc"><i>2445</i>&nbsp;            shouldPeriodicallyFlushAfterBigMerge.set(false);</b>
<b class="nc"><i>2446</i>&nbsp;            writer.commit();</b>
<b class="nc"><i>2447</i>&nbsp;        } catch (final Exception ex) {</b>
<i>2448</i>&nbsp;            try {
<b class="nc"><i>2449</i>&nbsp;                failEngine(&quot;lucene commit failed&quot;, ex);</b>
<b class="nc"><i>2450</i>&nbsp;            } catch (final Exception inner) {</b>
<b class="nc"><i>2451</i>&nbsp;                ex.addSuppressed(inner);</b>
<b class="nc"><i>2452</i>&nbsp;            }</b>
<b class="nc"><i>2453</i>&nbsp;            throw ex;</b>
<b class="nc"><i>2454</i>&nbsp;        } catch (final AssertionError e) {</b>
<i>2455</i>&nbsp;            /*
<i>2456</i>&nbsp;             * If assertions are enabled, IndexWriter throws AssertionError on commit if any files don&#39;t exist, but tests that randomly
<i>2457</i>&nbsp;             * throw FileNotFoundException or NoSuchFileException can also hit this.
<i>2458</i>&nbsp;             */
<b class="nc"><i>2459</i>&nbsp;            if (ExceptionsHelper.stackTrace(e).contains(&quot;org.apache.lucene.index.IndexWriter.filesExist&quot;)) {</b>
<b class="nc"><i>2460</i>&nbsp;                final EngineException engineException = new EngineException(shardId, &quot;failed to commit engine&quot;, e);</b>
<i>2461</i>&nbsp;                try {
<b class="nc"><i>2462</i>&nbsp;                    failEngine(&quot;lucene commit failed&quot;, engineException);</b>
<b class="nc"><i>2463</i>&nbsp;                } catch (final Exception inner) {</b>
<b class="nc"><i>2464</i>&nbsp;                    engineException.addSuppressed(inner);</b>
<b class="nc"><i>2465</i>&nbsp;                }</b>
<b class="nc"><i>2466</i>&nbsp;                throw engineException;</b>
<i>2467</i>&nbsp;            } else {
<b class="nc"><i>2468</i>&nbsp;                throw e;</b>
<i>2469</i>&nbsp;            }
<b class="nc"><i>2470</i>&nbsp;        }</b>
<b class="nc"><i>2471</i>&nbsp;    }</b>
<i>2472</i>&nbsp;
<i>2473</i>&nbsp;    final void ensureCanFlush() {
<i>2474</i>&nbsp;        // translog recovery happens after the engine is fully constructed.
<i>2475</i>&nbsp;        // If we are in this stage we have to prevent flushes from this
<i>2476</i>&nbsp;        // engine otherwise we might loose documents if the flush succeeds
<i>2477</i>&nbsp;        // and the translog recovery fails when we &quot;commit&quot; the translog on flush.
<b class="nc"><i>2478</i>&nbsp;        if (pendingTranslogRecovery.get()) {</b>
<b class="nc"><i>2479</i>&nbsp;            throw new IllegalStateException(shardId.toString() + &quot; flushes are disabled - pending translog recovery&quot;);</b>
<i>2480</i>&nbsp;        }
<b class="nc"><i>2481</i>&nbsp;    }</b>
<i>2482</i>&nbsp;
<i>2483</i>&nbsp;    @Override
<i>2484</i>&nbsp;    public void onSettingsChanged(TimeValue translogRetentionAge, ByteSizeValue translogRetentionSize, long softDeletesRetentionOps) {
<b class="fc"><i>2485</i>&nbsp;        mergeScheduler.refreshConfig();</b>
<i>2486</i>&nbsp;        // config().isEnableGcDeletes() or config.getGcDeletesInMillis() may have changed:
<b class="fc"><i>2487</i>&nbsp;        maybePruneDeletes();</b>
<b class="fc"><i>2488</i>&nbsp;        if (engineConfig.isAutoGeneratedIDsOptimizationEnabled() == false) {</b>
<i>2489</i>&nbsp;            // this is an anti-viral settings you can only opt out for the entire index
<i>2490</i>&nbsp;            // only if a shard starts up again due to relocation or if the index is closed
<i>2491</i>&nbsp;            // the setting will be re-interpreted if it&#39;s set to true
<b class="nc"><i>2492</i>&nbsp;            updateAutoIdTimestamp(Long.MAX_VALUE, true);</b>
<i>2493</i>&nbsp;        }
<b class="fc"><i>2494</i>&nbsp;        final TranslogDeletionPolicy translogDeletionPolicy = translog.getDeletionPolicy();</b>
<b class="fc"><i>2495</i>&nbsp;        translogDeletionPolicy.setRetentionAgeInMillis(translogRetentionAge.millis());</b>
<b class="fc"><i>2496</i>&nbsp;        translogDeletionPolicy.setRetentionSizeInBytes(translogRetentionSize.getBytes());</b>
<b class="fc"><i>2497</i>&nbsp;        softDeletesPolicy.setRetentionOperations(softDeletesRetentionOps);</b>
<b class="fc"><i>2498</i>&nbsp;    }</b>
<i>2499</i>&nbsp;
<i>2500</i>&nbsp;    public MergeStats getMergeStats() {
<b class="fc"><i>2501</i>&nbsp;        return mergeScheduler.stats();</b>
<i>2502</i>&nbsp;    }
<i>2503</i>&nbsp;
<i>2504</i>&nbsp;    LocalCheckpointTracker getLocalCheckpointTracker() {
<b class="nc"><i>2505</i>&nbsp;        return localCheckpointTracker;</b>
<i>2506</i>&nbsp;    }
<i>2507</i>&nbsp;
<i>2508</i>&nbsp;    @Override
<i>2509</i>&nbsp;    public long getLastSyncedGlobalCheckpoint() {
<b class="nc"><i>2510</i>&nbsp;        return getTranslog().getLastSyncedGlobalCheckpoint();</b>
<i>2511</i>&nbsp;    }
<i>2512</i>&nbsp;
<i>2513</i>&nbsp;    public long getProcessedLocalCheckpoint() {
<b class="nc"><i>2514</i>&nbsp;        return localCheckpointTracker.getProcessedCheckpoint();</b>
<i>2515</i>&nbsp;    }
<i>2516</i>&nbsp;
<i>2517</i>&nbsp;    @Override
<i>2518</i>&nbsp;    public long getPersistedLocalCheckpoint() {
<b class="fc"><i>2519</i>&nbsp;        return localCheckpointTracker.getPersistedCheckpoint();</b>
<i>2520</i>&nbsp;    }
<i>2521</i>&nbsp;
<i>2522</i>&nbsp;    /**
<i>2523</i>&nbsp;     * Marks the given seq_no as seen and advances the max_seq_no of this engine to at least that value.
<i>2524</i>&nbsp;     */
<i>2525</i>&nbsp;    protected final void markSeqNoAsSeen(long seqNo) {
<b class="nc"><i>2526</i>&nbsp;        localCheckpointTracker.advanceMaxSeqNo(seqNo);</b>
<b class="nc"><i>2527</i>&nbsp;    }</b>
<i>2528</i>&nbsp;
<i>2529</i>&nbsp;    /**
<i>2530</i>&nbsp;     * Checks if the given operation has been processed in this engine or not.
<i>2531</i>&nbsp;     * @return true if the given operation was processed; otherwise false.
<i>2532</i>&nbsp;     */
<i>2533</i>&nbsp;    protected final boolean hasBeenProcessedBefore(Operation op) {
<b class="nc"><i>2534</i>&nbsp;        if (Assertions.ENABLED) {</b>
<b class="nc"><i>2535</i>&nbsp;            assert op.seqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO : &quot;operation is not assigned seq_no&quot;;</b>
<b class="nc"><i>2536</i>&nbsp;            if (op.operationType() == Operation.TYPE.NO_OP) {</b>
<b class="nc"><i>2537</i>&nbsp;                assert noOpKeyedLock.isHeldByCurrentThread(op.seqNo());</b>
<i>2538</i>&nbsp;            } else {
<b class="nc"><i>2539</i>&nbsp;                assert versionMap.assertKeyedLockHeldByCurrentThread(op.uid().bytes());</b>
<i>2540</i>&nbsp;            }
<i>2541</i>&nbsp;        }
<b class="nc"><i>2542</i>&nbsp;        return localCheckpointTracker.hasProcessed(op.seqNo());</b>
<i>2543</i>&nbsp;    }
<i>2544</i>&nbsp;
<i>2545</i>&nbsp;    @Override
<i>2546</i>&nbsp;    public SeqNoStats getSeqNoStats(long globalCheckpoint) {
<b class="nc"><i>2547</i>&nbsp;        return localCheckpointTracker.getStats(globalCheckpoint);</b>
<i>2548</i>&nbsp;    }
<i>2549</i>&nbsp;
<i>2550</i>&nbsp;    /**
<i>2551</i>&nbsp;     * Returns the number of times a version was looked up either from the index.
<i>2552</i>&nbsp;     * Note this is only available if assertions are enabled
<i>2553</i>&nbsp;     */
<i>2554</i>&nbsp;    long getNumIndexVersionsLookups() { // for testing
<b class="nc"><i>2555</i>&nbsp;        return numIndexVersionsLookups.count();</b>
<i>2556</i>&nbsp;    }
<i>2557</i>&nbsp;
<i>2558</i>&nbsp;    /**
<i>2559</i>&nbsp;     * Returns the number of times a version was looked up either from memory or from the index.
<i>2560</i>&nbsp;     * Note this is only available if assertions are enabled
<i>2561</i>&nbsp;     */
<i>2562</i>&nbsp;    long getNumVersionLookups() { // for testing
<b class="nc"><i>2563</i>&nbsp;        return numVersionLookups.count();</b>
<i>2564</i>&nbsp;    }
<i>2565</i>&nbsp;
<i>2566</i>&nbsp;    private boolean incrementVersionLookup() { // only used by asserts
<b class="nc"><i>2567</i>&nbsp;        numVersionLookups.inc();</b>
<b class="nc"><i>2568</i>&nbsp;        return true;</b>
<i>2569</i>&nbsp;    }
<i>2570</i>&nbsp;
<i>2571</i>&nbsp;    private boolean incrementIndexVersionLookup() {
<b class="nc"><i>2572</i>&nbsp;        numIndexVersionsLookups.inc();</b>
<b class="nc"><i>2573</i>&nbsp;        return true;</b>
<i>2574</i>&nbsp;    }
<i>2575</i>&nbsp;
<i>2576</i>&nbsp;    boolean isSafeAccessRequired() {
<b class="nc"><i>2577</i>&nbsp;        return versionMap.isSafeAccessRequired();</b>
<i>2578</i>&nbsp;    }
<i>2579</i>&nbsp;
<i>2580</i>&nbsp;    /**
<i>2581</i>&nbsp;     * Returns the number of documents have been deleted since this engine was opened.
<i>2582</i>&nbsp;     * This count does not include the deletions from the existing segments before opening engine.
<i>2583</i>&nbsp;     */
<i>2584</i>&nbsp;    long getNumDocDeletes() {
<b class="nc"><i>2585</i>&nbsp;        return numDocDeletes.count();</b>
<i>2586</i>&nbsp;    }
<i>2587</i>&nbsp;
<i>2588</i>&nbsp;    /**
<i>2589</i>&nbsp;     * Returns the number of documents have been appended since this engine was opened.
<i>2590</i>&nbsp;     * This count does not include the appends from the existing segments before opening engine.
<i>2591</i>&nbsp;     */
<i>2592</i>&nbsp;    long getNumDocAppends() {
<b class="nc"><i>2593</i>&nbsp;        return numDocAppends.count();</b>
<i>2594</i>&nbsp;    }
<i>2595</i>&nbsp;
<i>2596</i>&nbsp;    /**
<i>2597</i>&nbsp;     * Returns the number of documents have been updated since this engine was opened.
<i>2598</i>&nbsp;     * This count does not include the updates from the existing segments before opening engine.
<i>2599</i>&nbsp;     */
<i>2600</i>&nbsp;    long getNumDocUpdates() {
<b class="nc"><i>2601</i>&nbsp;        return numDocUpdates.count();</b>
<i>2602</i>&nbsp;    }
<i>2603</i>&nbsp;
<i>2604</i>&nbsp;    private void ensureSoftDeletesEnabled() {
<b class="nc"><i>2605</i>&nbsp;        if (softDeleteEnabled == false) {</b>
<b class="nc"><i>2606</i>&nbsp;            assert false : &quot;index &quot; + shardId.getIndex() + &quot; does not have soft-deletes enabled&quot;;</b>
<b class="nc"><i>2607</i>&nbsp;            throw new IllegalStateException(&quot;index &quot; + shardId.getIndex() + &quot; does not have soft-deletes enabled&quot;);</b>
<i>2608</i>&nbsp;        }
<b class="nc"><i>2609</i>&nbsp;    }</b>
<i>2610</i>&nbsp;
<i>2611</i>&nbsp;    @Override
<i>2612</i>&nbsp;    public Translog.Snapshot newChangesSnapshot(String source, MapperService mapperService,
<i>2613</i>&nbsp;                                                long fromSeqNo, long toSeqNo, boolean requiredFullRange) throws IOException {
<b class="nc"><i>2614</i>&nbsp;        ensureSoftDeletesEnabled();</b>
<b class="nc"><i>2615</i>&nbsp;        ensureOpen();</b>
<b class="nc"><i>2616</i>&nbsp;        refreshIfNeeded(source, toSeqNo);</b>
<b class="nc"><i>2617</i>&nbsp;        Searcher searcher = acquireSearcher(source, SearcherScope.INTERNAL);</b>
<i>2618</i>&nbsp;        try {
<b class="nc"><i>2619</i>&nbsp;            LuceneChangesSnapshot snapshot = new LuceneChangesSnapshot(</b>
<i>2620</i>&nbsp;                searcher, mapperService, LuceneChangesSnapshot.DEFAULT_BATCH_SIZE, fromSeqNo, toSeqNo, requiredFullRange);
<b class="nc"><i>2621</i>&nbsp;            searcher = null;</b>
<b class="nc"><i>2622</i>&nbsp;            return snapshot;</b>
<b class="nc"><i>2623</i>&nbsp;        } catch (Exception e) {</b>
<i>2624</i>&nbsp;            try {
<b class="nc"><i>2625</i>&nbsp;                maybeFailEngine(&quot;acquire changes snapshot&quot;, e);</b>
<b class="nc"><i>2626</i>&nbsp;            } catch (Exception inner) {</b>
<b class="nc"><i>2627</i>&nbsp;                e.addSuppressed(inner);</b>
<b class="nc"><i>2628</i>&nbsp;            }</b>
<b class="nc"><i>2629</i>&nbsp;            throw e;</b>
<i>2630</i>&nbsp;        } finally {
<b class="nc"><i>2631</i>&nbsp;            IOUtils.close(searcher);</b>
<b class="nc"><i>2632</i>&nbsp;        }</b>
<i>2633</i>&nbsp;    }
<i>2634</i>&nbsp;
<i>2635</i>&nbsp;    @Override
<i>2636</i>&nbsp;    public boolean hasCompleteOperationHistory(String reason, HistorySource historySource,
<i>2637</i>&nbsp;                                               MapperService mapperService, long startingSeqNo) throws IOException {
<b class="nc"><i>2638</i>&nbsp;        if (historySource == HistorySource.INDEX) {</b>
<b class="nc"><i>2639</i>&nbsp;            ensureSoftDeletesEnabled();</b>
<b class="nc"><i>2640</i>&nbsp;            return getMinRetainedSeqNo() &lt;= startingSeqNo;</b>
<i>2641</i>&nbsp;        } else {
<b class="nc"><i>2642</i>&nbsp;            final long currentLocalCheckpoint = localCheckpointTracker.getProcessedCheckpoint();</b>
<i>2643</i>&nbsp;            // avoid scanning translog if not necessary
<b class="nc"><i>2644</i>&nbsp;            if (startingSeqNo &gt; currentLocalCheckpoint) {</b>
<b class="nc"><i>2645</i>&nbsp;                return true;</b>
<i>2646</i>&nbsp;            }
<b class="nc"><i>2647</i>&nbsp;            final LocalCheckpointTracker tracker = new LocalCheckpointTracker(startingSeqNo, startingSeqNo - 1);</b>
<b class="nc"><i>2648</i>&nbsp;            try (Translog.Snapshot snapshot = getTranslog().newSnapshotFromMinSeqNo(startingSeqNo)) {</b>
<i>2649</i>&nbsp;                Translog.Operation operation;
<b class="nc"><i>2650</i>&nbsp;                while ((operation = snapshot.next()) != null) {</b>
<b class="nc"><i>2651</i>&nbsp;                    if (operation.seqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO) {</b>
<b class="nc"><i>2652</i>&nbsp;                        tracker.markSeqNoAsProcessed(operation.seqNo());</b>
<i>2653</i>&nbsp;                    }
<i>2654</i>&nbsp;                }
<b class="nc"><i>2655</i>&nbsp;            }</b>
<b class="nc"><i>2656</i>&nbsp;            return tracker.getProcessedCheckpoint() &gt;= currentLocalCheckpoint;</b>
<i>2657</i>&nbsp;        }
<i>2658</i>&nbsp;    }
<i>2659</i>&nbsp;
<i>2660</i>&nbsp;    /**
<i>2661</i>&nbsp;     * Returns the minimum seqno that is retained in the Lucene index.
<i>2662</i>&nbsp;     * Operations whose seq# are at least this value should exist in the Lucene index.
<i>2663</i>&nbsp;     */
<i>2664</i>&nbsp;    public final long getMinRetainedSeqNo() {
<b class="nc"><i>2665</i>&nbsp;        ensureSoftDeletesEnabled();</b>
<b class="nc"><i>2666</i>&nbsp;        return softDeletesPolicy.getMinRetainedSeqNo();</b>
<i>2667</i>&nbsp;    }
<i>2668</i>&nbsp;
<i>2669</i>&nbsp;    @Override
<i>2670</i>&nbsp;    public Closeable acquireHistoryRetentionLock(HistorySource historySource) {
<b class="nc"><i>2671</i>&nbsp;        if (historySource == HistorySource.INDEX) {</b>
<b class="nc"><i>2672</i>&nbsp;            ensureSoftDeletesEnabled();</b>
<b class="nc"><i>2673</i>&nbsp;            return softDeletesPolicy.acquireRetentionLock();</b>
<i>2674</i>&nbsp;        } else {
<b class="nc"><i>2675</i>&nbsp;            return translog.acquireRetentionLock();</b>
<i>2676</i>&nbsp;        }
<i>2677</i>&nbsp;    }
<i>2678</i>&nbsp;
<i>2679</i>&nbsp;    /**
<i>2680</i>&nbsp;     * Gets the commit data from {@link IndexWriter} as a map.
<i>2681</i>&nbsp;     */
<i>2682</i>&nbsp;    private static Map&lt;String, String&gt; commitDataAsMap(final IndexWriter indexWriter) {
<b class="fc"><i>2683</i>&nbsp;        final Map&lt;String, String&gt; commitData = new HashMap&lt;&gt;(8);</b>
<b class="fc"><i>2684</i>&nbsp;        for (Map.Entry&lt;String, String&gt; entry : indexWriter.getLiveCommitData()) {</b>
<b class="fc"><i>2685</i>&nbsp;            commitData.put(entry.getKey(), entry.getValue());</b>
<b class="fc"><i>2686</i>&nbsp;        }</b>
<b class="fc"><i>2687</i>&nbsp;        return commitData;</b>
<i>2688</i>&nbsp;    }
<i>2689</i>&nbsp;
<b class="fc"><i>2690</i>&nbsp;    private final class AssertingIndexWriter extends IndexWriter {</b>
<b class="fc"><i>2691</i>&nbsp;        AssertingIndexWriter(Directory d, IndexWriterConfig conf) throws IOException {</b>
<b class="fc"><i>2692</i>&nbsp;            super(d, conf);</b>
<b class="fc"><i>2693</i>&nbsp;        }</b>
<i>2694</i>&nbsp;        @Override
<i>2695</i>&nbsp;        public long updateDocument(Term term, Iterable&lt;? extends IndexableField&gt; doc) throws IOException {
<b class="nc"><i>2696</i>&nbsp;            assert softDeleteEnabled == false : &quot;Call #updateDocument but soft-deletes is enabled&quot;;</b>
<b class="nc"><i>2697</i>&nbsp;            return super.updateDocument(term, doc);</b>
<i>2698</i>&nbsp;        }
<i>2699</i>&nbsp;        @Override
<i>2700</i>&nbsp;        public long updateDocuments(Term delTerm, Iterable&lt;? extends Iterable&lt;? extends IndexableField&gt;&gt; docs) throws IOException {
<b class="nc"><i>2701</i>&nbsp;            assert softDeleteEnabled == false : &quot;Call #updateDocuments but soft-deletes is enabled&quot;;</b>
<b class="nc"><i>2702</i>&nbsp;            return super.updateDocuments(delTerm, docs);</b>
<i>2703</i>&nbsp;        }
<i>2704</i>&nbsp;        @Override
<i>2705</i>&nbsp;        public long deleteDocuments(Term... terms) throws IOException {
<b class="nc"><i>2706</i>&nbsp;            assert softDeleteEnabled == false : &quot;Call #deleteDocuments but soft-deletes is enabled&quot;;</b>
<b class="nc"><i>2707</i>&nbsp;            return super.deleteDocuments(terms);</b>
<i>2708</i>&nbsp;        }
<i>2709</i>&nbsp;        @Override
<i>2710</i>&nbsp;        public long softUpdateDocument(Term term, Iterable&lt;? extends IndexableField&gt; doc, Field... softDeletes) throws IOException {
<b class="nc"><i>2711</i>&nbsp;            assert softDeleteEnabled : &quot;Call #softUpdateDocument but soft-deletes is disabled&quot;;</b>
<b class="nc"><i>2712</i>&nbsp;            return super.softUpdateDocument(term, doc, softDeletes);</b>
<i>2713</i>&nbsp;        }
<i>2714</i>&nbsp;        @Override
<i>2715</i>&nbsp;        public long softUpdateDocuments(Term term, Iterable&lt;? extends Iterable&lt;? extends IndexableField&gt;&gt; docs,
<i>2716</i>&nbsp;                                            Field... softDeletes) throws IOException {
<b class="nc"><i>2717</i>&nbsp;            assert softDeleteEnabled : &quot;Call #softUpdateDocuments but soft-deletes is disabled&quot;;</b>
<b class="nc"><i>2718</i>&nbsp;            return super.softUpdateDocuments(term, docs, softDeletes);</b>
<i>2719</i>&nbsp;        }
<i>2720</i>&nbsp;        @Override
<i>2721</i>&nbsp;        public long tryDeleteDocument(IndexReader readerIn, int docID) {
<b class="nc"><i>2722</i>&nbsp;            assert false : &quot;#tryDeleteDocument is not supported. See Lucene#DirectoryReaderWithAllLiveDocs&quot;;</b>
<b class="nc"><i>2723</i>&nbsp;            throw new UnsupportedOperationException();</b>
<i>2724</i>&nbsp;        }
<i>2725</i>&nbsp;    }
<i>2726</i>&nbsp;
<i>2727</i>&nbsp;    /**
<i>2728</i>&nbsp;     * Returned the last local checkpoint value has been refreshed internally.
<i>2729</i>&nbsp;     */
<i>2730</i>&nbsp;    final long lastRefreshedCheckpoint() {
<b class="fc"><i>2731</i>&nbsp;        return lastRefreshedCheckpointListener.refreshedCheckpoint.get();</b>
<i>2732</i>&nbsp;    }
<i>2733</i>&nbsp;
<i>2734</i>&nbsp;
<b class="fc"><i>2735</i>&nbsp;    private final Object refreshIfNeededMutex = new Object();</b>
<i>2736</i>&nbsp;
<i>2737</i>&nbsp;    /**
<i>2738</i>&nbsp;     * Refresh this engine **internally** iff the requesting seq_no is greater than the last refreshed checkpoint.
<i>2739</i>&nbsp;     */
<i>2740</i>&nbsp;    protected final void refreshIfNeeded(String source, long requestingSeqNo) {
<b class="nc"><i>2741</i>&nbsp;        if (lastRefreshedCheckpoint() &lt; requestingSeqNo) {</b>
<b class="nc"><i>2742</i>&nbsp;            synchronized (refreshIfNeededMutex) {</b>
<b class="nc"><i>2743</i>&nbsp;                if (lastRefreshedCheckpoint() &lt; requestingSeqNo) {</b>
<b class="nc"><i>2744</i>&nbsp;                    refresh(source, SearcherScope.INTERNAL, true);</b>
<i>2745</i>&nbsp;                }
<b class="nc"><i>2746</i>&nbsp;            }</b>
<i>2747</i>&nbsp;        }
<b class="nc"><i>2748</i>&nbsp;    }</b>
<i>2749</i>&nbsp;
<b class="fc"><i>2750</i>&nbsp;    private final class LastRefreshedCheckpointListener implements ReferenceManager.RefreshListener {</b>
<i>2751</i>&nbsp;        final AtomicLong refreshedCheckpoint;
<i>2752</i>&nbsp;        private long pendingCheckpoint;
<i>2753</i>&nbsp;
<b class="fc"><i>2754</i>&nbsp;        LastRefreshedCheckpointListener(long initialLocalCheckpoint) {</b>
<b class="fc"><i>2755</i>&nbsp;            this.refreshedCheckpoint = new AtomicLong(initialLocalCheckpoint);</b>
<b class="fc"><i>2756</i>&nbsp;        }</b>
<i>2757</i>&nbsp;
<i>2758</i>&nbsp;        @Override
<i>2759</i>&nbsp;        public void beforeRefresh() {
<i>2760</i>&nbsp;            // all changes until this point should be visible after refresh
<b class="fc"><i>2761</i>&nbsp;            pendingCheckpoint = localCheckpointTracker.getProcessedCheckpoint();</b>
<b class="fc"><i>2762</i>&nbsp;        }</b>
<i>2763</i>&nbsp;
<i>2764</i>&nbsp;        @Override
<i>2765</i>&nbsp;        public void afterRefresh(boolean didRefresh) {
<b class="fc"><i>2766</i>&nbsp;            if (didRefresh) {</b>
<b class="nc"><i>2767</i>&nbsp;                updateRefreshedCheckpoint(pendingCheckpoint);</b>
<i>2768</i>&nbsp;            }
<b class="fc"><i>2769</i>&nbsp;        }</b>
<i>2770</i>&nbsp;
<i>2771</i>&nbsp;        void updateRefreshedCheckpoint(long checkpoint) {
<b class="fc"><i>2772</i>&nbsp;            refreshedCheckpoint.updateAndGet(curr -&gt; Math.max(curr, checkpoint));</b>
<b class="fc"><i>2773</i>&nbsp;            assert refreshedCheckpoint.get() &gt;= checkpoint : refreshedCheckpoint.get() + &quot; &lt; &quot; + checkpoint;</b>
<b class="fc"><i>2774</i>&nbsp;        }</b>
<i>2775</i>&nbsp;    }
<i>2776</i>&nbsp;
<i>2777</i>&nbsp;    @Override
<i>2778</i>&nbsp;    public final long getMaxSeenAutoIdTimestamp() {
<b class="nc"><i>2779</i>&nbsp;        return maxSeenAutoIdTimestamp.get();</b>
<i>2780</i>&nbsp;    }
<i>2781</i>&nbsp;
<i>2782</i>&nbsp;    @Override
<i>2783</i>&nbsp;    public final void updateMaxUnsafeAutoIdTimestamp(long newTimestamp) {
<b class="nc"><i>2784</i>&nbsp;        updateAutoIdTimestamp(newTimestamp, true);</b>
<b class="nc"><i>2785</i>&nbsp;    }</b>
<i>2786</i>&nbsp;
<i>2787</i>&nbsp;    private void updateAutoIdTimestamp(long newTimestamp, boolean unsafe) {
<b class="fc"><i>2788</i>&nbsp;        assert newTimestamp &gt;= -1 : &quot;invalid timestamp [&quot; + newTimestamp + &quot;]&quot;;</b>
<b class="fc"><i>2789</i>&nbsp;        maxSeenAutoIdTimestamp.updateAndGet(curr -&gt; Math.max(curr, newTimestamp));</b>
<b class="fc"><i>2790</i>&nbsp;        if (unsafe) {</b>
<b class="fc"><i>2791</i>&nbsp;            maxUnsafeAutoIdTimestamp.updateAndGet(curr -&gt; Math.max(curr, newTimestamp));</b>
<i>2792</i>&nbsp;        }
<b class="fc"><i>2793</i>&nbsp;        assert maxUnsafeAutoIdTimestamp.get() &lt;= maxSeenAutoIdTimestamp.get();</b>
<b class="fc"><i>2794</i>&nbsp;    }</b>
<i>2795</i>&nbsp;
<i>2796</i>&nbsp;    @Override
<i>2797</i>&nbsp;    public long getMaxSeqNoOfUpdatesOrDeletes() {
<b class="nc"><i>2798</i>&nbsp;        return maxSeqNoOfUpdatesOrDeletes.get();</b>
<i>2799</i>&nbsp;    }
<i>2800</i>&nbsp;
<i>2801</i>&nbsp;    @Override
<i>2802</i>&nbsp;    public void advanceMaxSeqNoOfUpdatesOrDeletes(long maxSeqNoOfUpdatesOnPrimary) {
<b class="nc"><i>2803</i>&nbsp;        if (maxSeqNoOfUpdatesOnPrimary == SequenceNumbers.UNASSIGNED_SEQ_NO) {</b>
<b class="nc"><i>2804</i>&nbsp;            assert false : &quot;max_seq_no_of_updates on primary is unassigned&quot;;</b>
<b class="nc"><i>2805</i>&nbsp;            throw new IllegalArgumentException(&quot;max_seq_no_of_updates on primary is unassigned&quot;);</b>
<i>2806</i>&nbsp;        }
<b class="nc"><i>2807</i>&nbsp;        this.maxSeqNoOfUpdatesOrDeletes.updateAndGet(curr -&gt; Math.max(curr, maxSeqNoOfUpdatesOnPrimary));</b>
<b class="nc"><i>2808</i>&nbsp;    }</b>
<i>2809</i>&nbsp;
<i>2810</i>&nbsp;    private boolean assertMaxSeqNoOfUpdatesIsAdvanced(Term id, long seqNo, boolean allowDeleted, boolean relaxIfGapInSeqNo) {
<b class="nc"><i>2811</i>&nbsp;        final long maxSeqNoOfUpdates = getMaxSeqNoOfUpdatesOrDeletes();</b>
<i>2812</i>&nbsp;        // We treat a delete on the tombstones on replicas as a regular document, then use updateDocument (not addDocument).
<b class="nc"><i>2813</i>&nbsp;        if (allowDeleted) {</b>
<b class="nc"><i>2814</i>&nbsp;            final VersionValue versionValue = versionMap.getVersionForAssert(id.bytes());</b>
<b class="nc"><i>2815</i>&nbsp;            if (versionValue != null &amp;&amp; versionValue.isDelete()) {</b>
<b class="nc"><i>2816</i>&nbsp;                return true;</b>
<i>2817</i>&nbsp;            }
<i>2818</i>&nbsp;        }
<i>2819</i>&nbsp;        // Operations can be processed on a replica in a different order than on the primary. If the order on the primary is index-1,
<i>2820</i>&nbsp;        // delete-2, index-3, and the order on a replica is index-1, index-3, delete-2, then the msu of index-3 on the replica is 2
<i>2821</i>&nbsp;        // even though it is an update (overwrites index-1). We should relax this assertion if there is a pending gap in the seq_no.
<b class="nc"><i>2822</i>&nbsp;        if (relaxIfGapInSeqNo &amp;&amp; localCheckpointTracker.getProcessedCheckpoint() &lt; maxSeqNoOfUpdates) {</b>
<b class="nc"><i>2823</i>&nbsp;            return true;</b>
<i>2824</i>&nbsp;        }
<b class="nc"><i>2825</i>&nbsp;        assert seqNo &lt;= maxSeqNoOfUpdates : &quot;id=&quot; + id + &quot; seq_no=&quot; + seqNo + &quot; msu=&quot; + maxSeqNoOfUpdates;</b>
<b class="nc"><i>2826</i>&nbsp;        return true;</b>
<i>2827</i>&nbsp;    }
<i>2828</i>&nbsp;
<i>2829</i>&nbsp;    private static void trimUnsafeCommits(EngineConfig engineConfig) throws IOException {
<b class="fc"><i>2830</i>&nbsp;        final Store store = engineConfig.getStore();</b>
<b class="fc"><i>2831</i>&nbsp;        final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY);</b>
<b class="fc"><i>2832</i>&nbsp;        final Path translogPath = engineConfig.getTranslogConfig().getTranslogPath();</b>
<b class="fc"><i>2833</i>&nbsp;        final long globalCheckpoint = Translog.readGlobalCheckpoint(translogPath, translogUUID);</b>
<b class="fc"><i>2834</i>&nbsp;        final long minRetainedTranslogGen = Translog.readMinTranslogGeneration(translogPath, translogUUID);</b>
<b class="fc"><i>2835</i>&nbsp;        store.trimUnsafeCommits(globalCheckpoint, minRetainedTranslogGen, engineConfig.getIndexSettings().getIndexVersionCreated());</b>
<b class="fc"><i>2836</i>&nbsp;    }</b>
<i>2837</i>&nbsp;
<i>2838</i>&nbsp;    /**
<i>2839</i>&nbsp;     * Restores the live version map and local checkpoint of this engine using documents (including soft-deleted)
<i>2840</i>&nbsp;     * after the local checkpoint in the safe commit. This step ensures the live version map and checkpoint tracker
<i>2841</i>&nbsp;     * are in sync with the Lucene commit.
<i>2842</i>&nbsp;     */
<i>2843</i>&nbsp;    private void restoreVersionMapAndCheckpointTracker(DirectoryReader directoryReader) throws IOException {
<b class="nc"><i>2844</i>&nbsp;        final IndexSearcher searcher = new IndexSearcher(directoryReader);</b>
<b class="nc"><i>2845</i>&nbsp;        searcher.setQueryCache(null);</b>
<b class="nc"><i>2846</i>&nbsp;        final Query query = LongPoint.newRangeQuery(SeqNoFieldMapper.NAME, getPersistedLocalCheckpoint() + 1, Long.MAX_VALUE);</b>
<b class="nc"><i>2847</i>&nbsp;        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f);</b>
<b class="nc"><i>2848</i>&nbsp;        for (LeafReaderContext leaf : directoryReader.leaves()) {</b>
<b class="nc"><i>2849</i>&nbsp;            final Scorer scorer = weight.scorer(leaf);</b>
<b class="nc"><i>2850</i>&nbsp;            if (scorer == null) {</b>
<b class="nc"><i>2851</i>&nbsp;                continue;</b>
<i>2852</i>&nbsp;            }
<b class="nc"><i>2853</i>&nbsp;            final CombinedDocValues dv = new CombinedDocValues(leaf.reader());</b>
<b class="nc"><i>2854</i>&nbsp;            final IdOnlyFieldVisitor idFieldVisitor = new IdOnlyFieldVisitor();</b>
<b class="nc"><i>2855</i>&nbsp;            final DocIdSetIterator iterator = scorer.iterator();</b>
<i>2856</i>&nbsp;            int docId;
<b class="nc"><i>2857</i>&nbsp;            while ((docId = iterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {</b>
<b class="nc"><i>2858</i>&nbsp;                final long primaryTerm = dv.docPrimaryTerm(docId);</b>
<b class="nc"><i>2859</i>&nbsp;                if (primaryTerm == -1L) {</b>
<b class="nc"><i>2860</i>&nbsp;                    continue; // skip children docs which do not have primary term</b>
<i>2861</i>&nbsp;                }
<b class="nc"><i>2862</i>&nbsp;                final long seqNo = dv.docSeqNo(docId);</b>
<b class="nc"><i>2863</i>&nbsp;                localCheckpointTracker.markSeqNoAsProcessed(seqNo);</b>
<b class="nc"><i>2864</i>&nbsp;                localCheckpointTracker.markSeqNoAsPersisted(seqNo);</b>
<b class="nc"><i>2865</i>&nbsp;                idFieldVisitor.reset();</b>
<b class="nc"><i>2866</i>&nbsp;                leaf.reader().document(docId, idFieldVisitor);</b>
<b class="nc"><i>2867</i>&nbsp;                if (idFieldVisitor.getId() == null) {</b>
<b class="nc"><i>2868</i>&nbsp;                    assert dv.isTombstone(docId);</b>
<i>2869</i>&nbsp;                    continue;
<i>2870</i>&nbsp;                }
<b class="nc"><i>2871</i>&nbsp;                final BytesRef uid = new Term(IdFieldMapper.NAME, Uid.encodeId(idFieldVisitor.getId())).bytes();</b>
<b class="nc"><i>2872</i>&nbsp;                try (Releasable ignored = versionMap.acquireLock(uid)) {</b>
<b class="nc"><i>2873</i>&nbsp;                    final VersionValue curr = versionMap.getUnderLock(uid);</b>
<b class="nc"><i>2874</i>&nbsp;                    if (curr == null ||</b>
<b class="nc"><i>2875</i>&nbsp;                        compareOpToVersionMapOnSeqNo(idFieldVisitor.getId(), seqNo, primaryTerm, curr) == OpVsLuceneDocStatus.OP_NEWER) {</b>
<b class="nc"><i>2876</i>&nbsp;                        if (dv.isTombstone(docId)) {</b>
<i>2877</i>&nbsp;                            // use 0L for the start time so we can prune this delete tombstone quickly
<i>2878</i>&nbsp;                            // when the local checkpoint advances (i.e., after a recovery completed).
<b class="nc"><i>2879</i>&nbsp;                            final long startTime = 0L;</b>
<b class="nc"><i>2880</i>&nbsp;                            versionMap.putDeleteUnderLock(uid, new DeleteVersionValue(dv.docVersion(docId), seqNo, primaryTerm, startTime));</b>
<b class="nc"><i>2881</i>&nbsp;                        } else {</b>
<b class="nc"><i>2882</i>&nbsp;                            versionMap.putIndexUnderLock(uid, new IndexVersionValue(null, dv.docVersion(docId), seqNo, primaryTerm));</b>
<i>2883</i>&nbsp;                        }
<i>2884</i>&nbsp;                    }
<b class="nc"><i>2885</i>&nbsp;                }</b>
<b class="nc"><i>2886</i>&nbsp;            }</b>
<b class="nc"><i>2887</i>&nbsp;        }</b>
<i>2888</i>&nbsp;        // remove live entries in the version map
<b class="nc"><i>2889</i>&nbsp;        refresh(&quot;restore_version_map_and_checkpoint_tracker&quot;, SearcherScope.INTERNAL, true);</b>
<b class="nc"><i>2890</i>&nbsp;    }</b>
<i>2891</i>&nbsp;
<i>2892</i>&nbsp;}
</div>
</div>

<div class="footer">
    
    <div style="float:right;">generated on 2020-02-09 18:45</div>
</div>
</body>
</html>
